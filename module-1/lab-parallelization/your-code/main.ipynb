{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelization Lab\n",
    "\n",
    "In this lab, you will be leveraging several concepts you have learned to obtain a list of links from a web page and crawl and index the pages referenced by those links - both sequentially and in parallel. Follow the steps below to complete the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Use the requests library to retrieve the content from the URL below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "url = 'https://en.wikipedia.org/wiki/Data_science'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Use BeautifulSoup to extract a list of all the unique links on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.wikipedia.org/wiki/Information_science', 'https://www.wikipedia.org/wiki/Machine_learning', 'https://www.wikipedia.org/wiki/Data_mining', 'https://www.wikipedia.org/wiki/Statistical_classification', 'https://www.wikipedia.org/wiki/Cluster_analysis', 'https://www.wikipedia.org/wiki/Regression_analysis', 'https://www.wikipedia.org/wiki/Anomaly_detection', 'https://www.wikipedia.org/wiki/Automated_machine_learning', 'https://www.wikipedia.org/wiki/Association_rule_learning', 'https://www.wikipedia.org/wiki/Reinforcement_learning', 'https://www.wikipedia.org/wiki/Structured_prediction', 'https://www.wikipedia.org/wiki/Feature_engineering', 'https://www.wikipedia.org/wiki/Feature_learning', 'https://www.wikipedia.org/wiki/Online_machine_learning', 'https://www.wikipedia.org/wiki/Semi-supervised_learning', 'https://www.wikipedia.org/wiki/Unsupervised_learning', 'https://www.wikipedia.org/wiki/Learning_to_rank', 'https://www.wikipedia.org/wiki/Grammar_induction', 'https://www.wikipedia.org/wiki/Supervised_learning', 'https://www.wikipedia.org/wiki/Statistical_classification', 'https://www.wikipedia.org/wiki/Regression_analysis', 'https://www.wikipedia.org/wiki/Decision_tree_learning', 'https://www.wikipedia.org/wiki/Ensemble_learning', 'https://www.wikipedia.org/wiki/Bootstrap_aggregating', 'https://www.wikipedia.org/wiki/Boosting_(machine_learning)', 'https://www.wikipedia.org/wiki/Random_forest', 'https://www.wikipedia.org/wiki/K-nearest_neighbors_algorithm', 'https://www.wikipedia.org/wiki/Linear_regression', 'https://www.wikipedia.org/wiki/Naive_Bayes_classifier', 'https://www.wikipedia.org/wiki/Artificial_neural_network', 'https://www.wikipedia.org/wiki/Logistic_regression', 'https://www.wikipedia.org/wiki/Perceptron', 'https://www.wikipedia.org/wiki/Relevance_vector_machine', 'https://www.wikipedia.org/wiki/Support-vector_machine', 'https://www.wikipedia.org/wiki/Cluster_analysis', 'https://www.wikipedia.org/wiki/BIRCH', 'https://www.wikipedia.org/wiki/CURE_data_clustering_algorithm', 'https://www.wikipedia.org/wiki/Hierarchical_clustering', 'https://www.wikipedia.org/wiki/K-means_clustering', 'https://www.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm', 'https://www.wikipedia.org/wiki/DBSCAN', 'https://www.wikipedia.org/wiki/OPTICS_algorithm', 'https://www.wikipedia.org/wiki/Mean-shift', 'https://www.wikipedia.org/wiki/Dimensionality_reduction', 'https://www.wikipedia.org/wiki/Factor_analysis', 'https://www.wikipedia.org/wiki/Canonical_correlation', 'https://www.wikipedia.org/wiki/Independent_component_analysis', 'https://www.wikipedia.org/wiki/Linear_discriminant_analysis', 'https://www.wikipedia.org/wiki/Non-negative_matrix_factorization', 'https://www.wikipedia.org/wiki/Principal_component_analysis', 'https://www.wikipedia.org/wiki/Proper_generalized_decomposition', 'https://www.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding', 'https://www.wikipedia.org/wiki/Structured_prediction', 'https://www.wikipedia.org/wiki/Graphical_model', 'https://www.wikipedia.org/wiki/Bayesian_network', 'https://www.wikipedia.org/wiki/Conditional_random_field', 'https://www.wikipedia.org/wiki/Hidden_Markov_model', 'https://www.wikipedia.org/wiki/Anomaly_detection', 'https://www.wikipedia.org/wiki/K-nearest_neighbors_classification', 'https://www.wikipedia.org/wiki/Local_outlier_factor', 'https://www.wikipedia.org/wiki/Artificial_neural_network', 'https://www.wikipedia.org/wiki/Autoencoder', 'https://www.wikipedia.org/wiki/Deep_learning', 'https://www.wikipedia.org/wiki/DeepDream', 'https://www.wikipedia.org/wiki/Multilayer_perceptron', 'https://www.wikipedia.org/wiki/Recurrent_neural_network', 'https://www.wikipedia.org/wiki/Long_short-term_memory', 'https://www.wikipedia.org/wiki/Gated_recurrent_unit', 'https://www.wikipedia.org/wiki/Echo_state_network', 'https://www.wikipedia.org/wiki/Restricted_Boltzmann_machine', 'https://www.wikipedia.org/wiki/Generative_adversarial_network', 'https://www.wikipedia.org/wiki/Self-organizing_map', 'https://www.wikipedia.org/wiki/Convolutional_neural_network', 'https://www.wikipedia.org/wiki/U-Net', 'https://www.wikipedia.org/wiki/Reinforcement_learning', 'https://www.wikipedia.org/wiki/Q-learning', 'https://www.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action', 'https://www.wikipedia.org/wiki/Temporal_difference_learning', 'https://www.wikipedia.org/wiki/Bias%E2%80%93variance_dilemma', 'https://www.wikipedia.org/wiki/Computational_learning_theory', 'https://www.wikipedia.org/wiki/Empirical_risk_minimization', 'https://www.wikipedia.org/wiki/Occam_learning', 'https://www.wikipedia.org/wiki/Probably_approximately_correct_learning', 'https://www.wikipedia.org/wiki/Statistical_learning_theory', 'https://www.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory', 'https://www.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems', 'https://www.wikipedia.org/wiki/International_Conference_on_Machine_Learning', 'https://www.wikipedia.org/wiki/Machine_Learning_(journal)', 'https://www.wikipedia.org/wiki/Journal_of_Machine_Learning_Research', 'https://www.wikipedia.org/wiki/Glossary_of_artificial_intelligence', 'https://www.wikipedia.org/wiki/Glossary_of_artificial_intelligence', 'https://www.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research', 'https://www.wikipedia.org/wiki/Outline_of_machine_learning', 'https://www.wikipedia.org/wiki/Template:Machine_learning_bar', 'https://www.wikipedia.org/wiki/Template_talk:Machine_learning_bar', 'https://www.wikipedia.org/wiki/Inter-disciplinary', 'https://www.wikipedia.org/wiki/Knowledge', 'https://www.wikipedia.org/wiki/Unstructured_data', 'https://www.wikipedia.org/wiki/Data_mining', 'https://www.wikipedia.org/wiki/Machine_learning', 'https://www.wikipedia.org/wiki/Big_data', 'https://www.wikipedia.org/wiki/Statistics', 'https://www.wikipedia.org/wiki/Data_analysis', 'https://www.wikipedia.org/wiki/Machine_learning', 'https://www.wikipedia.org/wiki/Domain_knowledge', 'https://www.wikipedia.org/wiki/Mathematics', 'https://www.wikipedia.org/wiki/Statistics', 'https://www.wikipedia.org/wiki/Computer_science', 'https://www.wikipedia.org/wiki/Domain_knowledge', 'https://www.wikipedia.org/wiki/Information_science', 'https://www.wikipedia.org/wiki/Turing_award', 'https://www.wikipedia.org/wiki/Jim_Gray_(computer_scientist)', 'https://www.wikipedia.org/wiki/Empirical_research', 'https://www.wikipedia.org/wiki/Basic_research', 'https://www.wikipedia.org/wiki/Computational_science', 'https://www.wikipedia.org/wiki/Information_explosion', 'https://www.wikipedia.org/wiki/Big_data', 'https://www.wikipedia.org/wiki/Information_visualization', 'https://www.wikipedia.org/wiki/Complex_systems', 'https://www.wikipedia.org/wiki/Communication', 'https://www.wikipedia.org/wiki/Buzzword_bingo', 'https://www.wikipedia.org/wiki/Nathan_Yau', 'https://www.wikipedia.org/wiki/Ben_Fry', 'https://www.wikipedia.org/wiki/Human%E2%80%93computer_interaction', 'https://www.wikipedia.org/wiki/American_Statistical_Association', 'https://www.wikipedia.org/wiki/Database', 'https://www.wikipedia.org/wiki/Machine_learning', 'https://www.wikipedia.org/wiki/Distributed_computing', 'https://www.wikipedia.org/wiki/Nate_Silver', 'https://www.wikipedia.org/wiki/Vasant_Dhar', 'https://www.wikipedia.org/wiki/Andrew_Gelman', 'https://www.wikipedia.org/wiki/David_Donoho', 'https://www.wikipedia.org/wiki/John_Tukey', 'https://www.wikipedia.org/wiki/Montpellier_2_University', 'https://www.wikipedia.org/wiki/Peter_Naur', 'https://www.wikipedia.org/wiki/C.F._Jeff_Wu', 'https://www.wikipedia.org/wiki/William_S._Cleveland', 'https://www.wikipedia.org/wiki/Committee_on_Data_for_Science_and_Technology', 'https://www.wikipedia.org/wiki/American_Statistical_Association', 'https://www.wikipedia.org/wiki/DJ_Patil', 'https://www.wikipedia.org/wiki/Jeff_Hammerbacher', 'https://www.wikipedia.org/wiki/National_Science_Board', 'https://www.wikipedia.org/wiki/Wikipedia:LSC', 'https://www.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Stand-alone_lists', 'https://www.wikipedia.org/wiki/Talk:Data_science', 'https://www.wikipedia.org/wiki/Linear_regression', 'https://www.wikipedia.org/wiki/Logistic_regression', 'https://www.wikipedia.org/wiki/Support_vector_machine', 'https://www.wikipedia.org/wiki/Cluster_analysis', 'https://www.wikipedia.org/wiki/Dimensionality_reduction', 'https://www.wikipedia.org/wiki/Machine_learning', 'https://www.wikipedia.org/wiki/Python_(programming_language)', 'https://www.wikipedia.org/wiki/R_(programming_language)', 'https://www.wikipedia.org/wiki/Julia_(programming_language)', 'https://www.wikipedia.org/wiki/TensorFlow', 'https://www.wikipedia.org/wiki/Pytorch', 'https://www.wikipedia.org/wiki/Jupyter_Notebook', 'https://www.wikipedia.org/wiki/Apache_Hadoop', 'https://www.wikipedia.org/wiki/Plotly', 'https://www.wikipedia.org/wiki/Tableau_Software', 'https://www.wikipedia.org/wiki/Microsoft_Power_BI', 'https://www.wikipedia.org/wiki/Qlik', 'https://www.wikipedia.org/wiki/AnyChart', 'https://www.wikipedia.org/wiki/Google_Charts', 'https://www.wikipedia.org/wiki/Sisense', 'https://www.wikipedia.org/wiki/Webix', 'https://www.wikipedia.org/wiki/RapidMiner', 'https://www.wikipedia.org/wiki/Dataiku', 'https://www.wikipedia.org/wiki/Anaconda_(Python_distribution)', 'https://www.wikipedia.org/wiki/MATLAB', 'https://www.wikipedia.org/wiki/Doi_(identifier)', 'https://www.wikipedia.org/wiki/Jeffrey_T._Leek', 'https://www.wikipedia.org/wiki/Doi_(identifier)', 'https://www.wikipedia.org/wiki/ISBN_(identifier)', 'https://www.wikipedia.org/wiki/Special:BookSources/9784431702085', 'https://www.wikipedia.org/wiki/ISBN_(identifier)', 'https://www.wikipedia.org/wiki/Special:BookSources/978-0-9825442-0-4', 'https://www.wikipedia.org/wiki/Doi_(identifier)', 'https://www.wikipedia.org/wiki/ISSN_(identifier)', 'https://www.wikipedia.org/wiki/PMID_(identifier)', 'https://www.wikipedia.org/wiki/American_Statistical_Association', 'https://www.wikipedia.org/wiki/Doi_(identifier)', 'https://www.wikipedia.org/wiki/ISBN_(identifier)', 'https://www.wikipedia.org/wiki/Special:BookSources/0-12-241770-4', 'https://www.wikipedia.org/wiki/OCLC_(identifier)', 'https://www.wikipedia.org/wiki/Category:CS1_maint:_others', 'https://www.wikipedia.org/wiki/Doi_(identifier)', 'https://www.wikipedia.org/wiki/Doi_(identifier)', 'https://www.wikipedia.org/wiki/ISSN_(identifier)', 'https://www.wikipedia.org/wiki/Template:Data', 'https://www.wikipedia.org/wiki/Template_talk:Data', 'https://www.wikipedia.org/wiki/Data_(computing)', 'https://www.wikipedia.org/wiki/Data_analysis', 'https://www.wikipedia.org/wiki/Data_archaeology', 'https://www.wikipedia.org/wiki/Data_cleansing', 'https://www.wikipedia.org/wiki/Data_collection', 'https://www.wikipedia.org/wiki/Data_compression', 'https://www.wikipedia.org/wiki/Data_corruption', 'https://www.wikipedia.org/wiki/Data_curation', 'https://www.wikipedia.org/wiki/Data_degradation', 'https://www.wikipedia.org/wiki/Data_editing', 'https://www.wikipedia.org/wiki/Extract,_transform,_load', 'https://www.wikipedia.org/wiki/Data_farming', 'https://www.wikipedia.org/wiki/Data_format_management', 'https://www.wikipedia.org/wiki/Data_fusion', 'https://www.wikipedia.org/wiki/Data_integration', 'https://www.wikipedia.org/wiki/Data_integrity', 'https://www.wikipedia.org/wiki/Data_library', 'https://www.wikipedia.org/wiki/Data_loss', 'https://www.wikipedia.org/wiki/Data_management', 'https://www.wikipedia.org/wiki/Data_migration', 'https://www.wikipedia.org/wiki/Data_mining', 'https://www.wikipedia.org/wiki/Data_pre-processing', 'https://www.wikipedia.org/wiki/Data_preservation', 'https://www.wikipedia.org/wiki/Information_privacy', 'https://www.wikipedia.org/wiki/Data_recovery', 'https://www.wikipedia.org/wiki/Data_reduction', 'https://www.wikipedia.org/wiki/Data_retention', 'https://www.wikipedia.org/wiki/Data_quality', 'https://www.wikipedia.org/wiki/Data_scraping', 'https://www.wikipedia.org/wiki/Data_scrubbing', 'https://www.wikipedia.org/wiki/Data_security', 'https://www.wikipedia.org/wiki/Data_steward', 'https://www.wikipedia.org/wiki/Data_storage', 'https://www.wikipedia.org/wiki/Data_validation', 'https://www.wikipedia.org/wiki/Data_warehouse', 'https://www.wikipedia.org/wiki/Data_wrangling']\n"
     ]
    }
   ],
   "source": [
    "url = req.get('https://en.wikipedia.org/wiki/Data_science').content\n",
    "soup = bs(url, 'html.parser')\n",
    "table = soup.find('div',{'class':'mw-parser-output'})\n",
    "body = table.findAll('a')\n",
    "links = re.findall('\\/\\wiki.+?\\\"', str(body))\n",
    "for i in range(len(links)):    \n",
    "    links[i] = links[i][0:-1]\n",
    "lst = ['https://www.wikipedia.org' + sub for sub in links] \n",
    "print(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Use list comprehensions with conditions to clean the link list.\n",
    "\n",
    "There are two types of links, absolute and relative. Absolute links have the full URL and begin with http while relative links begin with a forward slash (/) and point to an internal page within the wikipedia.org domain. Clean the respective types of URLs as follows.\n",
    "\n",
    "- Absolute Links: Create a list of these and remove any that contain a percentage sign (%).\n",
    "- Relativel Links: Create a list of these, add the domain to the link so that you have the full URL, and remove any that contain a percentage sign (%).\n",
    "- Combine the list of absolute and relative links and ensure there are no duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'http://wikipedia.org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.wikipedia.org/wiki/Data_reduction',\n",
       " 'https://www.wikipedia.org/wiki/Data_validation',\n",
       " 'https://www.wikipedia.org/wiki/Independent_component_analysis',\n",
       " 'https://www.wikipedia.org/wiki/Logistic_regression',\n",
       " 'https://www.wikipedia.org/wiki/Turing_award',\n",
       " 'https://www.wikipedia.org/wiki/Plotly',\n",
       " 'https://www.wikipedia.org/wiki/Mean-shift',\n",
       " 'https://www.wikipedia.org/wiki/Ben_Fry',\n",
       " 'https://www.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding',\n",
       " 'https://www.wikipedia.org/wiki/Unsupervised_learning',\n",
       " 'https://www.wikipedia.org/wiki/Perceptron',\n",
       " 'https://www.wikipedia.org/wiki/Data_format_management',\n",
       " 'https://www.wikipedia.org/wiki/Online_machine_learning',\n",
       " 'https://www.wikipedia.org/wiki/Data_preservation',\n",
       " 'https://www.wikipedia.org/wiki/Knowledge',\n",
       " 'https://www.wikipedia.org/wiki/Echo_state_network',\n",
       " 'https://www.wikipedia.org/wiki/Basic_research',\n",
       " 'https://www.wikipedia.org/wiki/K-nearest_neighbors_algorithm',\n",
       " 'https://www.wikipedia.org/wiki/Complex_systems',\n",
       " 'https://www.wikipedia.org/wiki/Andrew_Gelman',\n",
       " 'https://www.wikipedia.org/wiki/Machine_learning',\n",
       " 'https://www.wikipedia.org/wiki/Cluster_analysis',\n",
       " 'https://www.wikipedia.org/wiki/Anomaly_detection',\n",
       " 'https://www.wikipedia.org/wiki/Learning_to_rank',\n",
       " 'https://www.wikipedia.org/wiki/Data_loss',\n",
       " 'https://www.wikipedia.org/wiki/Domain_knowledge',\n",
       " 'https://www.wikipedia.org/wiki/Dimensionality_reduction',\n",
       " 'https://www.wikipedia.org/wiki/Sisense',\n",
       " 'https://www.wikipedia.org/wiki/Generative_adversarial_network',\n",
       " 'https://www.wikipedia.org/wiki/RapidMiner',\n",
       " 'https://www.wikipedia.org/wiki/Data_warehouse',\n",
       " 'https://www.wikipedia.org/wiki/Pytorch',\n",
       " 'https://www.wikipedia.org/wiki/Recurrent_neural_network',\n",
       " 'https://www.wikipedia.org/wiki/Communication',\n",
       " 'https://www.wikipedia.org/wiki/Random_forest',\n",
       " 'https://www.wikipedia.org/wiki/C.F._Jeff_Wu',\n",
       " 'https://www.wikipedia.org/wiki/Nathan_Yau',\n",
       " 'https://www.wikipedia.org/wiki/Ensemble_learning',\n",
       " 'https://www.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Stand-alone_lists',\n",
       " 'https://www.wikipedia.org/wiki/Talk:Data_science',\n",
       " 'https://www.wikipedia.org/wiki/Dataiku',\n",
       " 'https://www.wikipedia.org/wiki/Supervised_learning',\n",
       " 'https://www.wikipedia.org/wiki/Data_fusion',\n",
       " 'https://www.wikipedia.org/wiki/Vasant_Dhar',\n",
       " 'https://www.wikipedia.org/wiki/Special:BookSources/978-0-9825442-0-4',\n",
       " 'https://www.wikipedia.org/wiki/Data_curation',\n",
       " 'https://www.wikipedia.org/wiki/Statistical_classification',\n",
       " 'https://www.wikipedia.org/wiki/Reinforcement_learning',\n",
       " 'https://www.wikipedia.org/wiki/Statistical_learning_theory',\n",
       " 'https://www.wikipedia.org/wiki/Data_migration',\n",
       " 'https://www.wikipedia.org/wiki/Jeffrey_T._Leek',\n",
       " 'https://www.wikipedia.org/wiki/Data_pre-processing',\n",
       " 'https://www.wikipedia.org/wiki/Information_visualization',\n",
       " 'https://www.wikipedia.org/wiki/Julia_(programming_language)',\n",
       " 'https://www.wikipedia.org/wiki/Doi_(identifier)',\n",
       " 'https://www.wikipedia.org/wiki/PMID_(identifier)',\n",
       " 'https://www.wikipedia.org/wiki/Webix',\n",
       " 'https://www.wikipedia.org/wiki/Feature_engineering',\n",
       " 'https://www.wikipedia.org/wiki/Data_scraping',\n",
       " 'https://www.wikipedia.org/wiki/Bayesian_network',\n",
       " 'https://www.wikipedia.org/wiki/U-Net',\n",
       " 'https://www.wikipedia.org/wiki/Data_wrangling',\n",
       " 'https://www.wikipedia.org/wiki/Data_integrity',\n",
       " 'https://www.wikipedia.org/wiki/ISBN_(identifier)',\n",
       " 'https://www.wikipedia.org/wiki/Information_science',\n",
       " 'https://www.wikipedia.org/wiki/John_Tukey',\n",
       " 'https://www.wikipedia.org/wiki/Convolutional_neural_network',\n",
       " 'https://www.wikipedia.org/wiki/Category:CS1_maint:_others',\n",
       " 'https://www.wikipedia.org/wiki/Bootstrap_aggregating',\n",
       " 'https://www.wikipedia.org/wiki/Data_degradation',\n",
       " 'https://www.wikipedia.org/wiki/American_Statistical_Association',\n",
       " 'https://www.wikipedia.org/wiki/R_(programming_language)',\n",
       " 'https://www.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research',\n",
       " 'https://www.wikipedia.org/wiki/Data_security',\n",
       " 'https://www.wikipedia.org/wiki/Data_library',\n",
       " 'https://www.wikipedia.org/wiki/Mathematics',\n",
       " 'https://www.wikipedia.org/wiki/Structured_prediction',\n",
       " 'https://www.wikipedia.org/wiki/Big_data',\n",
       " 'https://www.wikipedia.org/wiki/Local_outlier_factor',\n",
       " 'https://www.wikipedia.org/wiki/Data_analysis',\n",
       " 'https://www.wikipedia.org/wiki/Long_short-term_memory',\n",
       " 'https://www.wikipedia.org/wiki/MATLAB',\n",
       " 'https://www.wikipedia.org/wiki/Autoencoder',\n",
       " 'https://www.wikipedia.org/wiki/Conditional_random_field',\n",
       " 'https://www.wikipedia.org/wiki/Hidden_Markov_model',\n",
       " 'https://www.wikipedia.org/wiki/Multilayer_perceptron',\n",
       " 'https://www.wikipedia.org/wiki/Graphical_model',\n",
       " 'https://www.wikipedia.org/wiki/Semi-supervised_learning',\n",
       " 'https://www.wikipedia.org/wiki/Buzzword_bingo',\n",
       " 'https://www.wikipedia.org/wiki/Support_vector_machine',\n",
       " 'https://www.wikipedia.org/wiki/Jupyter_Notebook',\n",
       " 'https://www.wikipedia.org/wiki/Data_collection',\n",
       " 'https://www.wikipedia.org/wiki/Jeff_Hammerbacher',\n",
       " 'https://www.wikipedia.org/wiki/Decision_tree_learning',\n",
       " 'https://www.wikipedia.org/wiki/K-means_clustering',\n",
       " 'https://www.wikipedia.org/wiki/David_Donoho',\n",
       " 'https://www.wikipedia.org/wiki/OPTICS_algorithm',\n",
       " 'https://www.wikipedia.org/wiki/Data_storage',\n",
       " 'https://www.wikipedia.org/wiki/Boosting_(machine_learning)',\n",
       " 'https://www.wikipedia.org/wiki/Regression_analysis',\n",
       " 'https://www.wikipedia.org/wiki/CURE_data_clustering_algorithm',\n",
       " 'https://www.wikipedia.org/wiki/Linear_regression',\n",
       " 'https://www.wikipedia.org/wiki/Q-learning',\n",
       " 'https://www.wikipedia.org/wiki/Non-negative_matrix_factorization',\n",
       " 'https://www.wikipedia.org/wiki/Data_steward',\n",
       " 'https://www.wikipedia.org/wiki/Feature_learning',\n",
       " 'https://www.wikipedia.org/wiki/Data_(computing)',\n",
       " 'https://www.wikipedia.org/wiki/Microsoft_Power_BI',\n",
       " 'https://www.wikipedia.org/wiki/DeepDream',\n",
       " 'https://www.wikipedia.org/wiki/Tableau_Software',\n",
       " 'https://www.wikipedia.org/wiki/Data_archaeology',\n",
       " 'https://www.wikipedia.org/wiki/Probably_approximately_correct_learning',\n",
       " 'https://www.wikipedia.org/wiki/Special:BookSources/9784431702085',\n",
       " 'https://www.wikipedia.org/wiki/Relevance_vector_machine',\n",
       " 'https://www.wikipedia.org/wiki/Data_integration',\n",
       " 'https://www.wikipedia.org/wiki/DBSCAN',\n",
       " 'https://www.wikipedia.org/wiki/Template:Machine_learning_bar',\n",
       " 'https://www.wikipedia.org/wiki/William_S._Cleveland',\n",
       " 'https://www.wikipedia.org/wiki/Anaconda_(Python_distribution)',\n",
       " 'https://www.wikipedia.org/wiki/Data_editing',\n",
       " 'https://www.wikipedia.org/wiki/Proper_generalized_decomposition',\n",
       " 'https://www.wikipedia.org/wiki/Self-organizing_map',\n",
       " 'https://www.wikipedia.org/wiki/Data_management',\n",
       " 'https://www.wikipedia.org/wiki/Machine_Learning_(journal)',\n",
       " 'https://www.wikipedia.org/wiki/Peter_Naur',\n",
       " 'https://www.wikipedia.org/wiki/Association_rule_learning',\n",
       " 'https://www.wikipedia.org/wiki/Principal_component_analysis',\n",
       " 'https://www.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems',\n",
       " 'https://www.wikipedia.org/wiki/Information_privacy',\n",
       " 'https://www.wikipedia.org/wiki/Data_recovery',\n",
       " 'https://www.wikipedia.org/wiki/DJ_Patil',\n",
       " 'https://www.wikipedia.org/wiki/Empirical_research',\n",
       " 'https://www.wikipedia.org/wiki/Support-vector_machine',\n",
       " 'https://www.wikipedia.org/wiki/BIRCH',\n",
       " 'https://www.wikipedia.org/wiki/Data_scrubbing',\n",
       " 'https://www.wikipedia.org/wiki/Computer_science',\n",
       " 'https://www.wikipedia.org/wiki/Hierarchical_clustering',\n",
       " 'https://www.wikipedia.org/wiki/Automated_machine_learning',\n",
       " 'https://www.wikipedia.org/wiki/Artificial_neural_network',\n",
       " 'https://www.wikipedia.org/wiki/Factor_analysis',\n",
       " 'https://www.wikipedia.org/wiki/Gated_recurrent_unit',\n",
       " 'https://www.wikipedia.org/wiki/Empirical_risk_minimization',\n",
       " 'https://www.wikipedia.org/wiki/Temporal_difference_learning',\n",
       " 'https://www.wikipedia.org/wiki/Data_corruption',\n",
       " 'https://www.wikipedia.org/wiki/AnyChart',\n",
       " 'https://www.wikipedia.org/wiki/Template_talk:Data',\n",
       " 'https://www.wikipedia.org/wiki/Glossary_of_artificial_intelligence',\n",
       " 'https://www.wikipedia.org/wiki/National_Science_Board',\n",
       " 'https://www.wikipedia.org/wiki/Distributed_computing',\n",
       " 'https://www.wikipedia.org/wiki/Nate_Silver',\n",
       " 'https://www.wikipedia.org/wiki/Database',\n",
       " 'https://www.wikipedia.org/wiki/Canonical_correlation',\n",
       " 'https://www.wikipedia.org/wiki/Montpellier_2_University',\n",
       " 'https://www.wikipedia.org/wiki/Template_talk:Machine_learning_bar',\n",
       " 'https://www.wikipedia.org/wiki/Information_explosion',\n",
       " 'https://www.wikipedia.org/wiki/K-nearest_neighbors_classification',\n",
       " 'https://www.wikipedia.org/wiki/Jim_Gray_(computer_scientist)',\n",
       " 'https://www.wikipedia.org/wiki/Computational_science',\n",
       " 'https://www.wikipedia.org/wiki/Deep_learning',\n",
       " 'https://www.wikipedia.org/wiki/Occam_learning',\n",
       " 'https://www.wikipedia.org/wiki/Extract,_transform,_load',\n",
       " 'https://www.wikipedia.org/wiki/OCLC_(identifier)',\n",
       " 'https://www.wikipedia.org/wiki/Special:BookSources/0-12-241770-4',\n",
       " 'https://www.wikipedia.org/wiki/Outline_of_machine_learning',\n",
       " 'https://www.wikipedia.org/wiki/Grammar_induction',\n",
       " 'https://www.wikipedia.org/wiki/Data_farming',\n",
       " 'https://www.wikipedia.org/wiki/TensorFlow',\n",
       " 'https://www.wikipedia.org/wiki/Computational_learning_theory',\n",
       " 'https://www.wikipedia.org/wiki/Naive_Bayes_classifier',\n",
       " 'https://www.wikipedia.org/wiki/Restricted_Boltzmann_machine',\n",
       " 'https://www.wikipedia.org/wiki/Python_(programming_language)',\n",
       " 'https://www.wikipedia.org/wiki/International_Conference_on_Machine_Learning',\n",
       " 'https://www.wikipedia.org/wiki/Data_retention',\n",
       " 'https://www.wikipedia.org/wiki/Apache_Hadoop',\n",
       " 'https://www.wikipedia.org/wiki/Statistics',\n",
       " 'https://www.wikipedia.org/wiki/Committee_on_Data_for_Science_and_Technology',\n",
       " 'https://www.wikipedia.org/wiki/Data_mining',\n",
       " 'https://www.wikipedia.org/wiki/Wikipedia:LSC',\n",
       " 'https://www.wikipedia.org/wiki/Data_cleansing',\n",
       " 'https://www.wikipedia.org/wiki/Qlik',\n",
       " 'https://www.wikipedia.org/wiki/Linear_discriminant_analysis',\n",
       " 'https://www.wikipedia.org/wiki/Data_compression',\n",
       " 'https://www.wikipedia.org/wiki/Unstructured_data',\n",
       " 'https://www.wikipedia.org/wiki/ISSN_(identifier)',\n",
       " 'https://www.wikipedia.org/wiki/Template:Data',\n",
       " 'https://www.wikipedia.org/wiki/Inter-disciplinary',\n",
       " 'https://www.wikipedia.org/wiki/Google_Charts',\n",
       " 'https://www.wikipedia.org/wiki/Journal_of_Machine_Learning_Research',\n",
       " 'https://www.wikipedia.org/wiki/Data_quality']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_lst = list(set(i for i in lst if '%' not in i))\n",
    "clean_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Use the os library to create a folder called *wikipedia* and make that the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current working directory is C:\\Users\\omarm\\datamex_082020\\module-1\\lab-parallelization\\your-code\\wikipedia\\wikipedia2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = os.getcwd()\n",
    "print (\"The current working directory is %s\" % path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\omarm\\\\datamex_082020\\\\module-1\\\\lab-parallelization\\\\your-code\\\\wikipedia\\\\wikipedia2\\\\wikipedia2'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "os.mkdir(path+'\\\\wikipedia2')\n",
    "path = path+'\\\\wikipedia2'\n",
    "os.chdir(path)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Write a function called index_page that accepts a link and does the following.\n",
    "\n",
    "- Tries to request the content of the page referenced by that link.\n",
    "- Slugifies the filename using the `slugify` function from the [python-slugify](https://pypi.org/project/python-slugify/) library and adds a .html file extension.\n",
    "    - If you don't already have the python-slugify library installed, you can pip install it as follows: `$ pip install python-slugify`.\n",
    "    - To import the slugify function, you would do the following: `from slugify import slugify`.\n",
    "    - You can then slugify a link as follows `slugify(link)`.\n",
    "- Creates a file in the wikipedia folder using the slugified filename and writes the contents of the page to the file.\n",
    "- If an exception occurs during the process above, just `pass`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-slugify\n",
      "  Using cached python-slugify-4.0.1.tar.gz (11 kB)\n",
      "Collecting text-unidecode>=1.3\n",
      "  Using cached text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Building wheels for collected packages: python-slugify\n",
      "  Building wheel for python-slugify (setup.py): started\n",
      "  Building wheel for python-slugify (setup.py): finished with status 'done'\n",
      "  Created wheel for python-slugify: filename=python_slugify-4.0.1-py2.py3-none-any.whl size=6774 sha256=1fcf6d2fe6a2df98be18b80d624c309aeb71a5114dfefdc5b35c48355cf34f5e\n",
      "  Stored in directory: c:\\users\\omarm\\appdata\\local\\pip\\cache\\wheels\\91\\4d\\4f\\e740a68c215791688c46c4d6251770a570e8dfea91af1acb5c\n",
      "Successfully built python-slugify\n",
      "Installing collected packages: text-unidecode, python-slugify\n",
      "Successfully installed python-slugify-4.0.1 text-unidecode-1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-slugify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slugify import slugify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_page(link):\n",
    "    try:\n",
    "        req = requests.get(link).content\n",
    "        filename = slugify(link)+'.html'\n",
    "        with open(path+'\\\\'+filename, 'wb') as file:\n",
    "            file.write(req)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Sequentially loop through the list of links, running the index_page function each time.\n",
    "\n",
    "Remember to include `%%time` at the beginning of the cell so that it measures the time it takes for the cell to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in clean_lst:\n",
    "    index_page(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Perform the page indexing in parallel and note the difference in performance.\n",
    "\n",
    "Remember to include `%%time` at the beginning of the cell so that it measures the time it takes for the cell to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pathos\n",
      "  Using cached pathos-0.2.6.zip (219 kB)\n",
      "Collecting ppft>=1.6.6.2\n",
      "  Using cached ppft-1.6.6.2.zip (106 kB)\n",
      "Collecting dill>=0.3.2\n",
      "  Using cached dill-0.3.2.zip (177 kB)\n",
      "Collecting pox>=0.2.8\n",
      "  Using cached pox-0.2.8.zip (128 kB)\n",
      "Collecting multiprocess>=0.70.10\n",
      "  Using cached multiprocess-0.70.10.zip (2.4 MB)\n",
      "Requirement already satisfied: six>=1.7.3 in c:\\users\\omarm\\appdata\\roaming\\python\\python38\\site-packages (from ppft>=1.6.6.2->pathos) (1.15.0)\n",
      "Building wheels for collected packages: pathos, ppft, dill, pox, multiprocess\n",
      "  Building wheel for pathos (setup.py): started\n",
      "  Building wheel for pathos (setup.py): finished with status 'done'\n",
      "  Created wheel for pathos: filename=pathos-0.2.6-py3-none-any.whl size=77746 sha256=a72a6155d9c466a7cc4a99404dc44eb6801d428cc9a950643aa678af3d669414\n",
      "  Stored in directory: c:\\users\\omarm\\appdata\\local\\pip\\cache\\wheels\\83\\12\\43\\a7b335eac30d213e8cbe768bff7456ef4bc76351769864dd79\n",
      "  Building wheel for ppft (setup.py): started\n",
      "  Building wheel for ppft (setup.py): finished with status 'done'\n",
      "  Created wheel for ppft: filename=ppft-1.6.6.2-py3-none-any.whl size=64749 sha256=dcb9f468c41335459435f24a7aa450dc2f5545d5da81aaa5372dea1bc76607ec\n",
      "  Stored in directory: c:\\users\\omarm\\appdata\\local\\pip\\cache\\wheels\\32\\6c\\df\\4335813452c1ff652a854b78093302f0caf2359f49f3417fb4\n",
      "  Building wheel for dill (setup.py): started\n",
      "  Building wheel for dill (setup.py): finished with status 'done'\n",
      "  Created wheel for dill: filename=dill-0.3.2-py3-none-any.whl size=78977 sha256=ac772e75daec5f037bb1e66c706bb520d78e082b7d22bab0c9e468e4399019c4\n",
      "  Stored in directory: c:\\users\\omarm\\appdata\\local\\pip\\cache\\wheels\\93\\7f\\7d\\78ec535a4340ef2696aad8b17fe8bb063d56301bd62881b069\n",
      "  Building wheel for pox (setup.py): started\n",
      "  Building wheel for pox (setup.py): finished with status 'done'\n",
      "  Created wheel for pox: filename=pox-0.2.8-py3-none-any.whl size=28343 sha256=70db04a1ff643c420a9ac2918fbeedd9c2a956e539df36ab3b272ebdf078f9b1\n",
      "  Stored in directory: c:\\users\\omarm\\appdata\\local\\pip\\cache\\wheels\\f2\\af\\b4\\36b9ce8f3b271919abb82400d1548f534578420ab5ef36fb20\n",
      "  Building wheel for multiprocess (setup.py): started\n",
      "  Building wheel for multiprocess (setup.py): finished with status 'done'\n",
      "  Created wheel for multiprocess: filename=multiprocess-0.70.10-py3-none-any.whl size=125843 sha256=02d793bb273af12cd761568ee471df6d5f74e3b7dfbf2d6ac18751b4df18cde8\n",
      "  Stored in directory: c:\\users\\omarm\\appdata\\local\\pip\\cache\\wheels\\f4\\c7\\82\\0bcdc3506e6a8365963db24f477b81693438d4b9165c0757b7\n",
      "Successfully built pathos ppft dill pox multiprocess\n",
      "Installing collected packages: ppft, dill, pox, multiprocess, pathos\n",
      "Successfully installed dill-0.3.2 multiprocess-0.70.10 pathos-0.2.6 pox-0.2.8 ppft-1.6.6.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pathos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from pathos.multiprocessing import ProcessingPool as Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 110 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cores = mp.cpu_count()\n",
    "pool = Pool(cores)\n",
    "res = pool.map(index_page, clean_lst)\n",
    "pool.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
