<!DOCTYPE html>

<html class="client-nojs" dir="ltr" lang="en">
<head>
<meta charset="utf-8"/>
<title>Autoencoder - Wikipedia</title>
<script>document.documentElement.className="client-js";RLCONF={"wgBreakFrames":!1,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"1e6c08b7-9b68-44b6-ae37-58fbfa6567b2","wgCSPNonce":!1,"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":!1,"wgNamespaceNumber":0,"wgPageName":"Autoencoder","wgTitle":"Autoencoder","wgCurRevisionId":973684205,"wgRevisionId":973684205,"wgArticleId":6836612,"wgIsArticle":!0,"wgIsRedirect":!1,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["CS1 errors: missing periodical","Use dmy dates from March 2020","Articles to be split from May 2020","All articles to be split","Artificial neural networks","Unsupervised learning","Dimension reduction"],"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"Autoencoder",
"wgRelevantArticleId":6836612,"wgIsProbablyEditable":!0,"wgRelevantPageIsProbablyEditable":!0,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgMediaViewerOnClick":!0,"wgMediaViewerEnabledByDefault":!0,"wgPopupsReferencePreviews":!1,"wgPopupsConflictsWithNavPopupGadget":!1,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":!0,"nearby":!0,"watchlist":!0,"tagline":!1},"wgWMESchemaEditAttemptStepOversample":!1,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralAuthMobileDomain":!1,"wgEditSubmitButtonLabelPublish":!0,"wgULSPosition":"interlanguage","wgWikibaseItemId":"Q786435"};RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"loading","ext.cite.styles":"ready","ext.math.styles":"ready","skins.vector.styles.legacy":"ready",
"jquery.makeCollapsible.styles":"ready","mediawiki.toc.styles":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","wikibase.client.init":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements","ext.math.scripts","site","mediawiki.page.startup","mediawiki.page.ready","jquery.makeCollapsible","mediawiki.toc","skins.vector.legacy.js","ext.gadget.ReferenceTooltips","ext.gadget.charinsert","ext.gadget.extra-toolbar-buttons","ext.gadget.refToolbar","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.compactlinks","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.options@1hzgi",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});});</script>
<link href="/w/load.php?lang=en&modules=ext.cite.styles%7Cext.math.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cjquery.makeCollapsible.styles%7Cmediawiki.toc.styles%7Cskins.vector.styles.legacy%7Cwikibase.client.init&only=styles&skin=vector" rel="stylesheet"/>
<script async="" src="/w/load.php?lang=en&modules=startup&only=scripts&raw=1&skin=vector"></script>
<meta content="" name="ResourceLoaderDynamicStyles"/>
<link href="/w/load.php?lang=en&modules=site.styles&only=styles&skin=vector" rel="stylesheet"/>
<meta content="MediaWiki 1.36.0-wmf.4" name="generator"/>
<meta content="origin" name="referrer"/>
<meta content="origin-when-crossorigin" name="referrer"/>
<meta content="origin-when-cross-origin" name="referrer"/>
<link href="/w/index.php?title=Autoencoder&action=edit" rel="alternate" title="Edit this page" type="application/x-wiki"/>
<link href="/w/index.php?title=Autoencoder&action=edit" rel="edit" title="Edit this page"/>
<link href="/static/apple-touch/wikipedia.png" rel="apple-touch-icon"/>
<link href="/static/favicon/wikipedia.ico" rel="shortcut icon"/>
<link href="/w/opensearch_desc.php" rel="search" title="Wikipedia (en)" type="application/opensearchdescription+xml"/>
<link href="//en.wikipedia.org/w/api.php?action=rsd" rel="EditURI" type="application/rsd+xml"/>
<link href="//creativecommons.org/licenses/by-sa/3.0/" rel="license"/>
<link href="/w/index.php?title=Special:RecentChanges&feed=atom" rel="alternate" title="Wikipedia Atom feed" type="application/atom+xml"/>
<link href="https://en.wikipedia.org/wiki/Autoencoder" rel="canonical"/>
<link href="//login.wikimedia.org" rel="dns-prefetch"/>
<link href="//meta.wikimedia.org" rel="dns-prefetch"/>
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Autoencoder rootpage-Autoencoder skin-vector action-view skin-vector-legacy"><div class="noprint" id="mw-page-base"></div>
<div class="noprint" id="mw-head-base"></div>
<div class="mw-body" id="content" role="main">
<a id="top"></a>
<div class="mw-body-content" id="siteNotice"><!-- CentralNotice --></div>
<div class="mw-indicators mw-body-content">
</div>
<h1 class="firstHeading" id="firstHeading" lang="en">Autoencoder</h1>
<div class="mw-body-content" id="bodyContent">
<div class="noprint" id="siteSub">From Wikipedia, the free encyclopedia</div>
<div id="contentSub"></div>
<div id="contentSub2"></div>
<div id="jump-to-nav"></div>
<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
<a class="mw-jump-link" href="#searchInput">Jump to search</a>
<div class="mw-content-ltr" dir="ltr" id="mw-content-text" lang="en"><div class="mw-parser-output"><div class="hatnote navigation-not-searchable" role="note">Not to be confused with <a href="/wiki/Autocoder" title="Autocoder">Autocoder</a> or <a href="/wiki/Autocode" title="Autocode">Autocode</a>.</div>
<p class="mw-empty-elt">
</p>
<table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:22.0em;margin:0 0 1.0em 1.0em;background:#f8f9fa;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%"><tbody><tr><td style="padding-top:0.4em;line-height:1.2em">Part of a series on</td></tr><tr><th style="padding:0.2em 0.4em 0.2em;padding-top:0;font-size:145%;line-height:1.2em"><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a><br/>and<br/><a href="/wiki/Data_mining" title="Data mining">data mining</a></th></tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Problems</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>
<li><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>
<li><a href="/wiki/Automated_machine_learning" title="Automated machine learning">AutoML</a></li>
<li><a href="/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>
<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>
<li><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>
<li><a href="/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></li>
<li><a href="/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>
<li><a href="/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li>
<li><a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">Semi-supervised learning</a></li>
<li><a href="/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li>
<li><a href="/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></li>
<li><a href="/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><div style="display:inline-block; padding:0.1em 0;line-height:1.2em;"><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br/><style data-mw-deduplicate="TemplateStyles:r886047488">.mw-parser-output .nobold{font-weight:normal}</style><span class="nobold"><span style="font-size:85%;">(<b><a href="/wiki/Statistical_classification" title="Statistical classification">classification</a></b> • <b><a href="/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</span></span> </div></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>
<li><a href="/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a>
<ul><li><a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a></li>
<li><a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a></li>
<li><a href="/wiki/Random_forest" title="Random forest">Random forest</a></li></ul></li>
<li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>
<li><a href="/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>
<li><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>
<li><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural networks</a></li>
<li><a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>
<li><a href="/wiki/Perceptron" title="Perceptron">Perceptron</a></li>
<li><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li>
<li><a class="mw-redirect" href="/wiki/Support-vector_machine" title="Support-vector machine">Support vector machine (SVM)</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/BIRCH" title="BIRCH">BIRCH</a></li>
<li><a class="mw-redirect" href="/wiki/CURE_data_clustering_algorithm" title="CURE data clustering algorithm">CURE</a></li>
<li><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li>
<li><a href="/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means</a></li>
<li><a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation–maximization algorithm">Expectation–maximization (EM)</a></li>
<li><br/><a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>
<li><a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>
<li><a class="mw-redirect" href="/wiki/Mean-shift" title="Mean-shift">Mean-shift</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>
<li><a href="/wiki/Canonical_correlation" title="Canonical correlation">CCA</a></li>
<li><a href="/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>
<li><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>
<li><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>
<li><a href="/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>
<li><a href="/wiki/Proper_generalized_decomposition" title="Proper generalized decomposition">PGD</a></li>
<li><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Graphical_model" title="Graphical model">Graphical models</a>
<ul><li><a href="/wiki/Bayesian_network" title="Bayesian network">Bayes net</a></li>
<li><a href="/wiki/Conditional_random_field" title="Conditional random field">Conditional random field</a></li>
<li><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov</a></li></ul></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a class="mw-redirect" href="/wiki/K-nearest_neighbors_classification" title="K-nearest neighbors classification"><i>k</i>-NN</a></li>
<li><a href="/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural network</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a class="mw-selflink selflink">Autoencoder</a></li>
<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="/wiki/DeepDream" title="DeepDream">DeepDream</a></li>
<li><a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron</a></li>
<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">RNN</a>
<ul><li><a href="/wiki/Long_short-term_memory" title="Long short-term memory">LSTM</a></li>
<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">GRU</a></li></ul></li>
<li><a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted Boltzmann machine</a></li>
<li><a href="/wiki/Generative_adversarial_network" title="Generative adversarial network">GAN</a></li>
<li><a href="/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>
<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a>
<ul><li><a href="/wiki/U-Net" title="U-Net">U-Net</a></li></ul></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>
<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">SARSA</a></li>
<li><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference (TD)</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Theory</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a class="mw-redirect" href="/wiki/Bias%E2%80%93variance_dilemma" title="Bias–variance dilemma">Bias–variance dilemma</a></li>
<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>
<li><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>
<li><a href="/wiki/Occam_learning" title="Occam learning">Occam learning</a></li>
<li><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>
<li><a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>
<li><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik–Chervonenkis theory">VC theory</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Machine-learning venues</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NeurIPS</a></li>
<li><a href="/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a></li>
<li><a href="/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">ML</a></li>
<li><a href="/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">JMLR</a></li>
<li><a class="external text" href="https://arxiv.org/list/cs.LG/recent" rel="nofollow">ArXiv:cs.LG</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Related articles</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/List_of_datasets_for_machine-learning_research" title="List of datasets for machine-learning research">List of datasets for machine-learning research</a></li>
<li><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">Outline of machine learning</a></li></ul>
</div></div></div></td>
</tr><tr><td style="text-align:right;font-size:115%;padding-top: 0.6em;"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Machine_learning_bar" title="Template:Machine learning bar"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Machine_learning_bar" title="Template talk:Machine learning bar"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&action=edit"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>
<p>An <b>autoencoder</b> is a type of <a href="/wiki/Artificial_neural_network" title="Artificial neural network">artificial neural network</a> used to learn <a href="/wiki/Feature_learning" title="Feature learning">efficient data codings</a> in an <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised</a> manner.<sup class="reference" id="cite_ref-1"><a href="#cite_note-1">[1]</a></sup> The aim of an autoencoder is to learn a <a href="/wiki/Feature_learning" title="Feature learning">representation</a> (encoding) for a set of data, typically for <a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">dimensionality reduction</a>, by training the network to ignore signal “noise”. Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name. Several variants exist to the basic model, with the aim of forcing the learned representations of the input to assume useful properties.<sup class="reference" id="cite_ref-:0_2-0"><a href="#cite_note-:0-2">[2]</a></sup> Examples are the regularized autoencoders (<i>Sparse</i>, <i>Denoising</i> and <i>Contractive</i> autoencoders), proven effective in learning representations for subsequent classification tasks,<sup class="reference" id="cite_ref-:4_3-0"><a href="#cite_note-:4-3">[3]</a></sup> and <i>Variational</i> autoencoders, with their recent applications as generative models.<sup class="reference" id="cite_ref-:11_4-0"><a href="#cite_note-:11-4">[4]</a></sup> Autoencoders are effectively used for solving many applied problems, from <a class="mw-redirect" href="/wiki/Face_recognition" title="Face recognition">face recognition</a><sup class="reference" id="cite_ref-5"><a href="#cite_note-5">[5]</a></sup> to acquiring the semantic meaning of words.<sup class="reference" id="cite_ref-6"><a href="#cite_note-6">[6]</a></sup><sup class="reference" id="cite_ref-7"><a href="#cite_note-7">[7]</a></sup>
</p>
<style data-mw-deduplicate="TemplateStyles:r886046785">.mw-parser-output .toclimit-2 .toclevel-1 ul,.mw-parser-output .toclimit-3 .toclevel-2 ul,.mw-parser-output .toclimit-4 .toclevel-3 ul,.mw-parser-output .toclimit-5 .toclevel-4 ul,.mw-parser-output .toclimit-6 .toclevel-5 ul,.mw-parser-output .toclimit-7 .toclevel-6 ul{display:none}</style><div class="toclimit-3"><div aria-labelledby="mw-toc-heading" class="toc" id="toc" role="navigation"><input class="toctogglecheckbox" id="toctogglecheckbox" role="button" style="display:none" type="checkbox"/><div class="toctitle" dir="ltr" lang="en"><h2 id="mw-toc-heading">Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Introduction"><span class="tocnumber">1</span> <span class="toctext">Introduction</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Basic_Architecture"><span class="tocnumber">2</span> <span class="toctext">Basic Architecture</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="#Variations"><span class="tocnumber">3</span> <span class="toctext">Variations</span></a>
<ul>
<li class="toclevel-2 tocsection-4"><a href="#Regularized_Autoencoders"><span class="tocnumber">3.1</span> <span class="toctext">Regularized Autoencoders</span></a>
<ul>
<li class="toclevel-3 tocsection-5"><a href="#Sparse_autoencoder_(SAE)"><span class="tocnumber">3.1.1</span> <span class="toctext">Sparse autoencoder (SAE)</span></a></li>
<li class="toclevel-3 tocsection-6"><a href="#Denoising_autoencoder_(DAE)"><span class="tocnumber">3.1.2</span> <span class="toctext">Denoising autoencoder (DAE)</span></a></li>
<li class="toclevel-3 tocsection-7"><a href="#Contractive_autoencoder_(CAE)"><span class="tocnumber">3.1.3</span> <span class="toctext">Contractive autoencoder (CAE)</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-8"><a href="#Variational_autoencoder_(VAE)"><span class="tocnumber">3.2</span> <span class="toctext">Variational autoencoder (VAE)</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-9"><a href="#Advantages_of_Depth"><span class="tocnumber">4</span> <span class="toctext">Advantages of Depth</span></a>
<ul>
<li class="toclevel-2 tocsection-10"><a href="#Training_Deep_Architectures"><span class="tocnumber">4.1</span> <span class="toctext">Training Deep Architectures</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-11"><a href="#Applications"><span class="tocnumber">5</span> <span class="toctext">Applications</span></a>
<ul>
<li class="toclevel-2 tocsection-12"><a href="#Dimensionality_Reduction"><span class="tocnumber">5.1</span> <span class="toctext">Dimensionality Reduction</span></a>
<ul>
<li class="toclevel-3 tocsection-13"><a href="#Relationship_with_principal_component_analysis_(PCA)"><span class="tocnumber">5.1.1</span> <span class="toctext">Relationship with principal component analysis (PCA)</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-14"><a href="#Information_Retrieval"><span class="tocnumber">5.2</span> <span class="toctext">Information Retrieval</span></a></li>
<li class="toclevel-2 tocsection-15"><a href="#Anomaly_Detection"><span class="tocnumber">5.3</span> <span class="toctext">Anomaly Detection</span></a></li>
<li class="toclevel-2 tocsection-16"><a href="#Image_Processing"><span class="tocnumber">5.4</span> <span class="toctext">Image Processing</span></a></li>
<li class="toclevel-2 tocsection-17"><a href="#Drug_discovery"><span class="tocnumber">5.5</span> <span class="toctext">Drug discovery</span></a></li>
<li class="toclevel-2 tocsection-18"><a href="#Population_synthesis"><span class="tocnumber">5.6</span> <span class="toctext">Population synthesis</span></a></li>
<li class="toclevel-2 tocsection-19"><a href="#Popularity_prediction"><span class="tocnumber">5.7</span> <span class="toctext">Popularity prediction</span></a></li>
<li class="toclevel-2 tocsection-20"><a href="#Machine_Translation"><span class="tocnumber">5.8</span> <span class="toctext">Machine Translation</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-21"><a href="#See_also"><span class="tocnumber">6</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-22"><a href="#References"><span class="tocnumber">7</span> <span class="toctext">References</span></a></li>
</ul>
</div>
</div>
<h2><span class="mw-headline" id="Introduction">Introduction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&action=edit&section=1" title="Edit section: Introduction">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>An <i>autoencoder</i>  is a <a href="/wiki/Neural_network" title="Neural network">neural network</a> that learns to copy its input to its output. It has an internal (<i>hidden</i>) layer that describes a <i>code</i> used to represent the input, and it is constituted by two main parts: an encoder that maps the input into the code, and a decoder that maps the code to a reconstruction of the original input.
</p><p>Performing the copying task perfectly would simply duplicate the signal, and this is why autoencoders usually are restricted in ways that force them to reconstruct the input approximately, preserving only the most relevant aspects of the data in the copy.
</p><p>The idea of autoencoders has been popular in the field of neural networks for decades, and the first applications date back to the '80s.<sup class="reference" id="cite_ref-:0_2-1"><a href="#cite_note-:0-2">[2]</a></sup><sup class="reference" id="cite_ref-8"><a href="#cite_note-8">[8]</a></sup><sup class="reference" id="cite_ref-9"><a href="#cite_note-9">[9]</a></sup> Their most traditional application was <a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">dimensionality reduction</a> or <a href="/wiki/Feature_learning" title="Feature learning">feature learning</a>, but more recently the autoencoder concept has become more widely used for learning <a href="/wiki/Generative_model" title="Generative model">generative models</a> of data.<sup class="reference" id="cite_ref-VAE_10-0"><a href="#cite_note-VAE-10">[10]</a></sup><sup class="reference" id="cite_ref-gan_faces_11-0"><a href="#cite_note-gan_faces-11">[11]</a></sup> Some of the most powerful <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">AIs</a> in the 2010s involved sparse autoencoders stacked inside of <a href="/wiki/Deep_learning" title="Deep learning">deep</a> neural networks.<sup class="reference" id="cite_ref-domingos_12-0"><a href="#cite_note-domingos-12">[12]</a></sup>
</p>
<h2><span class="mw-headline" id="Basic_Architecture">Basic Architecture</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&action=edit&section=2" title="Edit section: Basic Architecture">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="thumb tright"><div class="thumbinner" style="width:222px;"><a class="image" href="/wiki/File:Autoencoder_schema.png"><img alt="" class="thumbimage" data-file-height="765" data-file-width="841" decoding="async" height="200" src="//upload.wikimedia.org/wikipedia/commons/thumb/3/37/Autoencoder_schema.png/220px-Autoencoder_schema.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/3/37/Autoencoder_schema.png/330px-Autoencoder_schema.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/3/37/Autoencoder_schema.png/440px-Autoencoder_schema.png 2x" width="220"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:Autoencoder_schema.png" title="Enlarge"></a></div>Schema of a basic Autoencoder</div></div></div><p>The simplest form of an autoencoder is a <a href="/wiki/Feedforward_neural_network" title="Feedforward neural network">feedforward</a>, non-<a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">recurrent neural network</a> similar to single layer perceptrons that participate in <a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">multilayer perceptrons</a> (MLP) – having an input layer, an output layer and one or more hidden layers connecting them – where the output layer has the same number of nodes (neurons) as the input layer, and with the purpose of reconstructing its inputs (minimizing the difference between the input and the output) instead of predicting the target value <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle Y}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>Y</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle Y}</annotation>
</semantics>
</math></span><img alt="Y" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/961d67d6b454b4df2301ac571808a3538b3a6d3f" style="vertical-align: -0.171ex; width:1.773ex; height:2.009ex;"/></span> given inputs <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle X}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>X</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle X}</annotation>
</semantics>
</math></span><img alt="X" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/68baa052181f707c662844a465bfeeb135e82bab" style="vertical-align: -0.338ex; width:1.98ex; height:2.176ex;"/></span>. Therefore, autoencoders are <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised learning</a> models (do not require labeled inputs to enable learning).
</p><p>An autoencoder consists of two parts, the encoder and the decoder, which can be defined as transitions <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \phi }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>ϕ<!-- ϕ --></mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \phi }</annotation>
</semantics>
</math></span><img alt="\phi " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/72b1f30316670aee6270a28334bdf4f5072cdde4" style="vertical-align: -0.671ex; width:1.385ex; height:2.509ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \psi ,}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>ψ<!-- ψ --></mi>
<mo>,</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \psi ,}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \psi ,}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/bfc4043b55bade492740e58cba74198873db1464" style="vertical-align: -0.671ex; width:2.16ex; height:2.509ex;"/></span> such that:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \phi :{\mathcal {X}}\rightarrow {\mathcal {F}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>ϕ<!-- ϕ --></mi>
<mo>:</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">X</mi>
</mrow>
</mrow>
<mo stretchy="false">→<!-- → --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">F</mi>
</mrow>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \phi :{\mathcal {X}}\rightarrow {\mathcal {F}}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \phi :{\mathcal {X}}\rightarrow {\mathcal {F}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/af15e24dd1ebaa221aa2be25d304f58de027135d" style="vertical-align: -0.671ex; width:10.739ex; height:2.509ex;"/></span></dd>
<dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \psi :{\mathcal {F}}\rightarrow {\mathcal {X}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>ψ<!-- ψ --></mi>
<mo>:</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">F</mi>
</mrow>
</mrow>
<mo stretchy="false">→<!-- → --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">X</mi>
</mrow>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \psi :{\mathcal {F}}\rightarrow {\mathcal {X}}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \psi :{\mathcal {F}}\rightarrow {\mathcal {X}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9b3812977cdf1ef5a7c522cae067fd186ac9cddd" style="vertical-align: -0.671ex; width:10.866ex; height:2.509ex;"/></span></dd>
<dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \phi ,\psi ={\underset {\phi ,\psi }{\operatorname {arg\,min} }}\,\|X-(\psi \circ \phi )X\|^{2}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>ϕ<!-- ϕ --></mi>
<mo>,</mo>
<mi>ψ<!-- ψ --></mi>
<mo>=</mo>
<mrow class="MJX-TeXAtom-ORD">
<munder>
<mrow class="MJX-TeXAtom-OP MJX-fixedlimits">
<mi mathvariant="normal">a</mi>
<mi mathvariant="normal">r</mi>
<mi mathvariant="normal">g</mi>
<mspace width="thinmathspace"></mspace>
<mi mathvariant="normal">m</mi>
<mi mathvariant="normal">i</mi>
<mi mathvariant="normal">n</mi>
</mrow>
<mrow>
<mi>ϕ<!-- ϕ --></mi>
<mo>,</mo>
<mi>ψ<!-- ψ --></mi>
</mrow>
</munder>
</mrow>
<mspace width="thinmathspace"></mspace>
<mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
<mi>X</mi>
<mo>−<!-- − --></mo>
<mo stretchy="false">(</mo>
<mi>ψ<!-- ψ --></mi>
<mo>∘<!-- ∘ --></mo>
<mi>ϕ<!-- ϕ --></mi>
<mo stretchy="false">)</mo>
<mi>X</mi>
<msup>
<mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \phi ,\psi ={\underset {\phi ,\psi }{\operatorname {arg\,min} }}\,\|X-(\psi \circ \phi )X\|^{2}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \phi ,\psi ={\underset {\phi ,\psi }{\operatorname {arg\,min} }}\,\|X-(\psi \circ \phi )X\|^{2}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4fa7f6a4ca888943ff470cbf76fb18e64815f831" style="vertical-align: -2.838ex; width:31.999ex; height:5.176ex;"/></span></dd></dl>
<p>In the simplest case, given one hidden layer, the encoder stage of an autoencoder takes the input <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbf {x} \in \mathbb {R} ^{d}={\mathcal {X}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>∈<!-- ∈ --></mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">R</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>d</mi>
</mrow>
</msup>
<mo>=</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">X</mi>
</mrow>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbf {x} \in \mathbb {R} ^{d}={\mathcal {X}}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \mathbf {x} \in \mathbb {R} ^{d}={\mathcal {X}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/70a238822b434f8555744fcb5ce2b4e028e47003" style="vertical-align: -0.338ex; width:11.996ex; height:2.676ex;"/></span> and maps it to <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbf {h} \in \mathbb {R} ^{p}={\mathcal {F}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">h</mi>
</mrow>
<mo>∈<!-- ∈ --></mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">R</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>p</mi>
</mrow>
</msup>
<mo>=</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">F</mi>
</mrow>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbf {h} \in \mathbb {R} ^{p}={\mathcal {F}}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \mathbf {h} \in \mathbb {R} ^{p}={\mathcal {F}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7a81ee279be26e05f46922c0cad8ab49630a78a3" style="vertical-align: -0.338ex; width:12.088ex; height:2.343ex;"/></span>:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbf {h} =\sigma (\mathbf {Wx} +\mathbf {b} )}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">h</mi>
</mrow>
<mo>=</mo>
<mi>σ<!-- σ --></mi>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">W</mi>
<mi mathvariant="bold">x</mi>
</mrow>
<mo>+</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">b</mi>
</mrow>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbf {h} =\sigma (\mathbf {Wx} +\mathbf {b} )}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \mathbf {h} =\sigma (\mathbf {Wx} +\mathbf {b} )}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d4783741ac8b0a41279ab2bd6575f736e73ec85f" style="vertical-align: -0.838ex; width:16.222ex; height:2.843ex;"/></span></dd></dl>
<p>This image <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbf {h} }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">h</mi>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbf {h} }</annotation>
</semantics>
</math></span><img alt="{\mathbf  {h}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1fce1c8de3a01ae39379db83781850619c4c0987" style="vertical-align: -0.338ex; width:1.485ex; height:2.176ex;"/></span> is usually referred to as <i>code</i>, <i>latent variables</i>, or <i>latent representation</i>. Here, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \sigma }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>σ<!-- σ --></mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \sigma }</annotation>
</semantics>
</math></span><img alt="\sigma " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/59f59b7c3e6fdb1d0365a494b81fb9a696138c36" style="vertical-align: -0.338ex; width:1.33ex; height:1.676ex;"/></span> is an element-wise <a href="/wiki/Activation_function" title="Activation function">activation function</a> such as a <a href="/wiki/Sigmoid_function" title="Sigmoid function">sigmoid function</a> or a <a class="mw-redirect" href="/wiki/Rectified_linear_unit" title="Rectified linear unit">rectified linear unit</a>.  <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbf {W} }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">W</mi>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbf {W} }</annotation>
</semantics>
</math></span><img alt="\mathbf {W} " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/04749f1e87cca59c094da23c79cc64b085b0df12" style="vertical-align: -0.338ex; width:2.763ex; height:2.176ex;"/></span> is a weight matrix and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbf {b} }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">b</mi>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbf {b} }</annotation>
</semantics>
</math></span><img alt="\mathbf {b} " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/13ebf4628a1adf07133a6009e4a78bdd990c6eb9" style="vertical-align: -0.338ex; width:1.485ex; height:2.176ex;"/></span> is a bias vector. Weights and biases are usually initialized randomly, and then updated iteratively during training through <a href="/wiki/Backpropagation" title="Backpropagation">Backpropagation</a>. After that, the decoder stage of the autoencoder maps <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbf {h} }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">h</mi>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbf {h} }</annotation>
</semantics>
</math></span><img alt="{\mathbf  {h}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1fce1c8de3a01ae39379db83781850619c4c0987" style="vertical-align: -0.338ex; width:1.485ex; height:2.176ex;"/></span> to the reconstruction <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbf {x'} }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<msup>
<mi mathvariant="bold">x</mi>
<mo>′</mo>
</msup>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbf {x'} }</annotation>
</semantics>
</math></span><img alt="\mathbf {x'} " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7d14ab6186e99346cb608a30858c3e1580f760e6" style="vertical-align: -0.338ex; width:2.096ex; height:2.509ex;"/></span> of the same shape as <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbf {x} }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbf {x} }</annotation>
</semantics>
</math></span><img alt="\mathbf {x} " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/32adf004df5eb0a8c7fd8c0b6b7405183c5a5ef2" style="vertical-align: -0.338ex; width:1.411ex; height:1.676ex;"/></span>:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbf {x'} =\sigma '(\mathbf {W'h} +\mathbf {b'} )}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<msup>
<mi mathvariant="bold">x</mi>
<mo>′</mo>
</msup>
</mrow>
<mo>=</mo>
<msup>
<mi>σ<!-- σ --></mi>
<mo>′</mo>
</msup>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<msup>
<mi mathvariant="bold">W</mi>
<mo>′</mo>
</msup>
<mi mathvariant="bold">h</mi>
</mrow>
<mo>+</mo>
<mrow class="MJX-TeXAtom-ORD">
<msup>
<mi mathvariant="bold">b</mi>
<mo>′</mo>
</msup>
</mrow>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbf {x'} =\sigma '(\mathbf {W'h} +\mathbf {b'} )}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \mathbf {x'} =\sigma '(\mathbf {W'h} +\mathbf {b'} )}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7e42a81660c4283b01fc43f1a6c4a33dc1557d03" style="vertical-align: -0.838ex; width:18.962ex; height:3.009ex;"/></span></dd></dl>
<p>where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbf {\sigma '} ,\mathbf {W'} ,{\text{ and }}\mathbf {b'} }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<msup>
<mi>σ<!-- σ --></mi>
<mo>′</mo>
</msup>
</mrow>
<mo>,</mo>
<mrow class="MJX-TeXAtom-ORD">
<msup>
<mi mathvariant="bold">W</mi>
<mo>′</mo>
</msup>
</mrow>
<mo>,</mo>
<mrow class="MJX-TeXAtom-ORD">
<mtext> and </mtext>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<msup>
<mi mathvariant="bold">b</mi>
<mo>′</mo>
</msup>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbf {\sigma '} ,\mathbf {W'} ,{\text{ and }}\mathbf {b'} }</annotation>
</semantics>
</math></span><img alt="{\displaystyle \mathbf {\sigma '} ,\mathbf {W'} ,{\text{ and }}\mathbf {b'} }" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/062a0023b06f960a4c9d005bbe283818d5946d33" style="vertical-align: -0.671ex; width:14.609ex; height:2.843ex;"/></span> for the decoder may be unrelated to the corresponding <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbf {\sigma } ,\mathbf {W} ,{\text{ and }}\mathbf {b} }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi>σ<!-- σ --></mi>
</mrow>
<mo>,</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">W</mi>
</mrow>
<mo>,</mo>
<mrow class="MJX-TeXAtom-ORD">
<mtext> and </mtext>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">b</mi>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbf {\sigma } ,\mathbf {W} ,{\text{ and }}\mathbf {b} }</annotation>
</semantics>
</math></span><img alt="{\displaystyle \mathbf {\sigma } ,\mathbf {W} ,{\text{ and }}\mathbf {b} }" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2c5d41529f76819f274a8466c0fbfdb937649f31" style="vertical-align: -0.671ex; width:12.554ex; height:2.509ex;"/></span> for the encoder.
</p><p>Autoencoders are trained to minimise reconstruction errors (such as <a href="/wiki/Mean_squared_error" title="Mean squared error">squared errors</a>), often referred to as the "<a href="/wiki/Loss_function" title="Loss function">loss</a>":
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\mathcal {L}}(\mathbf {x} ,\mathbf {x'} )=\|\mathbf {x} -\mathbf {x'} \|^{2}=\|\mathbf {x} -\sigma '(\mathbf {W'} (\sigma (\mathbf {Wx} +\mathbf {b} ))+\mathbf {b'} )\|^{2}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">L</mi>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<mrow class="MJX-TeXAtom-ORD">
<msup>
<mi mathvariant="bold">x</mi>
<mo>′</mo>
</msup>
</mrow>
<mo stretchy="false">)</mo>
<mo>=</mo>
<mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>−<!-- − --></mo>
<mrow class="MJX-TeXAtom-ORD">
<msup>
<mi mathvariant="bold">x</mi>
<mo>′</mo>
</msup>
</mrow>
<msup>
<mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo>=</mo>
<mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>−<!-- − --></mo>
<msup>
<mi>σ<!-- σ --></mi>
<mo>′</mo>
</msup>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<msup>
<mi mathvariant="bold">W</mi>
<mo>′</mo>
</msup>
</mrow>
<mo stretchy="false">(</mo>
<mi>σ<!-- σ --></mi>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">W</mi>
<mi mathvariant="bold">x</mi>
</mrow>
<mo>+</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">b</mi>
</mrow>
<mo stretchy="false">)</mo>
<mo stretchy="false">)</mo>
<mo>+</mo>
<mrow class="MJX-TeXAtom-ORD">
<msup>
<mi mathvariant="bold">b</mi>
<mo>′</mo>
</msup>
</mrow>
<mo stretchy="false">)</mo>
<msup>
<mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\mathcal {L}}(\mathbf {x} ,\mathbf {x'} )=\|\mathbf {x} -\mathbf {x'} \|^{2}=\|\mathbf {x} -\sigma '(\mathbf {W'} (\sigma (\mathbf {Wx} +\mathbf {b} ))+\mathbf {b'} )\|^{2}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\mathcal {L}}(\mathbf {x} ,\mathbf {x'} )=\|\mathbf {x} -\mathbf {x'} \|^{2}=\|\mathbf {x} -\sigma '(\mathbf {W'} (\sigma (\mathbf {Wx} +\mathbf {b} ))+\mathbf {b'} )\|^{2}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2c9d074db8e3d0e19f9f5128edcc26a2e7baad36" style="vertical-align: -0.838ex; width:57.237ex; height:3.176ex;"/></span></dd></dl>
<p>where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbf {x} }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbf {x} }</annotation>
</semantics>
</math></span><img alt="\mathbf {x} " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/32adf004df5eb0a8c7fd8c0b6b7405183c5a5ef2" style="vertical-align: -0.338ex; width:1.411ex; height:1.676ex;"/></span> is usually averaged over some input training set.
</p><p>As mentioned before, the training of an autoencoder is performed through <a href="/wiki/Backpropagation" title="Backpropagation">Backpropagation of the error</a>, just like a regular <a href="/wiki/Feedforward_neural_network" title="Feedforward neural network">feedforward neural network</a>.
</p><p>Should the <a href="/wiki/Feature_(machine_learning)" title="Feature (machine learning)">feature space</a> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\mathcal {F}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">F</mi>
</mrow>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\mathcal {F}}}</annotation>
</semantics>
</math></span><img alt="{\mathcal {F}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/205d4b91000d9dcf1a5bbabdfa6a8395fa60b676" style="vertical-align: -0.338ex; width:1.927ex; height:2.176ex;"/></span> have lower dimensionality than the input space <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\mathcal {X}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">X</mi>
</mrow>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\mathcal {X}}}</annotation>
</semantics>
</math></span><img alt="{\mathcal {X}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8c7e5461c5286852df4ef652fca7e4b0b63030e9" style="vertical-align: -0.338ex; width:1.875ex; height:2.176ex;"/></span>, the feature vector <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \phi (x)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>ϕ<!-- ϕ --></mi>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \phi (x)}</annotation>
</semantics>
</math></span><img alt="\phi (x)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/546b660b2f3cfb5f34be7b3ed8371d54f5c74227" style="vertical-align: -0.838ex; width:4.524ex; height:2.843ex;"/></span> can be regarded as a <a href="/wiki/Data_compression" title="Data compression">compressed</a> representation of the input <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle x}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>x</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle x}</annotation>
</semantics>
</math></span><img alt="x" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/87f9e315fd7e2ba406057a97300593c4802b53e4" style="vertical-align: -0.338ex; width:1.33ex; height:1.676ex;"/></span>. This is the case of <i>undercomplete</i> autoencoders. If the hidden layers are larger than (<i>overcomplete autoencoders)</i>, or equal to, the input layer, or the hidden units are given enough capacity, an autoencoder can potentially learn the <a href="/wiki/Identity_function" title="Identity function">identity function</a> and become useless. However, experimental results have shown that autoencoders might still <a href="/wiki/Feature_learning" title="Feature learning">learn useful features</a> in these cases.<sup class="reference" id="cite_ref-bengio_13-0"><a href="#cite_note-bengio-13">[13]</a></sup> In the ideal setting, one should be able to tailor the code dimension and the model capacity on the basis of the complexity of the data distribution to be modeled. One way to do so, is to exploit the model variants known as <i>Regularized Autoencoders</i>.<sup class="reference" id="cite_ref-:0_2-2"><a href="#cite_note-:0-2">[2]</a></sup>
</p>
<h2><span class="mw-headline" id="Variations">Variations</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&action=edit&section=3" title="Edit section: Variations">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Regularized_Autoencoders">Regularized Autoencoders</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&action=edit&section=4" title="Edit section: Regularized Autoencoders">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Various techniques exist to prevent autoencoders from learning the identity function and to improve their ability to capture important information and learn richer representations.
</p>
<h4><span id="Sparse_autoencoder_.28SAE.29"></span><span class="mw-headline" id="Sparse_autoencoder_(SAE)">Sparse autoencoder (SAE)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&action=edit&section=5" title="Edit section: Sparse autoencoder (SAE)">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<div class="thumb tright"><div class="thumbinner" style="width:222px;"><a class="image" href="/wiki/File:Autoencoder_sparso.png"><img alt="" class="thumbimage" data-file-height="550" data-file-width="442" decoding="async" height="274" src="//upload.wikimedia.org/wikipedia/commons/thumb/8/83/Autoencoder_sparso.png/220px-Autoencoder_sparso.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/8/83/Autoencoder_sparso.png/330px-Autoencoder_sparso.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/8/83/Autoencoder_sparso.png/440px-Autoencoder_sparso.png 2x" width="220"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:Autoencoder_sparso.png" title="Enlarge"></a></div>Simple schema of a single-layer sparse autoencoder. The hidden nodes in bright yellow are activated, while the light yellow ones are inactive. The activation depends on the input.</div></div></div>
<p>Recently, it has been observed that when <a class="mw-redirect" href="/wiki/Representation_learning" title="Representation learning">representations</a> are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks.<sup class="reference" id="cite_ref-:5_14-0"><a href="#cite_note-:5-14">[14]</a></sup> Sparse autoencoder may include more (rather than fewer) hidden units than inputs, but only a small number of the hidden units are allowed to be active at once.<sup class="reference" id="cite_ref-domingos_12-1"><a href="#cite_note-domingos-12">[12]</a></sup> This sparsity constraint forces the model to respond to the unique statistical features of the input data used for training.
</p><p>Specifically, a sparse autoencoder is an autoencoder whose training criterion involves a sparsity penalty <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \Omega ({\boldsymbol {h}})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi mathvariant="normal">Ω<!-- Ω --></mi>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold-italic">h</mi>
</mrow>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \Omega ({\boldsymbol {h}})}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \Omega ({\boldsymbol {h}})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/661a44fc4576ee0c3a37f17fb19b7ed34b5b336e" style="vertical-align: -0.838ex; width:5.04ex; height:2.843ex;"/></span> on the code layer <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\boldsymbol {h}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold-italic">h</mi>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {h}}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\boldsymbol {h}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/256342564b5fa949c4ec4c906b709422750982c8" style="vertical-align: -0.338ex; width:1.553ex; height:2.176ex;"/></span>.
</p><p><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\mathcal {L}}(\mathbf {x} ,\mathbf {x'} )+\Omega ({\boldsymbol {h}})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">L</mi>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<mrow class="MJX-TeXAtom-ORD">
<msup>
<mi mathvariant="bold">x</mi>
<mo>′</mo>
</msup>
</mrow>
<mo stretchy="false">)</mo>
<mo>+</mo>
<mi mathvariant="normal">Ω<!-- Ω --></mi>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold-italic">h</mi>
</mrow>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\mathcal {L}}(\mathbf {x} ,\mathbf {x'} )+\Omega ({\boldsymbol {h}})}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\mathcal {L}}(\mathbf {x} ,\mathbf {x'} )+\Omega ({\boldsymbol {h}})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fdaffbe9096d05eb9213604329dc2648fe482a0f" style="vertical-align: -0.838ex; width:15.834ex; height:3.009ex;"/></span>
</p><p>Recalling that <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\boldsymbol {h}}=f({\boldsymbol {W}}{\boldsymbol {x}}+{\boldsymbol {b}})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold-italic">h</mi>
</mrow>
<mo>=</mo>
<mi>f</mi>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold-italic">W</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold-italic">x</mi>
</mrow>
<mo>+</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold-italic">b</mi>
</mrow>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {h}}=f({\boldsymbol {W}}{\boldsymbol {x}}+{\boldsymbol {b}})}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\boldsymbol {h}}=f({\boldsymbol {W}}{\boldsymbol {x}}+{\boldsymbol {b}})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1a3766c038fda93dca0b38053100544f5c5a5587" style="vertical-align: -0.838ex; width:16.127ex; height:2.843ex;"/></span>, the penalty encourages the model to activate (i.e. output value close to 1) some specific areas of the network on the basis of the input data, while forcing all other neurons to be inactive (i.e. to have an output value close to 0).<sup class="reference" id="cite_ref-:6_15-0"><a href="#cite_note-:6-15">[15]</a></sup>
</p><p>This sparsity of activation can be achieved by formulating the penalty terms in different ways.
</p>
<ul><li>One way to do it, is by exploiting the <a href="/wiki/Kullback%E2%80%93Leibler_divergence" title="Kullback–Leibler divergence">Kullback-Leibler (KL) divergence</a>.<sup class="reference" id="cite_ref-:5_14-1"><a href="#cite_note-:5-14">[14]</a></sup><sup class="reference" id="cite_ref-:6_15-1"><a href="#cite_note-:6-15">[15]</a></sup><sup class="reference" id="cite_ref-16"><a href="#cite_note-16">[16]</a></sup><sup class="reference" id="cite_ref-17"><a href="#cite_note-17">[17]</a></sup>  Let</li></ul>
<p><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\hat {\rho _{j}}}={\frac {1}{m}}\sum _{i=1}^{m}[h_{j}(x_{i})]}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<msub>
<mi>ρ<!-- ρ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo>=</mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>1</mn>
<mi>m</mi>
</mfrac>
</mrow>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>m</mi>
</mrow>
</munderover>
<mo stretchy="false">[</mo>
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
<mo stretchy="false">]</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\hat {\rho _{j}}}={\frac {1}{m}}\sum _{i=1}^{m}[h_{j}(x_{i})]}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\hat {\rho _{j}}}={\frac {1}{m}}\sum _{i=1}^{m}[h_{j}(x_{i})]}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/582c2f9744cfcb64919ae703ac67aaed149972c4" style="vertical-align: -3.005ex; width:19.31ex; height:6.843ex;"/></span>
</p><p>be the average activation of the hidden unit <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle j}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>j</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle j}</annotation>
</semantics>
</math></span><img alt="j" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:0.985ex; height:2.509ex;"/></span> (averaged over the  <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle m}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>m</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle m}</annotation>
</semantics>
</math></span><img alt="m" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0a07d98bb302f3856cbabc47b2b9016692e3f7bc" style="vertical-align: -0.338ex; width:2.04ex; height:1.676ex;"/></span> training examples). Note that the notation <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle h_{j}(x_{i})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle h_{j}(x_{i})}</annotation>
</semantics>
</math></span><img alt="{\displaystyle h_{j}(x_{i})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fba55a2af839ba242476fb89dd24989f391f01ad" style="vertical-align: -1.005ex; width:6.187ex; height:3.009ex;"/></span> makes explicit what the input affecting the activation was, i.e. it identifies which input value the activation is function of. To encourage most of the neurons to be inactive, we would like <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\hat {\rho _{j}}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<msub>
<mi>ρ<!-- ρ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\hat {\rho _{j}}}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\hat {\rho _{j}}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1d476e1c65d425573658c5cd8dd92d7da854b236" style="vertical-align: -1.005ex; width:2.112ex; height:2.843ex;"/></span> to be as close to 0 as possible. Therefore, this method enforces the constraint <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\hat {\rho _{j}}}=\rho }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<msub>
<mi>ρ<!-- ρ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo>=</mo>
<mi>ρ<!-- ρ --></mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\hat {\rho _{j}}}=\rho }</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\hat {\rho _{j}}}=\rho }" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a3edbf61bbc005c4eef1b36d8398edd543e37d72" style="vertical-align: -1.005ex; width:6.412ex; height:2.843ex;"/></span>  where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \rho }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>ρ<!-- ρ --></mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \rho }</annotation>
</semantics>
</math></span><img alt="\rho " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1f7d439671d1289b6a816e6af7a304be40608d64" style="vertical-align: -0.838ex; width:1.202ex; height:2.176ex;"/></span> is the sparsity parameter, a value close to zero, leading the activation of the hidden units to be mostly zero as well. The penalty term <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \Omega ({\boldsymbol {h}})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi mathvariant="normal">Ω<!-- Ω --></mi>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold-italic">h</mi>
</mrow>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \Omega ({\boldsymbol {h}})}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \Omega ({\boldsymbol {h}})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/661a44fc4576ee0c3a37f17fb19b7ed34b5b336e" style="vertical-align: -0.838ex; width:5.04ex; height:2.843ex;"/></span> will then take a form that penalizes <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\hat {\rho _{j}}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<msub>
<mi>ρ<!-- ρ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\hat {\rho _{j}}}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\hat {\rho _{j}}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1d476e1c65d425573658c5cd8dd92d7da854b236" style="vertical-align: -1.005ex; width:2.112ex; height:2.843ex;"/></span> for deviating significantly from <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \rho }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>ρ<!-- ρ --></mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \rho }</annotation>
</semantics>
</math></span><img alt="\rho " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1f7d439671d1289b6a816e6af7a304be40608d64" style="vertical-align: -0.838ex; width:1.202ex; height:2.176ex;"/></span>, exploiting the KL divergence:
</p><p><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \sum _{j=1}^{s}KL(\rho ||{\hat {\rho _{j}}})=\sum _{j=1}^{s}\left[\rho \log {\frac {\rho }{\hat {\rho _{j}}}}+(1-\rho )\log {\frac {1-\rho }{1-{\hat {\rho _{j}}}}}\right]}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>s</mi>
</mrow>
</munderover>
<mi>K</mi>
<mi>L</mi>
<mo stretchy="false">(</mo>
<mi>ρ<!-- ρ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<msub>
<mi>ρ<!-- ρ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">)</mo>
<mo>=</mo>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>s</mi>
</mrow>
</munderover>
<mrow>
<mo>[</mo>
<mrow>
<mi>ρ<!-- ρ --></mi>
<mi>log</mi>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mi>ρ<!-- ρ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mover>
<msub>
<mi>ρ<!-- ρ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mfrac>
</mrow>
<mo>+</mo>
<mo stretchy="false">(</mo>
<mn>1</mn>
<mo>−<!-- − --></mo>
<mi>ρ<!-- ρ --></mi>
<mo stretchy="false">)</mo>
<mi>log</mi>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mrow>
<mn>1</mn>
<mo>−<!-- − --></mo>
<mi>ρ<!-- ρ --></mi>
</mrow>
<mrow>
<mn>1</mn>
<mo>−<!-- − --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<msub>
<mi>ρ<!-- ρ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
</mrow>
</mfrac>
</mrow>
</mrow>
<mo>]</mo>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \sum _{j=1}^{s}KL(\rho ||{\hat {\rho _{j}}})=\sum _{j=1}^{s}\left[\rho \log {\frac {\rho }{\hat {\rho _{j}}}}+(1-\rho )\log {\frac {1-\rho }{1-{\hat {\rho _{j}}}}}\right]}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \sum _{j=1}^{s}KL(\rho ||{\hat {\rho _{j}}})=\sum _{j=1}^{s}\left[\rho \log {\frac {\rho }{\hat {\rho _{j}}}}+(1-\rho )\log {\frac {1-\rho }{1-{\hat {\rho _{j}}}}}\right]}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/93bdf538fae80657148ec8b5f919daf16d3ecb5b" style="vertical-align: -3.338ex; width:51.55ex; height:7.176ex;"/></span>  where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle j}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>j</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle j}</annotation>
</semantics>
</math></span><img alt="j" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:0.985ex; height:2.509ex;"/></span> is summing over the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle s}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>s</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle s}</annotation>
</semantics>
</math></span><img alt="s" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/01d131dfd7673938b947072a13a9744fe997e632" style="vertical-align: -0.338ex; width:1.09ex; height:1.676ex;"/></span> hidden nodes in the hidden layer, and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle KL(\rho ||{\hat {\rho _{j}}})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>K</mi>
<mi>L</mi>
<mo stretchy="false">(</mo>
<mi>ρ<!-- ρ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<msub>
<mi>ρ<!-- ρ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle KL(\rho ||{\hat {\rho _{j}}})}</annotation>
</semantics>
</math></span><img alt="{\displaystyle KL(\rho ||{\hat {\rho _{j}}})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/250518385ae70f6a35100c7fb7fcc8e004ba1068" style="vertical-align: -1.005ex; width:10.065ex; height:3.009ex;"/></span> is the KL-divergence between a Bernoulli random variable with mean <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \rho }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>ρ<!-- ρ --></mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \rho }</annotation>
</semantics>
</math></span><img alt="\rho " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1f7d439671d1289b6a816e6af7a304be40608d64" style="vertical-align: -0.838ex; width:1.202ex; height:2.176ex;"/></span> and a Bernoulli random variable with mean <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\hat {\rho _{j}}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<msub>
<mi>ρ<!-- ρ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\hat {\rho _{j}}}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\hat {\rho _{j}}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1d476e1c65d425573658c5cd8dd92d7da854b236" style="vertical-align: -1.005ex; width:2.112ex; height:2.843ex;"/></span>.<sup class="reference" id="cite_ref-:6_15-2"><a href="#cite_note-:6-15">[15]</a></sup>
</p>
<ul><li>Another way to achieve sparsity in the activation of the hidden unit, is by applying L1 or L2 regularization terms on the activation, scaled by a certain parameter <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \lambda }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>λ<!-- λ --></mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \lambda }</annotation>
</semantics>
</math></span><img alt="\lambda " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b43d0ea3c9c025af1be9128e62a18fa74bedda2a" style="vertical-align: -0.338ex; width:1.355ex; height:2.176ex;"/></span>.<sup class="reference" id="cite_ref-18"><a href="#cite_note-18">[18]</a></sup> For instance, in the case of L1 the <a href="/wiki/Loss_function" title="Loss function">loss function</a> would become</li></ul>
<p><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\mathcal {L}}(\mathbf {x} ,\mathbf {x'} )+\lambda \sum _{i}|h_{i}|}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">L</mi>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<mrow class="MJX-TeXAtom-ORD">
<msup>
<mi mathvariant="bold">x</mi>
<mo>′</mo>
</msup>
</mrow>
<mo stretchy="false">)</mo>
<mo>+</mo>
<mi>λ<!-- λ --></mi>
<munder>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</munder>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\mathcal {L}}(\mathbf {x} ,\mathbf {x'} )+\lambda \sum _{i}|h_{i}|}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\mathcal {L}}(\mathbf {x} ,\mathbf {x'} )+\lambda \sum _{i}|h_{i}|}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0c10c34c80609ffb3969034231bcec57657fbf3f" style="vertical-align: -3.005ex; width:19.711ex; height:5.509ex;"/></span>
</p>
<ul><li>A further proposed strategy to force sparsity in the model is that of manually zeroing all but the strongest hidden unit activations (<i><a class="new" href="/w/index.php?title=K-sparse_autoencoder&action=edit&redlink=1" title="K-sparse autoencoder (page does not exist)">k-sparse autoencoder</a></i>).<sup class="reference" id="cite_ref-:1_19-0"><a href="#cite_note-:1-19">[19]</a></sup> The k-sparse autoencoder is based on a linear autoencoder (i.e. with linear activation function) and tied weights. The identification of the strongest activations can be achieved by sorting the activities and keeping only the first <i>k</i> values, or by using ReLU hidden units with thresholds that are adaptively adjusted until the k largest activities are identified. This selection acts like the previously mentioned regularization terms in that it prevents the model from reconstructing the input using too many neurons.<sup class="reference" id="cite_ref-:1_19-1"><a href="#cite_note-:1-19">[19]</a></sup></li></ul>
<h4><span id="Denoising_autoencoder_.28DAE.29"></span><span class="mw-headline" id="Denoising_autoencoder_(DAE)">Denoising autoencoder (DAE)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&action=edit&section=6" title="Edit section: Denoising autoencoder (DAE)">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Differently from sparse autoencoders or undercomplete autoencoders that constrain representation, <a class="new" href="/w/index.php?title=Denoising_autoencoders&action=edit&redlink=1" title="Denoising autoencoders (page does not exist)">Denoising autoencoders</a> (DAE) try to achieve a <i>good</i> representation by changing the <i>reconstruction criterion</i>.<sup class="reference" id="cite_ref-:0_2-3"><a href="#cite_note-:0-2">[2]</a></sup>
</p><p>Indeed, DAEs take a partially <b>corrupted input</b> and are trained to recover the original <i>undistorted</i> <i>input</i>.  In practice, the objective of denoising autoencoders is that of cleaning the corrupted input, or <i>denoising.</i> Two underlying assumptions are inherent to this approach:
</p>
<ul><li>Higher level representations are relatively stable and robust to the corruption of the input;</li>
<li>To perform denoising well, the model needs to extract features that capture useful structure in the distribution of the input.<sup class="reference" id="cite_ref-:4_3-1"><a href="#cite_note-:4-3">[3]</a></sup></li></ul>
<p>In other words, denoising is advocated as a training criterion for learning to extract useful features that will constitute better higher level representations of the input.<sup class="reference" id="cite_ref-:4_3-2"><a href="#cite_note-:4-3">[3]</a></sup>
</p><p>The training process of a DAE works as follows:
</p>
<ul><li>The initial input <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle x}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>x</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle x}</annotation>
</semantics>
</math></span><img alt="x" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/87f9e315fd7e2ba406057a97300593c4802b53e4" style="vertical-align: -0.338ex; width:1.33ex; height:1.676ex;"/></span> is corrupted into <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\boldsymbol {\tilde {x}}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi mathvariant="bold-italic">x</mi>
<mo mathvariant="bold" stretchy="false">~<!-- ~ --></mo>
</mover>
</mrow>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {\tilde {x}}}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\boldsymbol {\tilde {x}}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ccbe6c6a519b23654d7b4c3807282b4287bbdd47" style="vertical-align: -0.338ex; width:1.532ex; height:2.176ex;"/></span> through stochastic mapping <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\boldsymbol {\tilde {x}}}\thicksim q_{D}({\boldsymbol {\tilde {x}}}|{\boldsymbol {x}})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi mathvariant="bold-italic">x</mi>
<mo mathvariant="bold" stretchy="false">~<!-- ~ --></mo>
</mover>
</mrow>
</mrow>
<mo class="MJX-variant">∼<!-- ∼ --></mo>
<msub>
<mi>q</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>D</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi mathvariant="bold-italic">x</mi>
<mo mathvariant="bold" stretchy="false">~<!-- ~ --></mo>
</mover>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold-italic">x</mi>
</mrow>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {\tilde {x}}}\thicksim q_{D}({\boldsymbol {\tilde {x}}}|{\boldsymbol {x}})}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\boldsymbol {\tilde {x}}}\thicksim q_{D}({\boldsymbol {\tilde {x}}}|{\boldsymbol {x}})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/142363ae58513427315bebcdeab55a1f5e0bf629" style="vertical-align: -0.838ex; width:12.78ex; height:2.843ex;"/></span>.</li>
<li>The corrupted input <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\boldsymbol {\tilde {x}}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi mathvariant="bold-italic">x</mi>
<mo mathvariant="bold" stretchy="false">~<!-- ~ --></mo>
</mover>
</mrow>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {\tilde {x}}}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\boldsymbol {\tilde {x}}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ccbe6c6a519b23654d7b4c3807282b4287bbdd47" style="vertical-align: -0.338ex; width:1.532ex; height:2.176ex;"/></span> is then mapped to a hidden representation with the same process of the standard autoencoder, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\boldsymbol {h}}=f_{\theta }({\boldsymbol {\tilde {x}}})=s({\boldsymbol {W}}{\boldsymbol {\tilde {x}}}+{\boldsymbol {b}})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold-italic">h</mi>
</mrow>
<mo>=</mo>
<msub>
<mi>f</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>θ<!-- θ --></mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi mathvariant="bold-italic">x</mi>
<mo mathvariant="bold" stretchy="false">~<!-- ~ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">)</mo>
<mo>=</mo>
<mi>s</mi>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold-italic">W</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi mathvariant="bold-italic">x</mi>
<mo mathvariant="bold" stretchy="false">~<!-- ~ --></mo>
</mover>
</mrow>
</mrow>
<mo>+</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold-italic">b</mi>
</mrow>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {h}}=f_{\theta }({\boldsymbol {\tilde {x}}})=s({\boldsymbol {W}}{\boldsymbol {\tilde {x}}}+{\boldsymbol {b}})}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\boldsymbol {h}}=f_{\theta }({\boldsymbol {\tilde {x}}})=s({\boldsymbol {W}}{\boldsymbol {\tilde {x}}}+{\boldsymbol {b}})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/856b6f6efecae1718023ec6ac467c574309ba9fc" style="vertical-align: -0.838ex; width:24.521ex; height:2.843ex;"/></span>.</li>
<li>From the hidden representation the model reconstructs <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\boldsymbol {z}}=g_{\theta '}({\boldsymbol {h}})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold-italic">z</mi>
</mrow>
<mo>=</mo>
<msub>
<mi>g</mi>
<mrow class="MJX-TeXAtom-ORD">
<msup>
<mi>θ<!-- θ --></mi>
<mo>′</mo>
</msup>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold-italic">h</mi>
</mrow>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {z}}=g_{\theta '}({\boldsymbol {h}})}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\boldsymbol {z}}=g_{\theta '}({\boldsymbol {h}})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1d9d76c0ea4a82fc202091f0480565cee17d189b" style="vertical-align: -0.838ex; width:10.395ex; height:2.843ex;"/></span>.<sup class="reference" id="cite_ref-:4_3-3"><a href="#cite_note-:4-3">[3]</a></sup></li></ul>
<p>The model's parameters <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \theta }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>θ<!-- θ --></mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \theta }</annotation>
</semantics>
</math></span><img alt="\theta " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6e5ab2664b422d53eb0c7df3b87e1360d75ad9af" style="vertical-align: -0.338ex; width:1.09ex; height:2.176ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \theta '}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msup>
<mi>θ<!-- θ --></mi>
<mo>′</mo>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \theta '}</annotation>
</semantics>
</math></span><img alt="\theta '" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c02a68dba2972e9cf4faf9656f58da0df8be6ae9" style="vertical-align: -0.338ex; width:1.775ex; height:2.509ex;"/></span> are trained to minimize the average reconstruction error over the training data, specifically, minimizing the difference between <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\boldsymbol {z}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold-italic">z</mi>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {z}}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\boldsymbol {z}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/bfd80493f0fd8b383e270b7b5297fc11ef2459c0" style="vertical-align: -0.338ex; width:1.29ex; height:1.676ex;"/></span> and the original uncorrupted input  <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\boldsymbol {x}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold-italic">x</mi>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {x}}}</annotation>
</semantics>
</math></span><img alt="{\boldsymbol {x}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/606b7680d510560a505937143775ea80fa958051" style="vertical-align: -0.338ex; width:1.532ex; height:1.676ex;"/></span>.<sup class="reference" id="cite_ref-:4_3-4"><a href="#cite_note-:4-3">[3]</a></sup> Note that each time a random example <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\boldsymbol {x}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold-italic">x</mi>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {x}}}</annotation>
</semantics>
</math></span><img alt="{\boldsymbol {x}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/606b7680d510560a505937143775ea80fa958051" style="vertical-align: -0.338ex; width:1.532ex; height:1.676ex;"/></span> is presented to the model, a new corrupted version is generated stochastically on the basis of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle q_{D}({\boldsymbol {\tilde {x}}}|{\boldsymbol {x}})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>q</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>D</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi mathvariant="bold-italic">x</mi>
<mo mathvariant="bold" stretchy="false">~<!-- ~ --></mo>
</mover>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold-italic">x</mi>
</mrow>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle q_{D}({\boldsymbol {\tilde {x}}}|{\boldsymbol {x}})}</annotation>
</semantics>
</math></span><img alt="{\displaystyle q_{D}({\boldsymbol {\tilde {x}}}|{\boldsymbol {x}})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c602ee45112476b6e9aba3c8be437e5298b82bad" style="vertical-align: -0.838ex; width:8.15ex; height:2.843ex;"/></span>.
</p><p>The above-mentioned training process could be developed with any kind of corruption process. Some examples might be <i>additive isotropic Gaussian noise, Masking noise</i> (a fraction of the input chosen at random for each example is forced to 0) or <i>Salt-and-pepper noise</i> (a fraction of the input chosen at random for each example is set to its minimum or maximum value with uniform probability).<sup class="reference" id="cite_ref-:4_3-5"><a href="#cite_note-:4-3">[3]</a></sup>
</p><p>Finally, notice that the corruption of the input is performed only during the training phase of the DAE. Once the model has learnt the optimal parameters, in order to extract the representations from the original data no corruption is added.
</p>
<h4><span id="Contractive_autoencoder_.28CAE.29"></span><span class="mw-headline" id="Contractive_autoencoder_(CAE)">Contractive autoencoder (CAE)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&action=edit&section=7" title="Edit section: Contractive autoencoder (CAE)">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Contractive autoencoder adds an explicit regularizer in their objective function that forces the model to learn a function that is robust to slight variations of input values. This regularizer corresponds to the <a class="mw-redirect" href="/wiki/Frobenius_norm" title="Frobenius norm">Frobenius norm</a> of the <a href="/wiki/Jacobian_matrix_and_determinant" title="Jacobian matrix and determinant">Jacobian matrix</a> of the encoder activations with respect to the input. Since the penalty is applied to training examples only, this term forces the model to learn useful information about the training distribution. The final objective function has the following form:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\mathcal {L}}(\mathbf {x} ,\mathbf {x'} )+\lambda \sum _{i}||\nabla _{x}h_{i}||^{2}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">L</mi>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<mrow class="MJX-TeXAtom-ORD">
<msup>
<mi mathvariant="bold">x</mi>
<mo>′</mo>
</msup>
</mrow>
<mo stretchy="false">)</mo>
<mo>+</mo>
<mi>λ<!-- λ --></mi>
<munder>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</munder>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<msub>
<mi mathvariant="normal">∇<!-- ∇ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>x</mi>
</mrow>
</msub>
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\mathcal {L}}(\mathbf {x} ,\mathbf {x'} )+\lambda \sum _{i}||\nabla _{x}h_{i}||^{2}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\mathcal {L}}(\mathbf {x} ,\mathbf {x'} )+\lambda \sum _{i}||\nabla _{x}h_{i}||^{2}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8c969944dbfe7ccfb757e5430f98f1492d101282" style="vertical-align: -3.005ex; width:25.167ex; height:5.509ex;"/></span></dd></dl>
<p>The name contractive comes from the fact that the CAE is encouraged to map a neighborhood of input points to a smaller neighborhood of output points.<sup class="reference" id="cite_ref-:0_2-4"><a href="#cite_note-:0-2">[2]</a></sup>
</p><p>There is a connection between the denoising autoencoder (DAE) and the contractive autoencoder (CAE): in the limit of small Gaussian input noise, DAE make the reconstruction function resist small but finite-sized perturbations of the input, while CAE make the extracted features resist infinitesimal perturbations of the input.
</p>
<h3><span id="Variational_autoencoder_.28VAE.29"></span><span class="mw-headline" id="Variational_autoencoder_(VAE)">Variational autoencoder (VAE)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&action=edit&section=8" title="Edit section: Variational autoencoder (VAE)">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<table class="plainlinks metadata ambox ambox-move" role="presentation"><tbody><tr><td class="mbox-image"><div style="width:52px"><img alt="" data-file-height="20" data-file-width="60" decoding="async" height="17" src="//upload.wikimedia.org/wikipedia/commons/thumb/a/a7/Split-arrows.svg/50px-Split-arrows.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/a/a7/Split-arrows.svg/75px-Split-arrows.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/a/a7/Split-arrows.svg/100px-Split-arrows.svg.png 2x" width="50"/></div></td><td class="mbox-text"><div class="mbox-text-span">It has been suggested that this section be <a href="/wiki/Wikipedia:Splitting" title="Wikipedia:Splitting">split</a> out  into another article titled <i><a class="mw-redirect" href="/wiki/Variational_autoencoder" title="Variational autoencoder">Variational autoencoder</a></i>. (<a href="/wiki/Talk:Autoencoder#Split_proposed" title="Talk:Autoencoder">Discuss</a>) <small><i>(May 2020)</i></small></div></td></tr></tbody></table>
<p>Unlike classical (sparse, denoising, etc.) autoencoders, Variational autoencoders (VAEs) are <a href="/wiki/Generative_model" title="Generative model">generative models</a>, like <a href="/wiki/Generative_adversarial_network" title="Generative adversarial network">Generative Adversarial Networks</a>.<sup class="reference" id="cite_ref-:2_20-0"><a href="#cite_note-:2-20">[20]</a></sup> Their association with this group of models derives mainly from the architectural affinity with the basic autoencoder (the final training objective has an encoder and a decoder), but their mathematical formulation differs significantly.<sup class="reference" id="cite_ref-21"><a href="#cite_note-21">[21]</a></sup> VAEs are <a class="new" href="/w/index.php?title=Directed_probabilistic_graphical_model&action=edit&redlink=1" title="Directed probabilistic graphical model (page does not exist)">directed probabilistic graphical models</a> (DPGM) whose posterior is approximated by a neural network, forming an autoencoder-like architecture.<sup class="reference" id="cite_ref-:2_20-1"><a href="#cite_note-:2-20">[20]</a></sup><sup class="reference" id="cite_ref-1bitVAE_22-0"><a href="#cite_note-1bitVAE-22">[22]</a></sup> Differently from discriminative modeling that aims to learn a predictor given the observation, <i>generative modeling</i> tries to simulate how the data is generated, in order to understand the underlying causal relations. Causal relations have indeed the great potential of being generalizable.<sup class="reference" id="cite_ref-:11_4-1"><a href="#cite_note-:11-4">[4]</a></sup>
</p><p>Variational autoencoder models make strong assumptions concerning the distribution of <i>latent variables</i>. They use a <a href="/wiki/Variational_Bayesian_methods" title="Variational Bayesian methods">variational approach</a> for latent representation learning, which results in an additional loss component and a specific estimator for the training algorithm called the Stochastic Gradient Variational Bayes (SGVB) estimator.<sup class="reference" id="cite_ref-VAE_10-1"><a href="#cite_note-VAE-10">[10]</a></sup> It assumes that the data is generated by a directed <a href="/wiki/Graphical_model" title="Graphical model">graphical model</a> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle p_{\theta }(\mathbf {x} |\mathbf {h} )}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>p</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>θ<!-- θ --></mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">h</mi>
</mrow>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle p_{\theta }(\mathbf {x} |\mathbf {h} )}</annotation>
</semantics>
</math></span><img alt="{\displaystyle p_{\theta }(\mathbf {x} |\mathbf {h} )}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/82b54decede578e28c567007d4abfb0ca44685d6" style="vertical-align: -0.838ex; margin-left: -0.089ex; width:7.615ex; height:2.843ex;"/></span> and that the encoder is learning an approximation <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle q_{\phi }(\mathbf {h} |\mathbf {x} )}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>q</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>ϕ<!-- ϕ --></mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">h</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle q_{\phi }(\mathbf {h} |\mathbf {x} )}</annotation>
</semantics>
</math></span><img alt="{\displaystyle q_{\phi }(\mathbf {h} |\mathbf {x} )}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5b2a578f2622a6760cc7c128951543b18443ab05" style="vertical-align: -1.005ex; width:7.601ex; height:3.009ex;"/></span> to the <a href="/wiki/Posterior_probability" title="Posterior probability">posterior distribution</a> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle p_{\theta }(\mathbf {h} |\mathbf {x} )}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>p</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>θ<!-- θ --></mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">h</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle p_{\theta }(\mathbf {h} |\mathbf {x} )}</annotation>
</semantics>
</math></span><img alt="{\displaystyle p_{\theta }(\mathbf {h} |\mathbf {x} )}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6f73cc03b77cbec91ee0de9514de0bbdff8a6b4b" style="vertical-align: -0.838ex; margin-left: -0.089ex; width:7.615ex; height:2.843ex;"/></span> where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbf {\phi } }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi>ϕ<!-- ϕ --></mi>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbf {\phi } }</annotation>
</semantics>
</math></span><img alt="{\mathbf  {\phi }}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/03b16c8b97a15009afffb576b1424e184f48f6b3" style="vertical-align: -0.671ex; width:1.385ex; height:2.509ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbf {\theta } }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi>θ<!-- θ --></mi>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbf {\theta } }</annotation>
</semantics>
</math></span><img alt="\mathbf {\theta } " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9813b67e5416572ea1d10056c50d380742924a78" style="vertical-align: -0.338ex; width:1.09ex; height:2.176ex;"/></span> denote the parameters of the encoder (recognition model) and decoder (generative model) respectively. The probability distribution of the latent vector of a VAE typically matches that of the training data much closer than a standard autoencoder. The objective of VAE has the following form:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\mathcal {L}}(\mathbf {\phi } ,\mathbf {\theta } ,\mathbf {x} )=D_{\mathrm {KL} }(q_{\phi }(\mathbf {h} |\mathbf {x} )\Vert p_{\theta }(\mathbf {h} ))-\mathbb {E} _{q_{\phi }(\mathbf {h} |\mathbf {x} )}{\big (}\log p_{\theta }(\mathbf {x} |\mathbf {h} ){\big )}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">L</mi>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>ϕ<!-- ϕ --></mi>
</mrow>
<mo>,</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>θ<!-- θ --></mi>
</mrow>
<mo>,</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo stretchy="false">)</mo>
<mo>=</mo>
<msub>
<mi>D</mi>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="normal">K</mi>
<mi mathvariant="normal">L</mi>
</mrow>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>q</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>ϕ<!-- ϕ --></mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">h</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo stretchy="false">)</mo>
<mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
<msub>
<mi>p</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>θ<!-- θ --></mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">h</mi>
</mrow>
<mo stretchy="false">)</mo>
<mo stretchy="false">)</mo>
<mo>−<!-- − --></mo>
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">E</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<msub>
<mi>q</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>ϕ<!-- ϕ --></mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">h</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
</msub>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">(</mo>
</mrow>
</mrow>
<mi>log</mi>
<mo>⁡<!-- ⁡ --></mo>
<msub>
<mi>p</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>θ<!-- θ --></mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">h</mi>
</mrow>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">)</mo>
</mrow>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\mathcal {L}}(\mathbf {\phi } ,\mathbf {\theta } ,\mathbf {x} )=D_{\mathrm {KL} }(q_{\phi }(\mathbf {h} |\mathbf {x} )\Vert p_{\theta }(\mathbf {h} ))-\mathbb {E} _{q_{\phi }(\mathbf {h} |\mathbf {x} )}{\big (}\log p_{\theta }(\mathbf {x} |\mathbf {h} ){\big )}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\mathcal {L}}(\mathbf {\phi } ,\mathbf {\theta } ,\mathbf {x} )=D_{\mathrm {KL} }(q_{\phi }(\mathbf {h} |\mathbf {x} )\Vert p_{\theta }(\mathbf {h} ))-\mathbb {E} _{q_{\phi }(\mathbf {h} |\mathbf {x} )}{\big (}\log p_{\theta }(\mathbf {x} |\mathbf {h} ){\big )}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/db82e47614f6f6eb005b373264a06977a0bc98cb" style="vertical-align: -1.338ex; width:56.471ex; height:3.509ex;"/></span></dd></dl>
<p>Here, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle D_{\mathrm {KL} }}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>D</mi>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="normal">K</mi>
<mi mathvariant="normal">L</mi>
</mrow>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle D_{\mathrm {KL} }}</annotation>
</semantics>
</math></span><img alt="D_{{{\mathrm  {KL}}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2ab26e8799fe9a2b74e4c49da80bb18eaff04da0" style="vertical-align: -0.671ex; width:4.462ex; height:2.509ex;"/></span> stands for the <a href="/wiki/Kullback%E2%80%93Leibler_divergence" title="Kullback–Leibler divergence">Kullback–Leibler divergence</a>. The prior over the latent variables is usually set to be the centred isotropic multivariate <a href="/wiki/Gaussian_function" title="Gaussian function">Gaussian</a> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle p_{\theta }(\mathbf {h} )={\mathcal {N}}(\mathbf {0,I} )}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>p</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>θ<!-- θ --></mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">h</mi>
</mrow>
<mo stretchy="false">)</mo>
<mo>=</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">N</mi>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mn mathvariant="bold">0</mn>
<mo mathvariant="bold">,</mo>
<mi mathvariant="bold">I</mi>
</mrow>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle p_{\theta }(\mathbf {h} )={\mathcal {N}}(\mathbf {0,I} )}</annotation>
</semantics>
</math></span><img alt="{\displaystyle p_{\theta }(\mathbf {h} )={\mathcal {N}}(\mathbf {0,I} )}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/827b3608fe76050960d96d41b2eb1b22bc6d004b" style="vertical-align: -0.838ex; margin-left: -0.089ex; width:16.219ex; height:3.009ex;"/></span>; however, alternative configurations have been considered.<sup class="reference" id="cite_ref-23"><a href="#cite_note-23">[23]</a></sup>
</p><p>Commonly, the shape of the variational and the likelihood distributions are chosen such that they are factorized Gaussians:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\begin{aligned}q_{\phi }(\mathbf {h} |\mathbf {x} )&={\mathcal {N}}({\boldsymbol {\rho }}(\mathbf {x} ),{\boldsymbol {\omega }}^{2}(\mathbf {x} )\mathbf {I} ),\\p_{\theta }(\mathbf {x} |\mathbf {h} )&={\mathcal {N}}({\boldsymbol {\mu }}(\mathbf {h} ),{\boldsymbol {\sigma }}^{2}(\mathbf {h} )\mathbf {I} ),\end{aligned}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt">
<mtr>
<mtd>
<msub>
<mi>q</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>ϕ<!-- ϕ --></mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">h</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo stretchy="false">)</mo>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">N</mi>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold-italic">ρ<!-- ρ --></mi>
</mrow>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo stretchy="false">)</mo>
<mo>,</mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold-italic">ω<!-- ω --></mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">I</mi>
</mrow>
<mo stretchy="false">)</mo>
<mo>,</mo>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mi>p</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>θ<!-- θ --></mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">h</mi>
</mrow>
<mo stretchy="false">)</mo>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">N</mi>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold-italic">μ<!-- μ --></mi>
</mrow>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">h</mi>
</mrow>
<mo stretchy="false">)</mo>
<mo>,</mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold-italic">σ<!-- σ --></mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">h</mi>
</mrow>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">I</mi>
</mrow>
<mo stretchy="false">)</mo>
<mo>,</mo>
</mtd>
</mtr>
</mtable>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\begin{aligned}q_{\phi }(\mathbf {h} |\mathbf {x} )&={\mathcal {N}}({\boldsymbol {\rho }}(\mathbf {x} ),{\boldsymbol {\omega }}^{2}(\mathbf {x} )\mathbf {I} ),\\p_{\theta }(\mathbf {x} |\mathbf {h} )&={\mathcal {N}}({\boldsymbol {\mu }}(\mathbf {h} ),{\boldsymbol {\sigma }}^{2}(\mathbf {h} )\mathbf {I} ),\end{aligned}}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\begin{aligned}q_{\phi }(\mathbf {h} |\mathbf {x} )&={\mathcal {N}}({\boldsymbol {\rho }}(\mathbf {x} ),{\boldsymbol {\omega }}^{2}(\mathbf {x} )\mathbf {I} ),\\p_{\theta }(\mathbf {x} |\mathbf {h} )&={\mathcal {N}}({\boldsymbol {\mu }}(\mathbf {h} ),{\boldsymbol {\sigma }}^{2}(\mathbf {h} )\mathbf {I} ),\end{aligned}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/539b0e2a101cae0715c772fdeeffbf29c1bf5ee0" style="vertical-align: -2.671ex; width:29.114ex; height:6.509ex;"/></span></dd></dl>
<p>where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\boldsymbol {\rho }}(\mathbf {x} )}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold-italic">ρ<!-- ρ --></mi>
</mrow>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {\rho }}(\mathbf {x} )}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\boldsymbol {\rho }}(\mathbf {x} )}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/429e0e5f67d2fe1ca9455c15952d680ae686d597" style="vertical-align: -0.838ex; width:4.643ex; height:2.843ex;"/></span> and  <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\boldsymbol {\omega }}^{2}(\mathbf {x} )}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold-italic">ω<!-- ω --></mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {\omega }}^{2}(\mathbf {x} )}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\boldsymbol {\omega }}^{2}(\mathbf {x} )}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8765f05e7f021c6f2805faa360e534aabc6128da" style="vertical-align: -0.838ex; width:5.943ex; height:3.176ex;"/></span> are the encoder outputs, while <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\boldsymbol {\mu }}(\mathbf {h} )}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold-italic">μ<!-- μ --></mi>
</mrow>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">h</mi>
</mrow>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {\mu }}(\mathbf {h} )}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\boldsymbol {\mu }}(\mathbf {h} )}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3ffa0e62effa208e6f6a223ef27d24622dcadec1" style="vertical-align: -0.838ex; width:4.94ex; height:2.843ex;"/></span> and  <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\boldsymbol {\sigma }}^{2}(\mathbf {h} )}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold-italic">σ<!-- σ --></mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">h</mi>
</mrow>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {\sigma }}^{2}(\mathbf {h} )}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\boldsymbol {\sigma }}^{2}(\mathbf {h} )}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/903d2f4ffc16966a6a44c4623af8cc8ed283967d" style="vertical-align: -0.838ex; width:5.943ex; height:3.176ex;"/></span> are the decoder outputs.
This choice is justified by the simplifications<sup class="reference" id="cite_ref-VAE_10-2"><a href="#cite_note-VAE-10">[10]</a></sup> that it produces when evaluating both the KL divergence and the likelihood term in variational objective defined above.
</p><p>VAE have been criticized because they generate blurry images.<sup class="reference" id="cite_ref-SigmaVAE2_24-0"><a href="#cite_note-SigmaVAE2-24">[24]</a></sup> However, researchers employing this model were showing only the mean of the distributions, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\boldsymbol {\mu }}(\mathbf {h} )}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold-italic">μ<!-- μ --></mi>
</mrow>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">h</mi>
</mrow>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {\mu }}(\mathbf {h} )}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\boldsymbol {\mu }}(\mathbf {h} )}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3ffa0e62effa208e6f6a223ef27d24622dcadec1" style="vertical-align: -0.838ex; width:4.94ex; height:2.843ex;"/></span>, rather than a sample of the learned Gaussian distribution
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbf {x} \sim {\mathcal {N}}({\boldsymbol {\mu }}(\mathbf {h} ),{\boldsymbol {\sigma }}^{2}(\mathbf {h} )\mathbf {I} )}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>∼<!-- ∼ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">N</mi>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold-italic">μ<!-- μ --></mi>
</mrow>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">h</mi>
</mrow>
<mo stretchy="false">)</mo>
<mo>,</mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold-italic">σ<!-- σ --></mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">h</mi>
</mrow>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">I</mi>
</mrow>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbf {x} \sim {\mathcal {N}}({\boldsymbol {\mu }}(\mathbf {h} ),{\boldsymbol {\sigma }}^{2}(\mathbf {h} )\mathbf {I} )}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \mathbf {x} \sim {\mathcal {N}}({\boldsymbol {\mu }}(\mathbf {h} ),{\boldsymbol {\sigma }}^{2}(\mathbf {h} )\mathbf {I} )}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/51916344cffee92bfb2d83307a8c0b009e30b93a" style="vertical-align: -0.838ex; width:21.525ex; height:3.176ex;"/></span>.</dd></dl>
<p>These samples were shown to be overly noisy due to the choice of a factorized Gaussian distribution.<sup class="reference" id="cite_ref-SigmaVAE2_24-1"><a href="#cite_note-SigmaVAE2-24">[24]</a></sup><sup class="reference" id="cite_ref-SigmaVAE1_25-0"><a href="#cite_note-SigmaVAE1-25">[25]</a></sup> Employing a Gaussian distribution with a full covariance matrix,
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle p_{\theta }(\mathbf {x} |\mathbf {h} )={\mathcal {N}}({\boldsymbol {\mu }}(\mathbf {h} ),{\boldsymbol {\Sigma }}(\mathbf {h} )),}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>p</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>θ<!-- θ --></mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">h</mi>
</mrow>
<mo stretchy="false">)</mo>
<mo>=</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">N</mi>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold-italic">μ<!-- μ --></mi>
</mrow>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">h</mi>
</mrow>
<mo stretchy="false">)</mo>
<mo>,</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">Σ<!-- Σ --></mi>
</mrow>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">h</mi>
</mrow>
<mo stretchy="false">)</mo>
<mo stretchy="false">)</mo>
<mo>,</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle p_{\theta }(\mathbf {x} |\mathbf {h} )={\mathcal {N}}({\boldsymbol {\mu }}(\mathbf {h} ),{\boldsymbol {\Sigma }}(\mathbf {h} )),}</annotation>
</semantics>
</math></span><img alt="{\displaystyle p_{\theta }(\mathbf {x} |\mathbf {h} )={\mathcal {N}}({\boldsymbol {\mu }}(\mathbf {h} ),{\boldsymbol {\Sigma }}(\mathbf {h} )),}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/dd2cf23b42bc557e53f3ae81c55636173cae1fc9" style="vertical-align: -0.838ex; margin-left: -0.089ex; width:26.644ex; height:3.009ex;"/></span></dd></dl>
<p>could solve this issue, but is computationally intractable and numerically unstable, as it requires estimating a covariance matrix from a single data sample. However, later research<sup class="reference" id="cite_ref-SigmaVAE2_24-2"><a href="#cite_note-SigmaVAE2-24">[24]</a></sup><sup class="reference" id="cite_ref-SigmaVAE1_25-1"><a href="#cite_note-SigmaVAE1-25">[25]</a></sup> showed that a restricted approach where the inverse matrix <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\boldsymbol {\Sigma }}^{-1}(\mathbf {h} )}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">Σ<!-- Σ --></mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msup>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">h</mi>
</mrow>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {\Sigma }}^{-1}(\mathbf {h} )}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\boldsymbol {\Sigma }}^{-1}(\mathbf {h} )}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/13562429c2a3040a08ab9d1f49d15c6b5410a684" style="vertical-align: -0.838ex; width:7.559ex; height:3.176ex;"/></span> is sparse, could be tractably employed to generate images with high-frequency details.
</p><p>Large-scale VAE models have been developed in different domains to represent data in a compact probabilistic latent space. For example, VQ-VAE<sup class="reference" id="cite_ref-26"><a href="#cite_note-26">[26]</a></sup> for image generation and Optimus <sup class="reference" id="cite_ref-27"><a href="#cite_note-27">[27]</a></sup> for language modeling.
</p>
<h2><span class="mw-headline" id="Advantages_of_Depth">Advantages of Depth</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&action=edit&section=9" title="Edit section: Advantages of Depth">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="thumb tright"><div class="thumbinner" style="width:352px;"><a class="image" href="/wiki/File:Autoencoder_structure.png"><img alt="" class="thumbimage" data-file-height="506" data-file-width="677" decoding="async" height="262" src="//upload.wikimedia.org/wikipedia/commons/thumb/2/28/Autoencoder_structure.png/350px-Autoencoder_structure.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/2/28/Autoencoder_structure.png/525px-Autoencoder_structure.png 1.5x, //upload.wikimedia.org/wikipedia/commons/2/28/Autoencoder_structure.png 2x" width="350"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:Autoencoder_structure.png" title="Enlarge"></a></div>Schematic structure of an autoencoder with 3 fully connected hidden layers. The code (z, or h for reference in the text) is the most internal layer.</div></div></div>
<p>Autoencoders are often trained with only a single layer encoder and a single layer decoder, but using deep encoders and decoders offers many advantages.<sup class="reference" id="cite_ref-:0_2-5"><a href="#cite_note-:0-2">[2]</a></sup>
</p>
<ul><li>Depth can exponentially reduce the computational cost of representing some functions.<sup class="reference" id="cite_ref-:0_2-6"><a href="#cite_note-:0-2">[2]</a></sup></li>
<li>Depth can exponentially decrease the amount of training data needed to learn some functions.<sup class="reference" id="cite_ref-:0_2-7"><a href="#cite_note-:0-2">[2]</a></sup></li>
<li>Experimentally, deep autoencoders yield better compression compared to shallow or linear autoencoders.<sup class="reference" id="cite_ref-:7_28-0"><a href="#cite_note-:7-28">[28]</a></sup></li></ul>
<h3><span class="mw-headline" id="Training_Deep_Architectures">Training Deep Architectures</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&action=edit&section=10" title="Edit section: Training Deep Architectures">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="/wiki/Geoffrey_Hinton" title="Geoffrey Hinton">Geoffrey Hinton</a> developed a pretraining technique for training many-layered deep autoencoders. This method involves treating each neighbouring set of two layers as a <a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">restricted Boltzmann machine</a> so that the pretraining approximates a good solution, then using a backpropagation technique to fine-tune the results.<sup class="reference" id="cite_ref-:7_28-1"><a href="#cite_note-:7-28">[28]</a></sup> This model takes the name of <a href="/wiki/Deep_belief_network" title="Deep belief network">deep belief network</a>.
</p><p>Recently, researchers have debated whether joint training (i.e. training the whole architecture together with a single global reconstruction objective to optimize) would be better for deep auto-encoders.<sup class="reference" id="cite_ref-:9_29-0"><a href="#cite_note-:9-29">[29]</a></sup> A study published in 2015 empirically showed that the joint training method not only learns better data models, but also learned more representative features for classification as compared to the layerwise method.<sup class="reference" id="cite_ref-:9_29-1"><a href="#cite_note-:9-29">[29]</a></sup> However, their experiments highlighted how the success of joint training for deep autoencoder architectures depends heavily on the regularization strategies adopted in the modern variants of the model.<sup class="reference" id="cite_ref-:9_29-2"><a href="#cite_note-:9-29">[29]</a></sup><sup class="reference" id="cite_ref-30"><a href="#cite_note-30">[30]</a></sup>
</p>
<h2><span class="mw-headline" id="Applications">Applications</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&action=edit&section=11" title="Edit section: Applications">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The two main applications of autoencoders since the 80s have been <i>dimensionality reduction</i> and <i>information retrieval,</i><sup class="reference" id="cite_ref-:0_2-8"><a href="#cite_note-:0-2">[2]</a></sup> but modern variations of the basic model were proven successful when applied to different domains and tasks.
</p>
<h3><span class="mw-headline" id="Dimensionality_Reduction">Dimensionality Reduction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&action=edit&section=12" title="Edit section: Dimensionality Reduction">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="thumb tright"><div class="thumbinner" style="width:222px;"><a class="image" href="/wiki/File:PCA_vs_Linear_Autoencoder.png"><img alt="" class="thumbimage" data-file-height="288" data-file-width="576" decoding="async" height="110" src="//upload.wikimedia.org/wikipedia/commons/thumb/0/0b/PCA_vs_Linear_Autoencoder.png/220px-PCA_vs_Linear_Autoencoder.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/0/0b/PCA_vs_Linear_Autoencoder.png/330px-PCA_vs_Linear_Autoencoder.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/0/0b/PCA_vs_Linear_Autoencoder.png/440px-PCA_vs_Linear_Autoencoder.png 2x" width="220"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:PCA_vs_Linear_Autoencoder.png" title="Enlarge"></a></div>Plot of the first two Principal Components (left) and a two-dimension hidden layer of a Linear Autoencoder (Right) applied to the <a class="new" href="/w/index.php?title=Fashion_MNIST_dataset&action=edit&redlink=1" title="Fashion MNIST dataset (page does not exist)">Fashion MNIST dataset</a>.<sup class="reference" id="cite_ref-:10_31-0"><a href="#cite_note-:10-31">[31]</a></sup> The two models being both linear learn to span the same subspace. The projection of the data points is indeed identical, apart from rotation of the subspace - to which PCA is invariant.</div></div></div><p><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality Reduction</a> was one of the first applications of <a href="/wiki/Deep_learning" title="Deep learning">deep learning</a>, and one of the early motivations to study autoencoders.<sup class="reference" id="cite_ref-:0_2-9"><a href="#cite_note-:0-2">[2]</a></sup>  In a nutshell, the objective is to find a proper projection method, that maps data from high feature space to low feature space.<sup class="reference" id="cite_ref-:0_2-10"><a href="#cite_note-:0-2">[2]</a></sup>
</p><p>One milestone paper on the subject was that of <a href="/wiki/Geoffrey_Hinton" title="Geoffrey Hinton">Geoffrey Hinton</a> with his publication in <a href="/wiki/Science_(journal)" title="Science (journal)">Science Magazine</a> in 2006:<sup class="reference" id="cite_ref-:7_28-2"><a href="#cite_note-:7-28">[28]</a></sup> in that study, he pretrained a multi-layer autoencoder with a stack of <a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">RBMs</a> and then used their weights to initialize a deep autoencoder with gradually smaller hidden layers until a bottleneck of 30 neurons. The resulting 30 dimensions of the code yielded a smaller reconstruction error compared to the first 30 principal components of a <a href="/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a>, and learned a representation that was qualitatively easier to interpret, clearly separating clusters in the original data.<sup class="reference" id="cite_ref-:0_2-11"><a href="#cite_note-:0-2">[2]</a></sup><sup class="reference" id="cite_ref-:7_28-3"><a href="#cite_note-:7-28">[28]</a></sup>
</p><p>Representing data in a lower-dimensional space can improve performance on different tasks, such as classification.<sup class="reference" id="cite_ref-:0_2-12"><a href="#cite_note-:0-2">[2]</a></sup> Indeed, many forms of <a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">dimensionality reduction</a> place semantically related examples near each other,<sup class="reference" id="cite_ref-:3_32-0"><a href="#cite_note-:3-32">[32]</a></sup> aiding generalization.
</p>
<h4><span id="Relationship_with_principal_component_analysis_.28PCA.29"></span><span class="mw-headline" id="Relationship_with_principal_component_analysis_(PCA)">Relationship with principal component analysis (PCA)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&action=edit&section=13" title="Edit section: Relationship with principal component analysis (PCA)">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<div class="thumb tright"><div class="thumbinner" style="width:222px;"><a class="image" href="/wiki/File:Reconstruction_autoencoders_vs_PCA.png"><img alt="" class="thumbimage" data-file-height="441" data-file-width="2008" decoding="async" height="48" src="//upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Reconstruction_autoencoders_vs_PCA.png/220px-Reconstruction_autoencoders_vs_PCA.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Reconstruction_autoencoders_vs_PCA.png/330px-Reconstruction_autoencoders_vs_PCA.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Reconstruction_autoencoders_vs_PCA.png/440px-Reconstruction_autoencoders_vs_PCA.png 2x" width="220"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:Reconstruction_autoencoders_vs_PCA.png" title="Enlarge"></a></div>Reconstruction of 28x28pixel images by an Autoencoder with a code size of two (two-units hidden layer) and the reconstruction from the first two Principal Components of PCA. Images come from the <a class="new" href="/w/index.php?title=Fashion_MNIST_dataset&action=edit&redlink=1" title="Fashion MNIST dataset (page does not exist)">Fashion MNIST dataset</a>.<sup class="reference" id="cite_ref-:10_31-1"><a href="#cite_note-:10-31">[31]</a></sup></div></div></div>
<p>If linear activations are used, or only a single sigmoid hidden layer, then the optimal solution to an autoencoder is strongly related to <a href="/wiki/Principal_component_analysis" title="Principal component analysis">principal component analysis</a> (PCA).<sup class="reference" id="cite_ref-33"><a href="#cite_note-33">[33]</a></sup><sup class="reference" id="cite_ref-34"><a href="#cite_note-34">[34]</a></sup> The weights of an autoencoder with a single hidden layer of size <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle p}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>p</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle p}</annotation>
</semantics>
</math></span><img alt="p" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/81eac1e205430d1f40810df36a0edffdc367af36" style="vertical-align: -0.671ex; margin-left: -0.089ex; width:1.259ex; height:2.009ex;"/></span> (where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle p}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>p</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle p}</annotation>
</semantics>
</math></span><img alt="p" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/81eac1e205430d1f40810df36a0edffdc367af36" style="vertical-align: -0.671ex; margin-left: -0.089ex; width:1.259ex; height:2.009ex;"/></span> is less than the size of the input) span the same vector subspace as the one spanned by the first <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle p}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>p</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle p}</annotation>
</semantics>
</math></span><img alt="p" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/81eac1e205430d1f40810df36a0edffdc367af36" style="vertical-align: -0.671ex; margin-left: -0.089ex; width:1.259ex; height:2.009ex;"/></span> principal components, and the output of the autoencoder is an orthogonal projection onto this subspace. The autoencoder weights are not equal to the principal components, and are generally not orthogonal, yet the principal components may be recovered from them using the <a href="/wiki/Singular_value_decomposition" title="Singular value decomposition">singular value decomposition</a>.<sup class="reference" id="cite_ref-35"><a href="#cite_note-35">[35]</a></sup>
</p><p>However, the potential of Autoencoders resides in their non-linearity, allowing the model to learn more powerful generalizations compared to PCA, and to reconstruct back the input with a significantly lower loss of information.<sup class="reference" id="cite_ref-:7_28-4"><a href="#cite_note-:7-28">[28]</a></sup>
</p>
<h3><span class="mw-headline" id="Information_Retrieval">Information Retrieval</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&action=edit&section=14" title="Edit section: Information Retrieval">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="/wiki/Information_retrieval" title="Information retrieval">Information Retrieval</a> benefits particularly from <a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">dimensionality reduction</a> in that search can become extremely efficient in certain kinds of low dimensional spaces. Autoencoders were indeed applied to <b><a class="new" href="/w/index.php?title=Semantic_hashing&action=edit&redlink=1" title="Semantic hashing (page does not exist)">semantic hashing</a></b>, proposed by <a href="/wiki/Russ_Salakhutdinov" title="Russ Salakhutdinov">Salakhutdinov</a> and <a href="/wiki/Geoffrey_Hinton" title="Geoffrey Hinton">Hinton</a> in 2007.<sup class="reference" id="cite_ref-:3_32-1"><a href="#cite_note-:3-32">[32]</a></sup> In a nutshell, training the algorithm to produce a low-dimensional binary code, then all database entries could be stored in a <a href="/wiki/Hash_table" title="Hash table">hash table</a> mapping binary code vectors to entries. This table would then allow to perform information retrieval by returning all entries with the same binary code as the query, or slightly less similar entries by flipping some bits from the encoding of the query.
</p>
<h3><span class="mw-headline" id="Anomaly_Detection">Anomaly Detection</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&action=edit&section=15" title="Edit section: Anomaly Detection">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Another field of application for autoencoders is <a href="/wiki/Anomaly_detection" title="Anomaly detection">anomaly detection</a>.<sup class="reference" id="cite_ref-36"><a href="#cite_note-36">[36]</a></sup><sup class="reference" id="cite_ref-:8_37-0"><a href="#cite_note-:8-37">[37]</a></sup><sup class="reference" id="cite_ref-38"><a href="#cite_note-38">[38]</a></sup><sup class="reference" id="cite_ref-39"><a href="#cite_note-39">[39]</a></sup> By learning to replicate the most salient features in the training data under some of the constraints described previously, the model is encouraged to learn how to precisely reproduce the most frequent characteristics of the observations. When facing anomalies, the model should worsen its reconstruction performance. In most cases, only data with normal instances are used to train the autoencoder; in others, the frequency of anomalies is so small compared to the whole population of observations, that its contribution to the representation learnt by the model could be ignored. After training, the autoencoder will reconstruct normal data very well, while failing to do so with anomaly data which the autoencoder has not encountered.<sup class="reference" id="cite_ref-:8_37-1"><a href="#cite_note-:8-37">[37]</a></sup> Reconstruction error of a data point, which is the error between the original data point and its low dimensional reconstruction, is used as an anomaly score to detect anomalies.<sup class="reference" id="cite_ref-:8_37-2"><a href="#cite_note-:8-37">[37]</a></sup>
</p>
<h3><span class="mw-headline" id="Image_Processing">Image Processing</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&action=edit&section=16" title="Edit section: Image Processing">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The peculiar characteristics of autoencoders have rendered these model extremely useful in the processing of images for various tasks.
</p><p>One example can be found in lossy <a href="/wiki/Image_compression" title="Image compression">image compression</a> task, where autoencoders demonstrated their potential by outperforming other approaches and being proven competitive against <a href="/wiki/JPEG_2000" title="JPEG 2000">JPEG 2000</a>.<sup class="reference" id="cite_ref-40"><a href="#cite_note-40">[40]</a></sup>
</p><p>Another useful application of autoencoders in the field of image preprocessing is <a class="mw-redirect" href="/wiki/Image_denoising" title="Image denoising">image denoising</a><b>.</b><sup class="reference" id="cite_ref-41"><a href="#cite_note-41">[41]</a></sup><sup class="reference" id="cite_ref-42"><a href="#cite_note-42">[42]</a></sup> The need for efficient image restoration methods has grown with the massive production of digital images and movies of all kinds, often taken in poor conditions.<sup class="reference" id="cite_ref-43"><a href="#cite_note-43">[43]</a></sup>
</p><p>Autoencoders are increasingly proving their ability even in more delicate contexts such as <a href="/wiki/Medical_imaging" title="Medical imaging">medical imaging</a>. In this context, they have also been used for <a class="mw-redirect" href="/wiki/Image_denoising" title="Image denoising">image denoising</a><sup class="reference" id="cite_ref-44"><a href="#cite_note-44">[44]</a></sup> as well as <a class="mw-redirect" href="/wiki/Super-resolution" title="Super-resolution">super-resolution</a>.<sup class="reference" id="cite_ref-45"><a href="#cite_note-45">[45]</a></sup> In the field of image-assisted diagnosis, there exist some experiments using autoencoders for the detection of <a href="/wiki/Breast_cancer" title="Breast cancer">breast cancer</a><sup class="reference" id="cite_ref-46"><a href="#cite_note-46">[46]</a></sup> or even modelling the relation between the cognitive decline of <a class="mw-redirect" href="/wiki/Alzheimer%27s_Disease" title="Alzheimer's Disease">Alzheimer's Disease</a> and the latent features of an autoencoder trained with <a class="mw-redirect" href="/wiki/MRI" title="MRI">MRI</a><sup class="reference" id="cite_ref-47"><a href="#cite_note-47">[47]</a></sup>
</p><p>Lastly, other successful experiments have been carried out exploiting variations of the basic autoencoder for <a class="new" href="/w/index.php?title=Image_super-resolution&action=edit&redlink=1" title="Image super-resolution (page does not exist)">image super-resolution</a> tasks.<sup class="reference" id="cite_ref-48"><a href="#cite_note-48">[48]</a></sup>
</p>
<h3><span class="mw-headline" id="Drug_discovery">Drug discovery</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&action=edit&section=17" title="Edit section: Drug discovery">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>In 2019 molecules generated with a special type of variational autoencoders were validated experimentally all the way into mice.<sup class="reference" id="cite_ref-49"><a href="#cite_note-49">[49]</a></sup><sup class="reference" id="cite_ref-50"><a href="#cite_note-50">[50]</a></sup>
</p>
<h3><span class="mw-headline" id="Population_synthesis">Population synthesis</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&action=edit&section=18" title="Edit section: Population synthesis">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>In 2019 a variational autoencoder framework was used to do population synthesis by approximating high-dimensional survey data.<sup class="reference" id="cite_ref-51"><a href="#cite_note-51">[51]</a></sup> By sampling agents from the approximated distribution new synthetic 'fake' populations, with similar statistical properties as those of the original population, were generated.
</p>
<h3><span class="mw-headline" id="Popularity_prediction">Popularity prediction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&action=edit&section=19" title="Edit section: Popularity prediction">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Recently, stacked autoencoder framework have shown promising results in predicting popularity of social media posts,<sup class="reference" id="cite_ref-52"><a href="#cite_note-52">[52]</a></sup> which is helpful for online advertisement strategies.
</p>
<h3><span class="mw-headline" id="Machine_Translation">Machine Translation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&action=edit&section=20" title="Edit section: Machine Translation">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Autoencoder has been successfully applied to the <a href="/wiki/Machine_translation" title="Machine translation">machine translation</a> of human languages which is usually referred to as <a href="/wiki/Neural_machine_translation" title="Neural machine translation">neural machine translation</a> (NMT).<sup class="reference" id="cite_ref-53"><a href="#cite_note-53">[53]</a></sup><sup class="reference" id="cite_ref-54"><a href="#cite_note-54">[54]</a></sup> In NMT, the language texts are treated as sequences to be encoded into the learning procedure, while in the decoder side the target languages will be generated. Recent years also see the application of <a href="/wiki/Language" title="Language">language</a> specific autoencoders to incorporate the <a class="mw-redirect" href="/wiki/Linguistic" title="Linguistic">linguistic</a> features into the learning procedure, such as Chinese decomposition features.<sup class="reference" id="cite_ref-55"><a href="#cite_note-55">[55]</a></sup>
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&action=edit&section=21" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a class="mw-redirect" href="/wiki/Representation_learning" title="Representation learning">Representation learning</a></li>
<li><a href="/wiki/Sparse_dictionary_learning" title="Sparse dictionary learning">Sparse dictionary learning</a></li>
<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li></ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&action=edit&section=22" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist columns references-column-width" style="-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em; list-style-type: decimal;">
<ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFKramer1991">Kramer, Mark A. (1991). <a class="external text" href="https://www.researchgate.net/profile/Abir_Alobaid/post/To_learn_a_probability_density_function_by_using_neural_network_can_we_first_estimate_density_using_nonparametric_methods_then_train_the_network/attachment/59d6450279197b80779a031e/AS:451263696510979@1484601057779/download/NL+PCA+by+using+ANN.pdf" rel="nofollow">"Nonlinear principal component analysis using autoassociative neural networks"</a> <span class="cs1-format">(PDF)</span>. <i>AIChE Journal</i>. <b>37</b> (2): 233–243. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1002%2Faic.690370209" rel="nofollow">10.1002/aic.690370209</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=AIChE+Journal&rft.atitle=Nonlinear+principal+component+analysis+using+autoassociative+neural+networks&rft.volume=37&rft.issue=2&rft.pages=233-243&rft.date=1991&rft_id=info%3Adoi%2F10.1002%2Faic.690370209&rft.aulast=Kramer&rft.aufirst=Mark+A.&rft_id=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FAbir_Alobaid%2Fpost%2FTo_learn_a_probability_density_function_by_using_neural_network_can_we_first_estimate_density_using_nonparametric_methods_then_train_the_network%2Fattachment%2F59d6450279197b80779a031e%2FAS%3A451263696510979%401484601057779%2Fdownload%2FNL%2BPCA%2Bby%2Busing%2BANN.pdf&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><style data-mw-deduplicate="TemplateStyles:r951705291">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background-image:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png");background-image:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg");background-repeat:no-repeat;background-size:9px;background-position:right .1em center}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background-image:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png");background-image:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg");background-repeat:no-repeat;background-size:9px;background-position:right .1em center}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background-image:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png");background-image:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg");background-repeat:no-repeat;background-size:9px;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background-image:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png");background-image:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg");background-repeat:no-repeat;background-size:12px;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}</style></span>
</li>
<li id="cite_note-:0-2"><span class="mw-cite-backlink">^ <a href="#cite_ref-:0_2-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:0_2-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:0_2-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-:0_2-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-:0_2-4"><sup><i><b>e</b></i></sup></a> <a href="#cite_ref-:0_2-5"><sup><i><b>f</b></i></sup></a> <a href="#cite_ref-:0_2-6"><sup><i><b>g</b></i></sup></a> <a href="#cite_ref-:0_2-7"><sup><i><b>h</b></i></sup></a> <a href="#cite_ref-:0_2-8"><sup><i><b>i</b></i></sup></a> <a href="#cite_ref-:0_2-9"><sup><i><b>j</b></i></sup></a> <a href="#cite_ref-:0_2-10"><sup><i><b>k</b></i></sup></a> <a href="#cite_ref-:0_2-11"><sup><i><b>l</b></i></sup></a> <a href="#cite_ref-:0_2-12"><sup><i><b>m</b></i></sup></a></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFGoodfellowBengioCourville2016">Goodfellow, Ian; Bengio, Yoshua; Courville, Aaron (2016). <a class="external text" href="http://www.deeplearningbook.org" rel="nofollow"><i>Deep Learning</i></a>. MIT Press. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a> <a href="/wiki/Special:BookSources/978-0262035613" title="Special:BookSources/978-0262035613"><bdi>978-0262035613</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&rft.genre=book&rft.btitle=Deep+Learning&rft.pub=MIT+Press&rft.date=2016&rft.isbn=978-0262035613&rft.aulast=Goodfellow&rft.aufirst=Ian&rft.au=Bengio%2C+Yoshua&rft.au=Courville%2C+Aaron&rft_id=http%3A%2F%2Fwww.deeplearningbook.org&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-:4-3"><span class="mw-cite-backlink">^ <a href="#cite_ref-:4_3-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:4_3-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:4_3-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-:4_3-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-:4_3-4"><sup><i><b>e</b></i></sup></a> <a href="#cite_ref-:4_3-5"><sup><i><b>f</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFVincentLarochelle2010">Vincent, Pascal; Larochelle, Hugo (2010). "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion". <i>Journal of Machine Learning Research</i>. <b>11</b>: 3371–3408.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Journal+of+Machine+Learning+Research&rft.atitle=Stacked+Denoising+Autoencoders%3A+Learning+Useful+Representations+in+a+Deep+Network+with+a+Local+Denoising+Criterion&rft.volume=11&rft.pages=3371-3408&rft.date=2010&rft.aulast=Vincent&rft.aufirst=Pascal&rft.au=Larochelle%2C+Hugo&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-:11-4"><span class="mw-cite-backlink">^ <a href="#cite_ref-:11_4-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:11_4-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFWellingKingma2019">Welling, Max; Kingma, Diederik P. (2019). "An Introduction to Variational Autoencoders". <i>Foundations and Trends in Machine Learning</i>. <b>12</b> (4): 307–392. <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1906.02691" rel="nofollow">1906.02691</a></span>. <a class="mw-redirect" href="/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a class="external text" href="https://ui.adsabs.harvard.edu/abs/2019arXiv190602691K" rel="nofollow">2019arXiv190602691K</a>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1561%2F2200000056" rel="nofollow">10.1561/2200000056</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Foundations+and+Trends+in+Machine+Learning&rft.atitle=An+Introduction+to+Variational+Autoencoders&rft.volume=12&rft.issue=4&rft.pages=307-392&rft.date=2019&rft_id=info%3Aarxiv%2F1906.02691&rft_id=info%3Adoi%2F10.1561%2F2200000056&rft_id=info%3Abibcode%2F2019arXiv190602691K&rft.aulast=Welling&rft.aufirst=Max&rft.au=Kingma%2C+Diederik+P.&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text">Hinton GE, Krizhevsky A, Wang SD. <a class="external text" href="http://www.cs.toronto.edu/~fritz/absps/transauto6.pdf" rel="nofollow">Transforming auto-encoders.</a> In International Conference on Artificial Neural Networks 2011 Jun 14 (pp. 44-51). Springer, Berlin, Heidelberg.</span>
</li>
<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFLiouHuangYang2008">Liou, Cheng-Yuan; Huang, Jau-Chi; Yang, Wen-Chie (2008). "Modeling word perception using the Elman network". <i>Neurocomputing</i>. <b>71</b> (16–18): 3150. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1016%2Fj.neucom.2008.04.030" rel="nofollow">10.1016/j.neucom.2008.04.030</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Neurocomputing&rft.atitle=Modeling+word+perception+using+the+Elman+network&rft.volume=71&rft.issue=16%E2%80%9318&rft.pages=3150&rft.date=2008&rft_id=info%3Adoi%2F10.1016%2Fj.neucom.2008.04.030&rft.aulast=Liou&rft.aufirst=Cheng-Yuan&rft.au=Huang%2C+Jau-Chi&rft.au=Yang%2C+Wen-Chie&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-7">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFLiouChengLiouLiou2014">Liou, Cheng-Yuan; Cheng, Wei-Chen; Liou, Jiun-Wei; Liou, Daw-Ran (2014). "Autoencoder for words". <i>Neurocomputing</i>. <b>139</b>: 84–96. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1016%2Fj.neucom.2013.09.055" rel="nofollow">10.1016/j.neucom.2013.09.055</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Neurocomputing&rft.atitle=Autoencoder+for+words&rft.volume=139&rft.pages=84-96&rft.date=2014&rft_id=info%3Adoi%2F10.1016%2Fj.neucom.2013.09.055&rft.aulast=Liou&rft.aufirst=Cheng-Yuan&rft.au=Cheng%2C+Wei-Chen&rft.au=Liou%2C+Jiun-Wei&rft.au=Liou%2C+Daw-Ran&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFSchmidhuber2015">Schmidhuber, Jürgen (January 2015). "Deep learning in neural networks: An overview". <i>Neural Networks</i>. <b>61</b>: 85–117. <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1404.7828" rel="nofollow">1404.7828</a></span>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1016%2Fj.neunet.2014.09.003" rel="nofollow">10.1016/j.neunet.2014.09.003</a>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/25462637" rel="nofollow">25462637</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Neural+Networks&rft.atitle=Deep+learning+in+neural+networks%3A+An+overview&rft.volume=61&rft.pages=85-117&rft.date=2015-01&rft_id=info%3Aarxiv%2F1404.7828&rft_id=info%3Apmid%2F25462637&rft_id=info%3Adoi%2F10.1016%2Fj.neunet.2014.09.003&rft.aulast=Schmidhuber&rft.aufirst=J%C3%BCrgen&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text">Hinton, G. E., & Zemel, R. S. (1994). Autoencoders, minimum description length and Helmholtz free energy. In <i>Advances in neural information processing systems 6</i> (pp. 3-10).</span>
</li>
<li id="cite_note-VAE-10"><span class="mw-cite-backlink">^ <a href="#cite_ref-VAE_10-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-VAE_10-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-VAE_10-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation arxiv cs1" id="CITEREFDiederik_P_KingmaWelling2013">Diederik P Kingma; Welling, Max (2013). "Auto-Encoding Variational Bayes". <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1312.6114" rel="nofollow">1312.6114</a></span> [<a class="external text" href="//arxiv.org/archive/stat.ML" rel="nofollow">stat.ML</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=preprint&rft.jtitle=arXiv&rft.atitle=Auto-Encoding+Variational+Bayes&rft.date=2013&rft_id=info%3Aarxiv%2F1312.6114&rft.au=Diederik+P+Kingma&rft.au=Welling%2C+Max&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-gan_faces-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-gan_faces_11-0">^</a></b></span> <span class="reference-text">Generating Faces with Torch, Boesen A., Larsen L. and Sonderby S.K., 2015 <span class="url"><a class="external text" href="http://torch.ch/blog/2015/11/13/gan.html" rel="nofollow">torch<wbr/>.ch<wbr/>/blog<wbr/>/2015<wbr/>/11<wbr/>/13<wbr/>/gan<wbr/>.html</a></span></span>
</li>
<li id="cite_note-domingos-12"><span class="mw-cite-backlink">^ <a href="#cite_ref-domingos_12-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-domingos_12-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFDomingos2015"><a href="/wiki/Pedro_Domingos" title="Pedro Domingos">Domingos, Pedro</a> (2015). "4". <a href="/wiki/The_Master_Algorithm" title="The Master Algorithm"><i>The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World</i></a>. Basic Books. "Deeper into the Brain" subsection. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a> <a href="/wiki/Special:BookSources/978-046506192-1" title="Special:BookSources/978-046506192-1"><bdi>978-046506192-1</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&rft.genre=bookitem&rft.atitle=4&rft.btitle=The+Master+Algorithm%3A+How+the+Quest+for+the+Ultimate+Learning+Machine+Will+Remake+Our+World&rft.pages=%22Deeper+into+the+Brain%22+subsection&rft.pub=Basic+Books&rft.date=2015&rft.isbn=978-046506192-1&rft.aulast=Domingos&rft.aufirst=Pedro&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-bengio-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-bengio_13-0">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFBengio2009">Bengio, Y. (2009). <a class="external text" href="http://www.iro.umontreal.ca/~lisa/pointeurs/TR1312.pdf" rel="nofollow">"Learning Deep Architectures for AI"</a> <span class="cs1-format">(PDF)</span>. <i>Foundations and Trends in Machine Learning</i>. <b>2</b> (8): 1795–7. <a class="mw-redirect" href="/wiki/CiteSeerX_(identifier)" title="CiteSeerX (identifier)">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.701.9550" rel="nofollow">10.1.1.701.9550</a></span>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1561%2F2200000006" rel="nofollow">10.1561/2200000006</a>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/23946944" rel="nofollow">23946944</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Foundations+and+Trends+in+Machine+Learning&rft.atitle=Learning+Deep+Architectures+for+AI&rft.volume=2&rft.issue=8&rft.pages=1795-7&rft.date=2009&rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.701.9550&rft_id=info%3Apmid%2F23946944&rft_id=info%3Adoi%2F10.1561%2F2200000006&rft.aulast=Bengio&rft.aufirst=Y.&rft_id=http%3A%2F%2Fwww.iro.umontreal.ca%2F~lisa%2Fpointeurs%2FTR1312.pdf&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-:5-14"><span class="mw-cite-backlink">^ <a href="#cite_ref-:5_14-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:5_14-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFFreyMakhzani2013">Frey, Brendan; Makhzani, Alireza (2013-12-19). "k-Sparse Autoencoders". <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1312.5663" rel="nofollow">1312.5663</a></span>. <a class="mw-redirect" href="/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a class="external text" href="https://ui.adsabs.harvard.edu/abs/2013arXiv1312.5663M" rel="nofollow">2013arXiv1312.5663M</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.atitle=k-Sparse+Autoencoders&rft.date=2013-12-19&rft_id=info%3Aarxiv%2F1312.5663&rft_id=info%3Abibcode%2F2013arXiv1312.5663M&rft.aulast=Frey&rft.aufirst=Brendan&rft.au=Makhzani%2C+Alireza&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span> <span class="cs1-hidden-error error citation-comment">Cite journal requires <code class="cs1-code">|journal=</code> (<a href="/wiki/Help:CS1_errors#missing_periodical" title="Help:CS1 errors">help</a>)</span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-:6-15"><span class="mw-cite-backlink">^ <a href="#cite_ref-:6_15-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:6_15-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:6_15-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text">Ng, A. (2011). Sparse autoencoder. <i>CS294A Lecture notes</i>, <i>72</i>(2011), 1-19.</span>
</li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFNairHinton2009">Nair, Vinod; Hinton, Geoffrey E. (2009). <a class="external text" href="http://dl.acm.org/citation.cfm?id=2984093.2984244" rel="nofollow">"3D Object Recognition with Deep Belief Nets"</a>. <i>Proceedings of the 22Nd International Conference on Neural Information Processing Systems</i>. NIPS'09. USA: Curran Associates Inc.: 1339–1347. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a> <a href="/wiki/Special:BookSources/9781615679119" title="Special:BookSources/9781615679119"><bdi>9781615679119</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Proceedings+of+the+22Nd+International+Conference+on+Neural+Information+Processing+Systems&rft.atitle=3D+Object+Recognition+with+Deep+Belief+Nets&rft.pages=1339-1347&rft.date=2009&rft.isbn=9781615679119&rft.aulast=Nair&rft.aufirst=Vinod&rft.au=Hinton%2C+Geoffrey+E.&rft_id=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D2984093.2984244&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-17">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFZengZhangSongLiu2018">Zeng, Nianyin; Zhang, Hong; Song, Baoye; Liu, Weibo; Li, Yurong; Dobaie, Abdullah M. (2018-01-17). "Facial expression recognition via learning deep sparse autoencoders". <i>Neurocomputing</i>. <b>273</b>: 643–649. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1016%2Fj.neucom.2017.08.043" rel="nofollow">10.1016/j.neucom.2017.08.043</a>. <a class="mw-redirect" href="/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a> <a class="external text" href="//www.worldcat.org/issn/0925-2312" rel="nofollow">0925-2312</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Neurocomputing&rft.atitle=Facial+expression+recognition+via+learning+deep+sparse+autoencoders&rft.volume=273&rft.pages=643-649&rft.date=2018-01-17&rft_id=info%3Adoi%2F10.1016%2Fj.neucom.2017.08.043&rft.issn=0925-2312&rft.aulast=Zeng&rft.aufirst=Nianyin&rft.au=Zhang%2C+Hong&rft.au=Song%2C+Baoye&rft.au=Liu%2C+Weibo&rft.au=Li%2C+Yurong&rft.au=Dobaie%2C+Abdullah+M.&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-18">^</a></b></span> <span class="reference-text"><cite class="citation arxiv cs1" id="CITEREFArpitZhouNgoGovindaraju2015">Arpit, Devansh; Zhou, Yingbo; Ngo, Hung; Govindaraju, Venu (2015). "Why Regularized Auto-Encoders learn Sparse Representation?". <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1505.05561" rel="nofollow">1505.05561</a></span> [<a class="external text" href="//arxiv.org/archive/stat.ML" rel="nofollow">stat.ML</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=preprint&rft.jtitle=arXiv&rft.atitle=Why+Regularized+Auto-Encoders+learn+Sparse+Representation%3F&rft.date=2015&rft_id=info%3Aarxiv%2F1505.05561&rft.aulast=Arpit&rft.aufirst=Devansh&rft.au=Zhou%2C+Yingbo&rft.au=Ngo%2C+Hung&rft.au=Govindaraju%2C+Venu&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-:1-19"><span class="mw-cite-backlink">^ <a href="#cite_ref-:1_19-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:1_19-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation arxiv cs1" id="CITEREFMakhzaniFrey2013">Makhzani, Alireza; Frey, Brendan (2013). "K-Sparse Autoencoders". <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1312.5663" rel="nofollow">1312.5663</a></span> [<a class="external text" href="//arxiv.org/archive/cs.LG" rel="nofollow">cs.LG</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=preprint&rft.jtitle=arXiv&rft.atitle=K-Sparse+Autoencoders&rft.date=2013&rft_id=info%3Aarxiv%2F1312.5663&rft.aulast=Makhzani&rft.aufirst=Alireza&rft.au=Frey%2C+Brendan&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-:2-20"><span class="mw-cite-backlink">^ <a href="#cite_ref-:2_20-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:2_20-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">An, J., & Cho, S. (2015). Variational autoencoder based anomaly detection using reconstruction probability. <i>Special Lecture on IE</i>, <i>2</i>(1).</span>
</li>
<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text"><cite class="citation arxiv cs1" id="CITEREFDoersch2016">Doersch, Carl (2016). "Tutorial on Variational Autoencoders". <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1606.05908" rel="nofollow">1606.05908</a></span> [<a class="external text" href="//arxiv.org/archive/stat.ML" rel="nofollow">stat.ML</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=preprint&rft.jtitle=arXiv&rft.atitle=Tutorial+on+Variational+Autoencoders&rft.date=2016&rft_id=info%3Aarxiv%2F1606.05908&rft.aulast=Doersch&rft.aufirst=Carl&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-1bitVAE-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-1bitVAE_22-0">^</a></b></span> <span class="reference-text"><cite class="citation arxiv cs1" id="CITEREFKhobahi,_S.Soltanalian2019">Khobahi, S.; Soltanalian, M. (2019). "Model-Aware Deep Architectures for One-Bit Compressive Variational Autoencoding". <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1911.12410" rel="nofollow">1911.12410</a></span> [<a class="external text" href="//arxiv.org/archive/eess.SP" rel="nofollow">eess.SP</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=preprint&rft.jtitle=arXiv&rft.atitle=Model-Aware+Deep+Architectures+for+One-Bit+Compressive+Variational+Autoencoding&rft.date=2019&rft_id=info%3Aarxiv%2F1911.12410&rft.au=Khobahi%2C+S.&rft.au=Soltanalian%2C+M.&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFPartaouridesChatzis2017">Partaourides, Harris; Chatzis, Sotirios P. (June 2017). <a class="external text" href="https://zenodo.org/record/3452902" rel="nofollow">"Asymmetric deep generative models"</a>. <i>Neurocomputing</i>. <b>241</b>: 90–96. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1016%2Fj.neucom.2017.02.028" rel="nofollow">10.1016/j.neucom.2017.02.028</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Neurocomputing&rft.atitle=Asymmetric+deep+generative+models&rft.volume=241&rft.pages=90-96&rft.date=2017-06&rft_id=info%3Adoi%2F10.1016%2Fj.neucom.2017.02.028&rft.aulast=Partaourides&rft.aufirst=Harris&rft.au=Chatzis%2C+Sotirios+P.&rft_id=https%3A%2F%2Fzenodo.org%2Frecord%2F3452902&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-SigmaVAE2-24"><span class="mw-cite-backlink">^ <a href="#cite_ref-SigmaVAE2_24-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-SigmaVAE2_24-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-SigmaVAE2_24-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation arxiv cs1" id="CITEREFDortaVicenteAgapitoCampbell2018">Dorta, Garoe; Vicente, Sara; Agapito, Lourdes; Campbell, Neill D. F.; Simpson, Ivor (2018). "Training VAEs Under Structured Residuals". <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1804.01050" rel="nofollow">1804.01050</a></span> [<a class="external text" href="//arxiv.org/archive/stat.ML" rel="nofollow">stat.ML</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=preprint&rft.jtitle=arXiv&rft.atitle=Training+VAEs+Under+Structured+Residuals&rft.date=2018&rft_id=info%3Aarxiv%2F1804.01050&rft.aulast=Dorta&rft.aufirst=Garoe&rft.au=Vicente%2C+Sara&rft.au=Agapito%2C+Lourdes&rft.au=Campbell%2C+Neill+D.+F.&rft.au=Simpson%2C+Ivor&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-SigmaVAE1-25"><span class="mw-cite-backlink">^ <a href="#cite_ref-SigmaVAE1_25-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-SigmaVAE1_25-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation arxiv cs1" id="CITEREFDortaVicenteAgapitoCampbell2018">Dorta, Garoe; Vicente, Sara; Agapito, Lourdes; Campbell, Neill D. F.; Simpson, Ivor (2018). "Structured Uncertainty Prediction Networks". <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1802.07079" rel="nofollow">1802.07079</a></span> [<a class="external text" href="//arxiv.org/archive/stat.ML" rel="nofollow">stat.ML</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=preprint&rft.jtitle=arXiv&rft.atitle=Structured+Uncertainty+Prediction+Networks&rft.date=2018&rft_id=info%3Aarxiv%2F1802.07079&rft.aulast=Dorta&rft.aufirst=Garoe&rft.au=Vicente%2C+Sara&rft.au=Agapito%2C+Lourdes&rft.au=Campbell%2C+Neill+D.+F.&rft.au=Simpson%2C+Ivor&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-26"><span class="mw-cite-backlink"><b><a href="#cite_ref-26">^</a></b></span> <span class="reference-text">Generating Diverse High-Fidelity Images with VQ-VAE-2 <a class="external free" href="https://arxiv.org/abs/1906.00446" rel="nofollow">https://arxiv.org/abs/1906.00446</a></span>
</li>
<li id="cite_note-27"><span class="mw-cite-backlink"><b><a href="#cite_ref-27">^</a></b></span> <span class="reference-text">Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space <a class="external free" href="https://arxiv.org/abs/2004.04092" rel="nofollow">https://arxiv.org/abs/2004.04092</a></span>
</li>
<li id="cite_note-:7-28"><span class="mw-cite-backlink">^ <a href="#cite_ref-:7_28-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:7_28-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:7_28-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-:7_28-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-:7_28-4"><sup><i><b>e</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFHintonSalakhutdinov2006">Hinton, G. E.; Salakhutdinov, R.R. (2006-07-28). "Reducing the Dimensionality of Data with Neural Networks". <i>Science</i>. <b>313</b> (5786): 504–507. <a class="mw-redirect" href="/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a class="external text" href="https://ui.adsabs.harvard.edu/abs/2006Sci...313..504H" rel="nofollow">2006Sci...313..504H</a>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1126%2Fscience.1127647" rel="nofollow">10.1126/science.1127647</a>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/16873662" rel="nofollow">16873662</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Science&rft.atitle=Reducing+the+Dimensionality+of+Data+with+Neural+Networks&rft.volume=313&rft.issue=5786&rft.pages=504-507&rft.date=2006-07-28&rft_id=info%3Apmid%2F16873662&rft_id=info%3Adoi%2F10.1126%2Fscience.1127647&rft_id=info%3Abibcode%2F2006Sci...313..504H&rft.aulast=Hinton&rft.aufirst=G.+E.&rft.au=Salakhutdinov%2C+R.R.&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-:9-29"><span class="mw-cite-backlink">^ <a href="#cite_ref-:9_29-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:9_29-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:9_29-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation arxiv cs1" id="CITEREFZhouArpitNwoguGovindaraju2014">Zhou, Yingbo; Arpit, Devansh; Nwogu, Ifeoma; Govindaraju, Venu (2014). "Is Joint Training Better for Deep Auto-Encoders?". <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1405.1380" rel="nofollow">1405.1380</a></span> [<a class="external text" href="//arxiv.org/archive/stat.ML" rel="nofollow">stat.ML</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=preprint&rft.jtitle=arXiv&rft.atitle=Is+Joint+Training+Better+for+Deep+Auto-Encoders%3F&rft.date=2014&rft_id=info%3Aarxiv%2F1405.1380&rft.aulast=Zhou&rft.aufirst=Yingbo&rft.au=Arpit%2C+Devansh&rft.au=Nwogu%2C+Ifeoma&rft.au=Govindaraju%2C+Venu&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-30"><span class="mw-cite-backlink"><b><a href="#cite_ref-30">^</a></b></span> <span class="reference-text">R. Salakhutdinov and G. E. Hinton, “Deep boltzmann machines,” in

AISTATS, 2009, pp. 448–455.</span>
</li>
<li id="cite_note-:10-31"><span class="mw-cite-backlink">^ <a href="#cite_ref-:10_31-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:10_31-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation web cs1"><a class="external text" href="https://github.com/zalandoresearch/fashion-mnist" rel="nofollow">"Fashion MNIST"</a>. 2019-07-12.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&rft.genre=unknown&rft.btitle=Fashion+MNIST&rft.date=2019-07-12&rft_id=https%3A%2F%2Fgithub.com%2Fzalandoresearch%2Ffashion-mnist&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-:3-32"><span class="mw-cite-backlink">^ <a href="#cite_ref-:3_32-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:3_32-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFSalakhutdinovHinton2009">Salakhutdinov, Ruslan; Hinton, Geoffrey (2009-07-01). <a class="external text" href="https://doi.org/10.1016/j.ijar.2008.11.006" rel="nofollow">"Semantic hashing"</a>. <i>International Journal of Approximate Reasoning</i>. Special Section on Graphical Models and Information Retrieval. <b>50</b> (7): 969–978. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="https://doi.org/10.1016%2Fj.ijar.2008.11.006" rel="nofollow">10.1016/j.ijar.2008.11.006</a></span>. <a class="mw-redirect" href="/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a> <a class="external text" href="//www.worldcat.org/issn/0888-613X" rel="nofollow">0888-613X</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=International+Journal+of+Approximate+Reasoning&rft.atitle=Semantic+hashing&rft.volume=50&rft.issue=7&rft.pages=969-978&rft.date=2009-07-01&rft_id=info%3Adoi%2F10.1016%2Fj.ijar.2008.11.006&rft.issn=0888-613X&rft.aulast=Salakhutdinov&rft.aufirst=Ruslan&rft.au=Hinton%2C+Geoffrey&rft_id=%2F%2Fdoi.org%2F10.1016%2Fj.ijar.2008.11.006&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-33"><span class="mw-cite-backlink"><b><a href="#cite_ref-33">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFBourlardKamp1988">Bourlard, H.; Kamp, Y. (1988). <a class="external text" href="http://infoscience.epfl.ch/record/82601" rel="nofollow">"Auto-association by multilayer perceptrons and singular value decomposition"</a>. <i>Biological Cybernetics</i>. <b>59</b> (4–5): 291–294. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1007%2FBF00332918" rel="nofollow">10.1007/BF00332918</a>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/3196773" rel="nofollow">3196773</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Biological+Cybernetics&rft.atitle=Auto-association+by+multilayer+perceptrons+and+singular+value+decomposition&rft.volume=59&rft.issue=4%E2%80%935&rft.pages=291-294&rft.date=1988&rft_id=info%3Adoi%2F10.1007%2FBF00332918&rft_id=info%3Apmid%2F3196773&rft.aulast=Bourlard&rft.aufirst=H.&rft.au=Kamp%2C+Y.&rft_id=http%3A%2F%2Finfoscience.epfl.ch%2Frecord%2F82601&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-34"><span class="mw-cite-backlink"><b><a href="#cite_ref-34">^</a></b></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFChiccoSadowskiBaldi2014">Chicco, Davide; Sadowski, Peter; Baldi, Pierre (2014). "Deep autoencoder neural networks for gene ontology annotation predictions". <a class="external text" href="http://dl.acm.org/citation.cfm?id=2649442" rel="nofollow"><i>Proceedings of the 5th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics - BCB '14</i></a>. p. 533. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1145%2F2649387.2649442" rel="nofollow">10.1145/2649387.2649442</a>. <a class="mw-redirect" href="/wiki/Hdl_(identifier)" title="Hdl (identifier)">hdl</a>:<a class="external text" href="//hdl.handle.net/11311%2F964622" rel="nofollow">11311/964622</a>. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a> <a href="/wiki/Special:BookSources/9781450328944" title="Special:BookSources/9781450328944"><bdi>9781450328944</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&rft.genre=bookitem&rft.atitle=Deep+autoencoder+neural+networks+for+gene+ontology+annotation+predictions&rft.btitle=Proceedings+of+the+5th+ACM+Conference+on+Bioinformatics%2C+Computational+Biology%2C+and+Health+Informatics+-+BCB+%2714&rft.pages=533&rft.date=2014&rft_id=info%3Ahdl%2F11311%2F964622&rft_id=info%3Adoi%2F10.1145%2F2649387.2649442&rft.isbn=9781450328944&rft.aulast=Chicco&rft.aufirst=Davide&rft.au=Sadowski%2C+Peter&rft.au=Baldi%2C+Pierre&rft_id=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D2649442&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-35"><span class="mw-cite-backlink"><b><a href="#cite_ref-35">^</a></b></span> <span class="reference-text"><cite class="citation arxiv cs1" id="CITEREFPlaut2018">Plaut, E (2018). "From Principal Subspaces to Principal Components with Linear Autoencoders". <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1804.10253" rel="nofollow">1804.10253</a></span> [<a class="external text" href="//arxiv.org/archive/stat.ML" rel="nofollow">stat.ML</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=preprint&rft.jtitle=arXiv&rft.atitle=From+Principal+Subspaces+to+Principal+Components+with+Linear+Autoencoders&rft.date=2018&rft_id=info%3Aarxiv%2F1804.10253&rft.aulast=Plaut&rft.aufirst=E&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-36"><span class="mw-cite-backlink"><b><a href="#cite_ref-36">^</a></b></span> <span class="reference-text">Sakurada, M., & Yairi, T. (2014, December). Anomaly detection using autoencoders with nonlinear dimensionality reduction. In <i>Proceedings of the MLSDA 2014 2nd Workshop on Machine Learning for Sensory Data Analysis</i> (p. 4). ACM.</span>
</li>
<li id="cite_note-:8-37"><span class="mw-cite-backlink">^ <a href="#cite_ref-:8_37-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:8_37-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:8_37-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text">An, J., & Cho, S. (2015). Variational autoencoder based anomaly detection using reconstruction probability. <i>Special Lecture on IE</i>, <i>2</i>, 1-18.</span>
</li>
<li id="cite_note-38"><span class="mw-cite-backlink"><b><a href="#cite_ref-38">^</a></b></span> <span class="reference-text">Zhou, C., & Paffenroth, R. C. (2017, August). Anomaly detection with robust deep autoencoders. In <i>Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</i> (pp. 665-674). ACM.</span>
</li>
<li id="cite_note-39"><span class="mw-cite-backlink"><b><a href="#cite_ref-39">^</a></b></span> <span class="reference-text">Ribeiro, M., Lazzaretti, A. E., & Lopes, H. S. (2018). A study of deep convolutional auto-encoders for anomaly detection in videos. <i>Pattern Recognition Letters</i>, <i>105</i>, 13-22.</span>
</li>
<li id="cite_note-40"><span class="mw-cite-backlink"><b><a href="#cite_ref-40">^</a></b></span> <span class="reference-text"><cite class="citation arxiv cs1" id="CITEREFTheisShiCunninghamHuszár2017">Theis, Lucas; Shi, Wenzhe; Cunningham, Andrew; Huszár, Ferenc (2017). "Lossy Image Compression with Compressive Autoencoders". <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1703.00395" rel="nofollow">1703.00395</a></span> [<a class="external text" href="//arxiv.org/archive/stat.ML" rel="nofollow">stat.ML</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=preprint&rft.jtitle=arXiv&rft.atitle=Lossy+Image+Compression+with+Compressive+Autoencoders&rft.date=2017&rft_id=info%3Aarxiv%2F1703.00395&rft.aulast=Theis&rft.aufirst=Lucas&rft.au=Shi%2C+Wenzhe&rft.au=Cunningham%2C+Andrew&rft.au=Husz%C3%A1r%2C+Ferenc&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-41"><span class="mw-cite-backlink"><b><a href="#cite_ref-41">^</a></b></span> <span class="reference-text">Cho, K. (2013, February). Simple sparsification improves sparse denoising autoencoders in denoising highly corrupted images. In <i>International Conference on Machine Learning</i> (pp. 432-440).</span>
</li>
<li id="cite_note-42"><span class="mw-cite-backlink"><b><a href="#cite_ref-42">^</a></b></span> <span class="reference-text"><cite class="citation arxiv cs1" id="CITEREFCho2013">Cho, Kyunghyun (2013). "Boltzmann Machines and Denoising Autoencoders for Image Denoising". <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1301.3468" rel="nofollow">1301.3468</a></span> [<a class="external text" href="//arxiv.org/archive/stat.ML" rel="nofollow">stat.ML</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=preprint&rft.jtitle=arXiv&rft.atitle=Boltzmann+Machines+and+Denoising+Autoencoders+for+Image+Denoising&rft.date=2013&rft_id=info%3Aarxiv%2F1301.3468&rft.aulast=Cho&rft.aufirst=Kyunghyun&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-43"><span class="mw-cite-backlink"><b><a href="#cite_ref-43">^</a></b></span> <span class="reference-text">Antoni Buades, Bartomeu Coll, Jean-Michel Morel. A review of image denoising algorithms, with a new one. Multiscale Modeling and Simulation: A SIAM Interdisciplinary Journal, Society for Industrial and Applied Mathematics, 2005, 4 (2), pp.490-530. hal-00271141</span>
</li>
<li id="cite_note-44"><span class="mw-cite-backlink"><b><a href="#cite_ref-44">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFGondara2016">Gondara, Lovedeep (December 2016). "Medical Image Denoising Using Convolutional Denoising Autoencoders". <i>2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)</i>. Barcelona, Spain: IEEE: 241–246. <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1608.04667" rel="nofollow">1608.04667</a></span>. <a class="mw-redirect" href="/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a class="external text" href="https://ui.adsabs.harvard.edu/abs/2016arXiv160804667G" rel="nofollow">2016arXiv160804667G</a>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1109%2FICDMW.2016.0041" rel="nofollow">10.1109/ICDMW.2016.0041</a>. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a> <a href="/wiki/Special:BookSources/9781509059102" title="Special:BookSources/9781509059102"><bdi>9781509059102</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=2016+IEEE+16th+International+Conference+on+Data+Mining+Workshops+%28ICDMW%29&rft.atitle=Medical+Image+Denoising+Using+Convolutional+Denoising+Autoencoders&rft.pages=241-246&rft.date=2016-12&rft_id=info%3Aarxiv%2F1608.04667&rft_id=info%3Adoi%2F10.1109%2FICDMW.2016.0041&rft_id=info%3Abibcode%2F2016arXiv160804667G&rft.isbn=9781509059102&rft.aulast=Gondara&rft.aufirst=Lovedeep&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-45"><span class="mw-cite-backlink"><b><a href="#cite_ref-45">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFTzu-HsiSanchezHeshamNasir_M.2017">Tzu-Hsi, Song; Sanchez, Victor; Hesham, EIDaly; Nasir M., Rajpoot (2017). "Hybrid deep autoencoder with Curvature Gaussian for detection of various types of cells in bone marrow trephine biopsy images". <i>2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)</i>: 1040–1043. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1109%2FISBI.2017.7950694" rel="nofollow">10.1109/ISBI.2017.7950694</a>. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a> <a href="/wiki/Special:BookSources/978-1-5090-1172-8" title="Special:BookSources/978-1-5090-1172-8"><bdi>978-1-5090-1172-8</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=2017+IEEE+14th+International+Symposium+on+Biomedical+Imaging+%28ISBI+2017%29&rft.atitle=Hybrid+deep+autoencoder+with+Curvature+Gaussian+for+detection+of+various+types+of+cells+in+bone+marrow+trephine+biopsy+images&rft.pages=1040-1043&rft.date=2017&rft_id=info%3Adoi%2F10.1109%2FISBI.2017.7950694&rft.isbn=978-1-5090-1172-8&rft.aulast=Tzu-Hsi&rft.aufirst=Song&rft.au=Sanchez%2C+Victor&rft.au=Hesham%2C+EIDaly&rft.au=Nasir+M.%2C+Rajpoot&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-46"><span class="mw-cite-backlink"><b><a href="#cite_ref-46">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFXuXiangLiuGilmore2016">Xu, Jun; Xiang, Lei; Liu, Qingshan; Gilmore, Hannah; Wu, Jianzhong; Tang, Jinghai; Madabhushi, Anant (January 2016). <a class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC4729702" rel="nofollow">"Stacked Sparse Autoencoder (SSAE) for Nuclei Detection on Breast Cancer Histopathology Images"</a>. <i>IEEE Transactions on Medical Imaging</i>. <b>35</b> (1): 119–130. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1109%2FTMI.2015.2458702" rel="nofollow">10.1109/TMI.2015.2458702</a>. <a class="mw-redirect" href="/wiki/PMC_(identifier)" title="PMC (identifier)">PMC</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC4729702" rel="nofollow">4729702</a></span>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/26208307" rel="nofollow">26208307</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=IEEE+Transactions+on+Medical+Imaging&rft.atitle=Stacked+Sparse+Autoencoder+%28SSAE%29+for+Nuclei+Detection+on+Breast+Cancer+Histopathology+Images&rft.volume=35&rft.issue=1&rft.pages=119-130&rft.date=2016-01&rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC4729702&rft_id=info%3Apmid%2F26208307&rft_id=info%3Adoi%2F10.1109%2FTMI.2015.2458702&rft.aulast=Xu&rft.aufirst=Jun&rft.au=Xiang%2C+Lei&rft.au=Liu%2C+Qingshan&rft.au=Gilmore%2C+Hannah&rft.au=Wu%2C+Jianzhong&rft.au=Tang%2C+Jinghai&rft.au=Madabhushi%2C+Anant&rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC4729702&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-47"><span class="mw-cite-backlink"><b><a href="#cite_ref-47">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFMartinez-MurciaOrtizGorrizRamirez2020">Martinez-Murcia, Francisco J.; Ortiz, Andres; Gorriz, Juan M.; Ramirez, Javier; Castillo-Barnes, Diego (2020). "Studying the Manifold Structure of Alzheimer's Disease: A Deep Learning Approach Using Convolutional Autoencoders". <i>IEEE Journal of Biomedical and Health Informatics</i>. <b>24</b> (1): 17–26. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1109%2FJBHI.2019.2914970" rel="nofollow">10.1109/JBHI.2019.2914970</a>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/31217131" rel="nofollow">31217131</a>. <a class="mw-redirect" href="/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a> <a class="external text" href="https://api.semanticscholar.org/CorpusID:195187846" rel="nofollow">195187846</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=IEEE+Journal+of+Biomedical+and+Health+Informatics&rft.atitle=Studying+the+Manifold+Structure+of+Alzheimer%27s+Disease%3A+A+Deep+Learning+Approach+Using+Convolutional+Autoencoders&rft.volume=24&rft.issue=1&rft.pages=17-26&rft.date=2020&rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A195187846&rft_id=info%3Apmid%2F31217131&rft_id=info%3Adoi%2F10.1109%2FJBHI.2019.2914970&rft.aulast=Martinez-Murcia&rft.aufirst=Francisco+J.&rft.au=Ortiz%2C+Andres&rft.au=Gorriz%2C+Juan+M.&rft.au=Ramirez%2C+Javier&rft.au=Castillo-Barnes%2C+Diego&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-48"><span class="mw-cite-backlink"><b><a href="#cite_ref-48">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFZengYuWangLi2017">Zeng, Kun; Yu, Jun; Wang, Ruxin; Li, Cuihua; Tao, Dacheng (January 2017). "Coupled Deep Autoencoder for Single Image Super-Resolution". <i>IEEE Transactions on Cybernetics</i>. <b>47</b> (1): 27–37. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1109%2FTCYB.2015.2501373" rel="nofollow">10.1109/TCYB.2015.2501373</a>. <a class="mw-redirect" href="/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a> <a class="external text" href="//www.worldcat.org/issn/2168-2267" rel="nofollow">2168-2267</a>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/26625442" rel="nofollow">26625442</a>. <a class="mw-redirect" href="/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a> <a class="external text" href="https://api.semanticscholar.org/CorpusID:20787612" rel="nofollow">20787612</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=IEEE+Transactions+on+Cybernetics&rft.atitle=Coupled+Deep+Autoencoder+for+Single+Image+Super-Resolution&rft.volume=47&rft.issue=1&rft.pages=27-37&rft.date=2017-01&rft.issn=2168-2267&rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A20787612&rft_id=info%3Apmid%2F26625442&rft_id=info%3Adoi%2F10.1109%2FTCYB.2015.2501373&rft.aulast=Zeng&rft.aufirst=Kun&rft.au=Yu%2C+Jun&rft.au=Wang%2C+Ruxin&rft.au=Li%2C+Cuihua&rft.au=Tao%2C+Dacheng&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-49"><span class="mw-cite-backlink"><b><a href="#cite_ref-49">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFZhavoronkov2019">Zhavoronkov, Alex (2019). "Deep learning enables rapid identification of potent DDR1 kinase inhibitors". <i>Nature Biotechnology</i>. <b>37</b> (9): 1038–1040. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1038%2Fs41587-019-0224-x" rel="nofollow">10.1038/s41587-019-0224-x</a>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/31477924" rel="nofollow">31477924</a>. <a class="mw-redirect" href="/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a> <a class="external text" href="https://api.semanticscholar.org/CorpusID:201716327" rel="nofollow">201716327</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Nature+Biotechnology&rft.atitle=Deep+learning+enables+rapid+identification+of+potent+DDR1+kinase+inhibitors&rft.volume=37&rft.issue=9&rft.pages=1038-1040&rft.date=2019&rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A201716327&rft_id=info%3Apmid%2F31477924&rft_id=info%3Adoi%2F10.1038%2Fs41587-019-0224-x&rft.aulast=Zhavoronkov&rft.aufirst=Alex&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-50"><span class="mw-cite-backlink"><b><a href="#cite_ref-50">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFGregory">Gregory, Barber. <a class="external text" href="https://www.wired.com/story/molecule-designed-ai-exhibits-druglike-qualities/" rel="nofollow">"A Molecule Designed By AI Exhibits 'Druglike' Qualities"</a>. <i>Wired</i>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Wired&rft.atitle=A+Molecule+Designed+By+AI+Exhibits+%27Druglike%27+Qualities&rft.aulast=Gregory&rft.aufirst=Barber&rft_id=https%3A%2F%2Fwww.wired.com%2Fstory%2Fmolecule-designed-ai-exhibits-druglike-qualities%2F&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-51"><span class="mw-cite-backlink"><b><a href="#cite_ref-51">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFBorysovRichPereira2019">Borysov, Stanislav S.; Rich, Jeppe; Pereira, Francisco C. (September 2019). "How to generate micro-agents? A deep generative modeling approach to population synthesis". <i>Transportation Research Part C: Emerging Technologies</i>. <b>106</b>: 73–97. <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1808.06910" rel="nofollow">1808.06910</a></span>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1016%2Fj.trc.2019.07.006" rel="nofollow">10.1016/j.trc.2019.07.006</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Transportation+Research+Part+C%3A+Emerging+Technologies&rft.atitle=How+to+generate+micro-agents%3F+A+deep+generative+modeling+approach+to+population+synthesis&rft.volume=106&rft.pages=73-97&rft.date=2019-09&rft_id=info%3Aarxiv%2F1808.06910&rft_id=info%3Adoi%2F10.1016%2Fj.trc.2019.07.006&rft.aulast=Borysov&rft.aufirst=Stanislav+S.&rft.au=Rich%2C+Jeppe&rft.au=Pereira%2C+Francisco+C.&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-52"><span class="mw-cite-backlink"><b><a href="#cite_ref-52">^</a></b></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFDeMaityGoelShitole2017">De, Shaunak; Maity, Abhishek; Goel, Vritti; Shitole, Sanjay; Bhattacharya, Avik (2017). "Predicting the popularity of instagram posts for a lifestyle magazine using deep learning". <i>2017 2nd IEEE International Conference on Communication Systems, Computing and IT Applications (CSCITA)</i>. pp. 174–177. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1109%2FCSCITA.2017.8066548" rel="nofollow">10.1109/CSCITA.2017.8066548</a>. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a> <a href="/wiki/Special:BookSources/978-1-5090-4381-1" title="Special:BookSources/978-1-5090-4381-1"><bdi>978-1-5090-4381-1</bdi></a>. <a class="mw-redirect" href="/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a> <a class="external text" href="https://api.semanticscholar.org/CorpusID:35350962" rel="nofollow">35350962</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&rft.genre=bookitem&rft.atitle=Predicting+the+popularity+of+instagram+posts+for+a+lifestyle+magazine+using+deep+learning&rft.btitle=2017+2nd+IEEE+International+Conference+on+Communication+Systems%2C+Computing+and+IT+Applications+%28CSCITA%29&rft.pages=174-177&rft.date=2017&rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A35350962&rft_id=info%3Adoi%2F10.1109%2FCSCITA.2017.8066548&rft.isbn=978-1-5090-4381-1&rft.aulast=De&rft.aufirst=Shaunak&rft.au=Maity%2C+Abhishek&rft.au=Goel%2C+Vritti&rft.au=Shitole%2C+Sanjay&rft.au=Bhattacharya%2C+Avik&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-53"><span class="mw-cite-backlink"><b><a href="#cite_ref-53">^</a></b></span> <span class="reference-text"><cite class="citation arxiv cs1" id="CITEREFChoBart_van_MerrienboerBahdanauBengio2014">Cho, Kyunghyun; Bart van Merrienboer; Bahdanau, Dzmitry; Bengio, Yoshua (2014). "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches". <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1409.1259" rel="nofollow">1409.1259</a></span> [<a class="external text" href="//arxiv.org/archive/cs.CL" rel="nofollow">cs.CL</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=preprint&rft.jtitle=arXiv&rft.atitle=On+the+Properties+of+Neural+Machine+Translation%3A+Encoder-Decoder+Approaches&rft.date=2014&rft_id=info%3Aarxiv%2F1409.1259&rft.aulast=Cho&rft.aufirst=Kyunghyun&rft.au=Bart+van+Merrienboer&rft.au=Bahdanau%2C+Dzmitry&rft.au=Bengio%2C+Yoshua&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-54"><span class="mw-cite-backlink"><b><a href="#cite_ref-54">^</a></b></span> <span class="reference-text"><cite class="citation arxiv cs1" id="CITEREFSutskeverVinyalsLe2014">Sutskever, Ilya; Vinyals, Oriol; Le, Quoc V. (2014). "Sequence to Sequence Learning with Neural Networks". <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1409.3215" rel="nofollow">1409.3215</a></span> [<a class="external text" href="//arxiv.org/archive/cs.CL" rel="nofollow">cs.CL</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=preprint&rft.jtitle=arXiv&rft.atitle=Sequence+to+Sequence+Learning+with+Neural+Networks&rft.date=2014&rft_id=info%3Aarxiv%2F1409.3215&rft.aulast=Sutskever&rft.aufirst=Ilya&rft.au=Vinyals%2C+Oriol&rft.au=Le%2C+Quoc+V.&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-55"><span class="mw-cite-backlink"><b><a href="#cite_ref-55">^</a></b></span> <span class="reference-text"><cite class="citation arxiv cs1" id="CITEREFHanKuang2018">Han, Lifeng; Kuang, Shaohui (2018). "Incorporating Chinese Radicals into Neural Machine Translation: Deeper Than Character Level". <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1805.01565" rel="nofollow">1805.01565</a></span> [<a class="external text" href="//arxiv.org/archive/cs.CL" rel="nofollow">cs.CL</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=preprint&rft.jtitle=arXiv&rft.atitle=Incorporating+Chinese+Radicals+into+Neural+Machine+Translation%3A+Deeper+Than+Character+Level&rft.date=2018&rft_id=info%3Aarxiv%2F1805.01565&rft.aulast=Han&rft.aufirst=Lifeng&rft.au=Kuang%2C+Shaohui&rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
</ol></div>
<div aria-labelledby="Noise_(physics_and_telecommunications)" class="navbox" role="navigation" style="padding:3px"><table class="nowraplinks mw-collapsible autocollapse navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th class="navbox-title" colspan="2" scope="col"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Noise" title="Template:Noise"><abbr style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;" title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Noise" title="Template talk:Noise"><abbr style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;" title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Noise&action=edit"><abbr style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;" title="Edit this template">e</abbr></a></li></ul></div><div id="Noise_(physics_and_telecommunications)" style="font-size:114%;margin:0 4em"><a href="/wiki/Noise" title="Noise">Noise</a> (physics and telecommunications)</div></th></tr><tr><th class="navbox-group" scope="row" style="width:1%">General</th><td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Acoustic_quieting" title="Acoustic quieting">Acoustic quieting</a></li>
<li><a href="/wiki/Distortion" title="Distortion">Distortion</a></li>
<li><a href="/wiki/Active_noise_control" title="Active noise control">Noise cancellation</a></li>
<li><a href="/wiki/Noise_control" title="Noise control">Noise control</a></li>
<li><a href="/wiki/Noise_measurement" title="Noise measurement">Noise measurement</a></li>
<li><a href="/wiki/Noise_power" title="Noise power">Noise power</a></li>
<li><a href="/wiki/Noise_reduction" title="Noise reduction">Noise reduction</a></li>
<li><a href="/wiki/Noise_temperature" title="Noise temperature">Noise temperature</a></li>
<li><a href="/wiki/Phase_distortion" title="Phase distortion">Phase distortion</a></li></ul>
</div></td></tr><tr><th class="navbox-group" scope="row" style="width:1%">Noise in...</th><td class="navbox-list navbox-even hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a class="mw-redirect" href="/wiki/Noise_(audio)" title="Noise (audio)">Audio</a></li>
<li><a href="/wiki/Architectural_acoustics" title="Architectural acoustics">Buildings</a></li>
<li><a href="/wiki/Noise_(electronics)" title="Noise (electronics)">Electronics</a></li>
<li><a href="/wiki/Noise_pollution" title="Noise pollution">Environment</a></li>
<li><a href="/wiki/Noise_regulation" title="Noise regulation">Government regulation</a></li>
<li><a href="/wiki/Health_effects_from_noise" title="Health effects from noise">Human health</a></li>
<li><a href="/wiki/Image_noise" title="Image noise">Images</a></li>
<li><a class="mw-redirect" href="/wiki/Noise_(radio)" title="Noise (radio)">Radio</a></li>
<li><a href="/wiki/Soundproofing" title="Soundproofing">Rooms</a></li>
<li><a href="/wiki/Noise_and_vibration_on_maritime_vessels" title="Noise and vibration on maritime vessels">Ships</a></li>
<li><a href="/wiki/Sound_masking" title="Sound masking">Sound masking</a></li>
<li><a href="/wiki/Noise_barrier" title="Noise barrier">Transportation</a></li>
<li><a href="/wiki/Noise_(video)" title="Noise (video)">Video</a></li></ul>
</div></td></tr><tr><th class="navbox-group" scope="row" style="width:1%">Class of noise</th><td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Additive_white_Gaussian_noise" title="Additive white Gaussian noise">Additive white Gaussian noise</a> (AWGN)</li>
<li><a href="/wiki/Atmospheric_noise" title="Atmospheric noise">Atmospheric noise</a></li>
<li><a href="/wiki/Background_noise" title="Background noise">Background noise</a></li>
<li><a href="/wiki/Brownian_noise" title="Brownian noise">Brownian noise</a></li>
<li><a href="/wiki/Burst_noise" title="Burst noise">Burst noise</a></li>
<li><a href="/wiki/Cosmic_noise" title="Cosmic noise">Cosmic noise</a></li>
<li><a href="/wiki/Flicker_noise" title="Flicker noise">Flicker noise</a></li>
<li><a href="/wiki/Gaussian_noise" title="Gaussian noise">Gaussian noise</a></li>
<li><a href="/wiki/Grey_noise" title="Grey noise">Grey noise</a></li>
<li><a href="/wiki/Jitter" title="Jitter">Jitter</a></li>
<li><a href="/wiki/Johnson%E2%80%93Nyquist_noise" title="Johnson–Nyquist noise">Johnson–Nyquist noise</a> (thermal noise)</li>
<li><a href="/wiki/Pink_noise" title="Pink noise">Pink noise</a></li>
<li><a class="mw-redirect" href="/wiki/Quantization_error" title="Quantization error">Quantization error</a> (or q. noise)</li>
<li><a href="/wiki/Shot_noise" title="Shot noise">Shot noise</a></li>
<li><a href="/wiki/White_noise" title="White noise">White noise</a></li>
<li>Coherent noise
<ul><li><a href="/wiki/Value_noise" title="Value noise">Value noise</a></li>
<li><a href="/wiki/Gradient_noise" title="Gradient noise">Gradient noise</a></li>
<li><a href="/wiki/Worley_noise" title="Worley noise">Worley noise</a></li></ul></li></ul>
</div></td></tr><tr><th class="navbox-group" scope="row" style="width:1%">Engineering <br/>terms</th><td class="navbox-list navbox-even hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Channel_noise_level" title="Channel noise level">Channel noise level</a></li>
<li><a href="/wiki/Circuit_noise_level" title="Circuit noise level">Circuit noise level</a></li>
<li><a href="/wiki/Effective_input_noise_temperature" title="Effective input noise temperature">Effective input noise temperature</a></li>
<li><a href="/wiki/Equivalent_noise_resistance" title="Equivalent noise resistance">Equivalent noise resistance</a></li>
<li><a href="/wiki/Equivalent_pulse_code_modulation_noise" title="Equivalent pulse code modulation noise">Equivalent pulse code modulation noise</a></li>
<li><a class="mw-redirect" href="/wiki/Impulse_noise_(audio)" title="Impulse noise (audio)">Impulse noise (audio)</a></li>
<li><a href="/wiki/Noise_figure" title="Noise figure">Noise figure</a></li>
<li><a href="/wiki/Noise_floor" title="Noise floor">Noise floor</a></li>
<li><a href="/wiki/Noise_shaping" title="Noise shaping">Noise shaping</a></li>
<li><a href="/wiki/Noise_spectral_density" title="Noise spectral density">Noise spectral density</a></li>
<li><a href="/wiki/Noise,_vibration,_and_harshness" title="Noise, vibration, and harshness">Noise, vibration, and harshness</a> (NVH)</li>
<li><a href="/wiki/Phase_noise" title="Phase noise">Phase noise</a></li>
<li><a href="/wiki/Pseudorandom_noise" title="Pseudorandom noise">Pseudorandom noise</a></li>
<li><a class="mw-redirect" href="/wiki/Statistical_noise" title="Statistical noise">Statistical noise</a></li></ul>
</div></td></tr><tr><th class="navbox-group" scope="row" style="width:1%">Ratios</th><td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Carrier-to-noise_ratio" title="Carrier-to-noise ratio">Carrier-to-noise ratio</a> (<i>C</i>/<i>N</i>)</li>
<li><a class="mw-redirect" href="/wiki/Carrier-to-receiver_noise_density" title="Carrier-to-receiver noise density">Carrier-to-receiver noise density</a> (<i>C</i>/<i>kT</i>)</li>
<li><i><a class="mw-redirect" href="/wiki/DBrnC" title="DBrnC">dBrnC</a></i></li>
<li><i><a href="/wiki/Eb/N0" title="Eb/N0">E<sub>b</sub>/N<sub>0</sub></a></i> (energy per bit to noise density)</li>
<li><i><a href="/wiki/Eb/N0#Relation_to_Es.2FN0" title="Eb/N0">E<sub>s</sub>/N<sub>0</sub></a></i> (energy per symbol to noise density)</li>
<li><a href="/wiki/Modulation_error_ratio" title="Modulation error ratio">Modulation error ratio</a> (<i>MER</i>)</li>
<li><a href="/wiki/SINAD" title="SINAD">Signal, noise and distortion</a> (<i>SINAD</i>)</li>
<li><a href="/wiki/Signal-to-interference_ratio" title="Signal-to-interference ratio">Signal-to-interference ratio</a> (<i>S</i>/<i>I</i>)</li>
<li><a href="/wiki/Signal-to-noise_ratio" title="Signal-to-noise ratio">Signal-to-noise ratio</a> (<i>S</i>/<i>N</i>, <i>SNR</i>)</li>
<li><a href="/wiki/Signal-to-noise_ratio_(imaging)" title="Signal-to-noise ratio (imaging)">Signal-to-noise ratio (imaging)</a></li>
<li><a href="/wiki/Signal-to-interference-plus-noise_ratio" title="Signal-to-interference-plus-noise ratio">Signal-to-interference-plus-noise ratio</a> (<i>SINR</i>)</li>
<li><a href="/wiki/Signal-to-quantization-noise_ratio" title="Signal-to-quantization-noise ratio">Signal-to-quantization-noise ratio</a> (<i>SQNR</i>)</li>
<li><a href="/wiki/Contrast-to-noise_ratio" title="Contrast-to-noise ratio">Contrast-to-noise ratio</a> (<i>CNR</i>)</li></ul>
</div></td></tr><tr><th class="navbox-group" scope="row" style="width:1%">Related topics</th><td class="navbox-list navbox-even hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/List_of_noise_topics" title="List of noise topics">List of noise topics</a></li>
<li><a href="/wiki/Acoustics" title="Acoustics">Acoustics</a></li>
<li><a href="/wiki/Colors_of_noise" title="Colors of noise">Colors of noise</a></li>
<li><a href="/wiki/Interference_(communication)" title="Interference (communication)">Interference (communication)</a></li>
<li><a href="/wiki/Noise_generator" title="Noise generator">Noise generator</a></li>
<li><a href="/wiki/Spectrum_analyzer" title="Spectrum analyzer">Spectrum analyzer</a></li>
<li><a href="/wiki/Thermal_radiation" title="Thermal radiation">Thermal radiation</a></li></ul>
</div></td></tr><tr><th class="navbox-group" scope="row" style="width:1%">Denoise <br/>methods</th><td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th class="navbox-group" scope="row" style="width:1%">General</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Low-pass_filter" title="Low-pass filter">Low-pass filter</a></li>
<li><a href="/wiki/Median_filter" title="Median filter">Median filter</a></li>
<li><a href="/wiki/Total_variation_denoising" title="Total variation denoising">Total variation denoising</a></li>
<li><a href="/wiki/Wavelet#Wavelet_denoising" title="Wavelet">Wavelet denoising</a></li></ul>
</div></td></tr><tr><th class="navbox-group" scope="row" style="width:1%">2D (Image)</th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Gaussian_blur" title="Gaussian blur">Gaussian blur</a></li>
<li><a href="/wiki/Anisotropic_diffusion" title="Anisotropic diffusion">Anisotropic diffusion</a></li>
<li><a href="/wiki/Bilateral_filter" title="Bilateral filter">Bilateral filter</a></li>
<li><a href="/wiki/Non-local_means" title="Non-local means">Non-local means</a></li>
<li><a href="/wiki/Block-matching_and_3D_filtering" title="Block-matching and 3D filtering">Block-matching and 3D filtering</a> (BM3D)</li>
<li><a href="/wiki/Shrinkage_Fields_(image_restoration)" title="Shrinkage Fields (image restoration)">Shrinkage Fields</a></li>
<li><a href="/wiki/Autoencoder#Denoising_autoencoder_(DAE)" title="Autoencoder">Denoising autoencoder</a> (DAE)</li>
<li><a href="/wiki/Deep_Image_Prior" title="Deep Image Prior">Deep Image Prior</a></li></ul>
</div></td></tr></tbody></table><div></div></td></tr></tbody></table></div>
<!-- 
NewPP limit report
Parsed by mw1400
Cached time: 20200818164443
Cache expiry: 2592000
Dynamic content: false
Complications: [vary‐revision‐sha1]
CPU time usage: 0.680 seconds
Real time usage: 1.009 seconds
Preprocessor visited node count: 3700/1000000
Post‐expand include size: 144249/2097152 bytes
Template argument size: 2212/2097152 bytes
Highest expansion depth: 16/40
Expensive parser function count: 2/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 185561/5000000 bytes
Lua time usage: 0.247/10.000 seconds
Lua memory usage: 5.3 MB/50 MB
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  571.700      1 -total
 57.92%  331.120      1 Template:Reflist
 32.76%  187.289     22 Template:Cite_journal
 10.07%   57.573     14 Template:Cite_arxiv
  8.86%   50.630      1 Template:Machine_learning_bar
  8.09%   46.250      1 Template:Sidebar_with_collapsible_lists
  7.04%   40.241      1 Template:Use_dmy_dates
  5.69%   32.548      1 Template:Split_section
  5.64%   32.269      1 Template:Distinguish
  5.11%   29.237      1 Template:Split_portions
-->
<!-- Saved in parser cache with key enwiki:pcache:idhash:6836612-0!canonical!math=5 and timestamp 20200818164453 and revision id 973684205
 -->
</div><noscript><img alt="" height="1" src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" style="border: none; position: absolute;" title="" width="1"/></noscript>
<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Autoencoder&oldid=973684205">https://en.wikipedia.org/w/index.php?title=Autoencoder&oldid=973684205</a>"</div></div>
<div class="catlinks" data-mw="interface" id="catlinks"><div class="mw-normal-catlinks" id="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Artificial_neural_networks" title="Category:Artificial neural networks">Artificial neural networks</a></li><li><a href="/wiki/Category:Unsupervised_learning" title="Category:Unsupervised learning">Unsupervised learning</a></li><li><a href="/wiki/Category:Dimension_reduction" title="Category:Dimension reduction">Dimension reduction</a></li></ul></div><div class="mw-hidden-catlinks mw-hidden-cats-hidden" id="mw-hidden-catlinks">Hidden categories: <ul><li><a href="/wiki/Category:CS1_errors:_missing_periodical" title="Category:CS1 errors: missing periodical">CS1 errors: missing periodical</a></li><li><a href="/wiki/Category:Use_dmy_dates_from_March_2020" title="Category:Use dmy dates from March 2020">Use dmy dates from March 2020</a></li><li><a href="/wiki/Category:Articles_to_be_split_from_May_2020" title="Category:Articles to be split from May 2020">Articles to be split from May 2020</a></li><li><a href="/wiki/Category:All_articles_to_be_split" title="Category:All articles to be split">All articles to be split</a></li></ul></div></div>
</div>
</div>
<div id="mw-data-after-content">
<div class="read-more-container"></div>
</div>
<div id="mw-navigation">
<h2>Navigation menu</h2>
<div id="mw-head">
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav aria-labelledby="p-personal-label" class="vector-menu" id="p-personal" role="navigation">
<h3 id="p-personal-label">
<span>Personal tools</span>
</h3>
<!-- Please do not use the .body class, it is deprecated. -->
<div class="body vector-menu-content">
<!-- Please do not use the .menu class, it is deprecated. -->
<ul class="vector-menu-content-list"><li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a accesskey="n" href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]">Talk</a></li><li id="pt-anoncontribs"><a accesskey="y" href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&returnto=Autoencoder" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a accesskey="o" href="/w/index.php?title=Special:UserLogin&returnto=Autoencoder" title="You're encouraged to log in; however, it's not mandatory. [o]">Log in</a></li></ul>
</div>
</nav>
<div id="left-navigation">
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav aria-labelledby="p-namespaces-label" class="vector-menu vector-menu-tabs vectorTabs" id="p-namespaces" role="navigation">
<h3 id="p-namespaces-label">
<span>Namespaces</span>
</h3>
<!-- Please do not use the .body class, it is deprecated. -->
<div class="body vector-menu-content">
<!-- Please do not use the .menu class, it is deprecated. -->
<ul class="vector-menu-content-list"><li class="selected" id="ca-nstab-main"><a accesskey="c" href="/wiki/Autoencoder" title="View the content page [c]">Article</a></li><li id="ca-talk"><a accesskey="t" href="/wiki/Talk:Autoencoder" rel="discussion" title="Discuss improvements to the content page [t]">Talk</a></li></ul>
</div>
</nav>
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav aria-labelledby="p-variants-label" class="vector-menu-empty emptyPortlet vector-menu vector-menu-dropdown vectorMenu" id="p-variants" role="navigation">
<input aria-labelledby="p-variants-label" class="vector-menu-checkbox vectorMenuCheckbox" type="checkbox"/>
<h3 id="p-variants-label">
<span>Variants</span>
</h3>
<!-- Please do not use the .body class, it is deprecated. -->
<div class="body vector-menu-content">
<!-- Please do not use the .menu class, it is deprecated. -->
<ul class="menu vector-menu-content-list"></ul>
</div>
</nav>
</div>
<div id="right-navigation">
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav aria-labelledby="p-views-label" class="vector-menu vector-menu-tabs vectorTabs" id="p-views" role="navigation">
<h3 id="p-views-label">
<span>Views</span>
</h3>
<!-- Please do not use the .body class, it is deprecated. -->
<div class="body vector-menu-content">
<!-- Please do not use the .menu class, it is deprecated. -->
<ul class="vector-menu-content-list"><li class="selected" id="ca-view"><a href="/wiki/Autoencoder">Read</a></li><li id="ca-edit"><a accesskey="e" href="/w/index.php?title=Autoencoder&action=edit" title="Edit this page [e]">Edit</a></li><li id="ca-history"><a accesskey="h" href="/w/index.php?title=Autoencoder&action=history" title="Past revisions of this page [h]">View history</a></li></ul>
</div>
</nav>
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav aria-labelledby="p-cactions-label" class="vector-menu-empty emptyPortlet vector-menu vector-menu-dropdown vectorMenu" id="p-cactions" role="navigation">
<input aria-labelledby="p-cactions-label" class="vector-menu-checkbox vectorMenuCheckbox" type="checkbox"/>
<h3 id="p-cactions-label">
<span>More</span>
</h3>
<!-- Please do not use the .body class, it is deprecated. -->
<div class="body vector-menu-content">
<!-- Please do not use the .menu class, it is deprecated. -->
<ul class="menu vector-menu-content-list"></ul>
</div>
</nav>
<div id="p-search" role="search">
<h3>
<label for="searchInput">Search</label>
</h3>
<form action="/w/index.php" id="searchform">
<div id="simpleSearch">
<input accesskey="f" id="searchInput" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" type="search"/>
<input name="title" type="hidden" value="Special:Search"/>
<input class="searchButton mw-fallbackSearchButton" id="mw-searchButton" name="fulltext" title="Search Wikipedia for this text" type="submit" value="Search">
<input class="searchButton" id="searchButton" name="go" title="Go to a page with this exact name if it exists" type="submit" value="Go"/>
</input></div>
</form>
</div>
</div>
</div>
<div id="mw-panel">
<div id="p-logo" role="banner">
<a class="mw-wiki-logo" href="/wiki/Main_Page" title="Visit the main page"></a>
</div>
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav aria-labelledby="p-navigation-label" class="vector-menu vector-menu-portal portal portal-first" id="p-navigation" role="navigation">
<h3 id="p-navigation-label">
<span>Navigation</span>
</h3>
<!-- Please do not use the .body class, it is deprecated. -->
<div class="body vector-menu-content">
<!-- Please do not use the .menu class, it is deprecated. -->
<ul class="vector-menu-content-list"><li id="n-mainpage-description"><a accesskey="z" href="/wiki/Main_Page" title="Visit the main page [z]">Main page</a></li><li id="n-contents"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a accesskey="x" href="/wiki/Special:Random" title="Visit a randomly selected article [x]">Random article</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Learn about Wikipedia and how it works">About Wikipedia</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact us</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en" title="Support us by donating to the Wikimedia Foundation">Donate</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li></ul>
</div>
</nav>
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav aria-labelledby="p-interaction-label" class="vector-menu vector-menu-portal portal" id="p-interaction" role="navigation">
<h3 id="p-interaction-label">
<span>Contribute</span>
</h3>
<!-- Please do not use the .body class, it is deprecated. -->
<div class="body vector-menu-content">
<!-- Please do not use the .menu class, it is deprecated. -->
<ul class="vector-menu-content-list"><li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a accesskey="r" href="/wiki/Special:RecentChanges" title="A list of recent changes to Wikipedia [r]">Recent changes</a></li><li id="n-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Add images or other media for use on Wikipedia">Upload file</a></li></ul>
</div>
</nav>
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav aria-labelledby="p-tb-label" class="vector-menu vector-menu-portal portal" id="p-tb" role="navigation">
<h3 id="p-tb-label">
<span>Tools</span>
</h3>
<!-- Please do not use the .body class, it is deprecated. -->
<div class="body vector-menu-content">
<!-- Please do not use the .menu class, it is deprecated. -->
<ul class="vector-menu-content-list"><li id="t-whatlinkshere"><a accesskey="j" href="/wiki/Special:WhatLinksHere/Autoencoder" title="List of all English Wikipedia pages containing links to this page [j]">What links here</a></li><li id="t-recentchangeslinked"><a accesskey="k" href="/wiki/Special:RecentChangesLinked/Autoencoder" rel="nofollow" title="Recent changes in pages linked from this page [k]">Related changes</a></li><li id="t-upload"><a accesskey="u" href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]">Upload file</a></li><li id="t-specialpages"><a accesskey="q" href="/wiki/Special:SpecialPages" title="A list of all special pages [q]">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Autoencoder&oldid=973684205" title="Permanent link to this revision of this page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Autoencoder&action=info" title="More information about this page">Page information</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&page=Autoencoder&id=973684205&wpFormIdentifier=titleform" title="Information on how to cite this page">Cite this page</a></li><li id="t-wikibase"><a accesskey="g" href="https://www.wikidata.org/wiki/Special:EntityPage/Q786435" title="Structured data on this page hosted by Wikidata [g]">Wikidata item</a></li></ul>
</div>
</nav>
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav aria-labelledby="p-coll-print_export-label" class="vector-menu vector-menu-portal portal" id="p-coll-print_export" role="navigation">
<h3 id="p-coll-print_export-label">
<span>Print/export</span>
</h3>
<!-- Please do not use the .body class, it is deprecated. -->
<div class="body vector-menu-content">
<!-- Please do not use the .menu class, it is deprecated. -->
<ul class="vector-menu-content-list"><li id="coll-download-as-rl"><a href="/w/index.php?title=Special:ElectronPdf&page=Autoencoder&action=show-download-screen" title="Download this page as a PDF file">Download as PDF</a></li><li id="t-print"><a accesskey="p" href="/w/index.php?title=Autoencoder&printable=yes" title="Printable version of this page [p]">Printable version</a></li></ul>
</div>
</nav>
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav aria-labelledby="p-lang-label" class="vector-menu vector-menu-portal portal" id="p-lang" role="navigation">
<h3 id="p-lang-label">
<span>Languages</span>
</h3>
<!-- Please do not use the .body class, it is deprecated. -->
<div class="body vector-menu-content">
<!-- Please do not use the .menu class, it is deprecated. -->
<ul class="vector-menu-content-list"><li class="interlanguage-link interwiki-de"><a class="interlanguage-link-target" href="https://de.wikipedia.org/wiki/Autoencoder" hreflang="de" lang="de" title="Autoencoder – German">Deutsch</a></li><li class="interlanguage-link interwiki-fa"><a class="interlanguage-link-target" href="https://fa.wikipedia.org/wiki/%D8%AE%D9%88%D8%AF%D8%B1%D9%85%D8%B2%DA%AF%D8%B0%D8%A7%D8%B1" hreflang="fa" lang="fa" title="خودرمزگذار – Persian">فارسی</a></li><li class="interlanguage-link interwiki-fr"><a class="interlanguage-link-target" href="https://fr.wikipedia.org/wiki/Auto-encodeur" hreflang="fr" lang="fr" title="Auto-encodeur – French">Français</a></li><li class="interlanguage-link interwiki-ja"><a class="interlanguage-link-target" href="https://ja.wikipedia.org/wiki/%E3%82%AA%E3%83%BC%E3%83%88%E3%82%A8%E3%83%B3%E3%82%B3%E3%83%BC%E3%83%80" hreflang="ja" lang="ja" title="オートエンコーダ – Japanese">日本語</a></li><li class="interlanguage-link interwiki-ru"><a class="interlanguage-link-target" href="https://ru.wikipedia.org/wiki/%D0%90%D0%B2%D1%82%D0%BE%D0%BA%D0%BE%D0%B4%D0%B8%D1%80%D0%BE%D0%B2%D1%89%D0%B8%D0%BA" hreflang="ru" lang="ru" title="Автокодировщик – Russian">Русский</a></li><li class="interlanguage-link interwiki-uk"><a class="interlanguage-link-target" href="https://uk.wikipedia.org/wiki/%D0%90%D0%B2%D1%82%D0%BE%D0%BA%D0%BE%D0%B4%D1%83%D0%B2%D0%B0%D0%BB%D1%8C%D0%BD%D0%B8%D0%BA" hreflang="uk" lang="uk" title="Автокодувальник – Ukrainian">Українська</a></li><li class="interlanguage-link interwiki-zh-yue"><a class="interlanguage-link-target" href="https://zh-yue.wikipedia.org/wiki/%E8%87%AA%E7%B7%A8%E7%A2%BC%E5%99%A8" hreflang="yue" lang="yue" title="自編碼器 – Cantonese">粵語</a></li><li class="interlanguage-link interwiki-zh"><a class="interlanguage-link-target" href="https://zh.wikipedia.org/wiki/%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8" hreflang="zh" lang="zh" title="自编码器 – Chinese">中文</a></li></ul>
<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a class="wbc-editpage" href="https://www.wikidata.org/wiki/Special:EntityPage/Q786435#sitelinks-wikipedia" title="Edit interlanguage links">Edit links</a></span></div>
</div>
</nav>
</div>
</div>
<footer class="mw-footer" id="footer" role="contentinfo">
<ul id="footer-info">
<li id="footer-info-lastmod"> This page was last edited on 18 August 2020, at 16:44<span class="anonymous-show"> (UTC)</span>.</li>
<li id="footer-info-copyright">Text is available under the <a href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License" rel="license">Creative Commons Attribution-ShareAlike License</a><a href="//creativecommons.org/licenses/by-sa/3.0/" rel="license" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
</ul>
<ul id="footer-places">
<li id="footer-places-privacy"><a class="extiw" href="https://foundation.wikimedia.org/wiki/Privacy_policy" title="wmf:Privacy policy">Privacy policy</a></li>
<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
<li id="footer-places-mobileview"><a class="noprint stopMobileRedirectToggle" href="//en.m.wikipedia.org/w/index.php?title=Autoencoder&mobileaction=toggle_view_mobile">Mobile view</a></li>
<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>
<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
</ul>
<ul class="noprint" id="footer-icons">
<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img alt="Wikimedia Foundation" height="31" loading="lazy" src="/static/images/footer/wikimedia-button.png" srcset="/static/images/footer/wikimedia-button-1.5x.png 1.5x, /static/images/footer/wikimedia-button-2x.png 2x" width="88"/></a></li>
<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img alt="Powered by MediaWiki" height="31" loading="lazy" src="/static/images/footer/poweredby_mediawiki_88x31.png" srcset="/static/images/footer/poweredby_mediawiki_132x47.png 1.5x, /static/images/footer/poweredby_mediawiki_176x62.png 2x" width="88"/></a></li>
</ul>
<div style="clear: both;"></div>
</footer>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.680","walltime":"1.009","ppvisitednodes":{"value":3700,"limit":1000000},"postexpandincludesize":{"value":144249,"limit":2097152},"templateargumentsize":{"value":2212,"limit":2097152},"expansiondepth":{"value":16,"limit":40},"expensivefunctioncount":{"value":2,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":185561,"limit":5000000},"entityaccesscount":{"value":0,"limit":400},"timingprofile":["100.00%  571.700      1 -total"," 57.92%  331.120      1 Template:Reflist"," 32.76%  187.289     22 Template:Cite_journal"," 10.07%   57.573     14 Template:Cite_arxiv","  8.86%   50.630      1 Template:Machine_learning_bar","  8.09%   46.250      1 Template:Sidebar_with_collapsible_lists","  7.04%   40.241      1 Template:Use_dmy_dates","  5.69%   32.548      1 Template:Split_section","  5.64%   32.269      1 Template:Distinguish","  5.11%   29.237      1 Template:Split_portions"]},"scribunto":{"limitreport-timeusage":{"value":"0.247","limit":"10.000"},"limitreport-memusage":{"value":5560743,"limit":52428800}},"cachereport":{"origin":"mw1400","timestamp":"20200818164443","ttl":2592000,"transientcontent":false}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"Autoencoder","url":"https:\/\/en.wikipedia.org\/wiki\/Autoencoder","sameAs":"http:\/\/www.wikidata.org\/entity\/Q786435","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q786435","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2006-09-04T09:11:50Z","dateModified":"2020-08-18T16:44:53Z"}</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":117,"wgHostname":"mw1271"});});</script>
</body></html>