<!DOCTYPE html>

<html class="client-nojs" dir="ltr" lang="en">
<head>
<meta charset="utf-8"/>
<title>Long short-term memory - Wikipedia</title>
<script>document.documentElement.className="client-js";RLCONF={"wgBreakFrames":!1,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"7cb26faa-2f8c-4813-a883-a18fb455f577","wgCSPNonce":!1,"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":!1,"wgNamespaceNumber":0,"wgPageName":"Long_short-term_memory","wgTitle":"Long short-term memory","wgCurRevisionId":974048165,"wgRevisionId":974048165,"wgArticleId":10711453,"wgIsArticle":!0,"wgIsRedirect":!1,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["All articles with unsourced statements","Articles with unsourced statements from October 2017","Artificial neural networks"],"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"Long_short-term_memory","wgRelevantArticleId":10711453,
"wgIsProbablyEditable":!0,"wgRelevantPageIsProbablyEditable":!0,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgMediaViewerOnClick":!0,"wgMediaViewerEnabledByDefault":!0,"wgPopupsReferencePreviews":!1,"wgPopupsConflictsWithNavPopupGadget":!1,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":!0,"nearby":!0,"watchlist":!0,"tagline":!1},"wgWMESchemaEditAttemptStepOversample":!1,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralAuthMobileDomain":!1,"wgEditSubmitButtonLabelPublish":!0,"wgULSPosition":"interlanguage","wgWikibaseItemId":"Q6673524"};RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"loading","ext.cite.styles":"ready","ext.math.styles":"ready","skins.vector.styles.legacy":"ready","mediawiki.toc.styles":"ready",
"ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","wikibase.client.init":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements","ext.math.scripts","site","mediawiki.page.ready","mediawiki.toc","skins.vector.legacy.js","ext.gadget.ReferenceTooltips","ext.gadget.charinsert","ext.gadget.extra-toolbar-buttons","ext.gadget.refToolbar","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.compactlinks","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.options@1hzgi",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});});</script>
<link href="/w/load.php?lang=en&modules=ext.cite.styles%7Cext.math.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.toc.styles%7Cskins.vector.styles.legacy%7Cwikibase.client.init&only=styles&skin=vector" rel="stylesheet"/>
<script async="" src="/w/load.php?lang=en&modules=startup&only=scripts&raw=1&skin=vector"></script>
<meta content="" name="ResourceLoaderDynamicStyles"/>
<link href="/w/load.php?lang=en&modules=site.styles&only=styles&skin=vector" rel="stylesheet"/>
<meta content="MediaWiki 1.36.0-wmf.5" name="generator"/>
<meta content="origin" name="referrer"/>
<meta content="origin-when-crossorigin" name="referrer"/>
<meta content="origin-when-cross-origin" name="referrer"/>
<meta content="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3b/The_LSTM_cell.png/1200px-The_LSTM_cell.png" property="og:image"/>
<link href="//en.m.wikipedia.org/wiki/Long_short-term_memory" media="only screen and (max-width: 720px)" rel="alternate"/>
<link href="/w/index.php?title=Long_short-term_memory&action=edit" rel="alternate" title="Edit this page" type="application/x-wiki"/>
<link href="/w/index.php?title=Long_short-term_memory&action=edit" rel="edit" title="Edit this page"/>
<link href="/static/apple-touch/wikipedia.png" rel="apple-touch-icon"/>
<link href="/static/favicon/wikipedia.ico" rel="shortcut icon"/>
<link href="/w/opensearch_desc.php" rel="search" title="Wikipedia (en)" type="application/opensearchdescription+xml"/>
<link href="//en.wikipedia.org/w/api.php?action=rsd" rel="EditURI" type="application/rsd+xml"/>
<link href="//creativecommons.org/licenses/by-sa/3.0/" rel="license"/>
<link href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="canonical"/>
<link href="//login.wikimedia.org" rel="dns-prefetch"/>
<link href="//meta.wikimedia.org" rel="dns-prefetch"/>
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Long_short-term_memory rootpage-Long_short-term_memory skin-vector action-view skin-vector-legacy"><div class="noprint" id="mw-page-base"></div>
<div class="noprint" id="mw-head-base"></div>
<div class="mw-body" id="content" role="main">
<a id="top"></a>
<div class="mw-body-content" id="siteNotice"><!-- CentralNotice --></div>
<div class="mw-indicators mw-body-content">
</div>
<h1 class="firstHeading" id="firstHeading" lang="en">Long short-term memory</h1>
<div class="mw-body-content" id="bodyContent">
<div class="noprint" id="siteSub">From Wikipedia, the free encyclopedia</div>
<div id="contentSub"></div>
<div id="contentSub2"></div>
<div id="jump-to-nav"></div>
<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
<a class="mw-jump-link" href="#searchInput">Jump to search</a>
<div class="mw-content-ltr" dir="ltr" id="mw-content-text" lang="en"><div class="mw-parser-output"><table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:22.0em;margin:0 0 1.0em 1.0em;background:#f8f9fa;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%"><tbody><tr><td style="padding-top:0.4em;line-height:1.2em">Part of a series on</td></tr><tr><th style="padding:0.2em 0.4em 0.2em;padding-top:0;font-size:145%;line-height:1.2em"><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a><br/>and<br/><a href="/wiki/Data_mining" title="Data mining">data mining</a></th></tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Problems</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>
<li><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>
<li><a href="/wiki/Automated_machine_learning" title="Automated machine learning">AutoML</a></li>
<li><a href="/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>
<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>
<li><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>
<li><a href="/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></li>
<li><a href="/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>
<li><a href="/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li>
<li><a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">Semi-supervised learning</a></li>
<li><a href="/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li>
<li><a href="/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></li>
<li><a href="/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><div style="display:inline-block; padding:0.1em 0;line-height:1.2em;"><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br/><style data-mw-deduplicate="TemplateStyles:r886047488">.mw-parser-output .nobold{font-weight:normal}</style><span class="nobold"><span style="font-size:85%;">(<b><a href="/wiki/Statistical_classification" title="Statistical classification">classification</a></b> • <b><a href="/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</span></span> </div></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>
<li><a href="/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a>
<ul><li><a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a></li>
<li><a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a></li>
<li><a href="/wiki/Random_forest" title="Random forest">Random forest</a></li></ul></li>
<li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>
<li><a href="/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>
<li><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>
<li><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural networks</a></li>
<li><a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>
<li><a href="/wiki/Perceptron" title="Perceptron">Perceptron</a></li>
<li><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li>
<li><a class="mw-redirect" href="/wiki/Support-vector_machine" title="Support-vector machine">Support vector machine (SVM)</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/BIRCH" title="BIRCH">BIRCH</a></li>
<li><a class="mw-redirect" href="/wiki/CURE_data_clustering_algorithm" title="CURE data clustering algorithm">CURE</a></li>
<li><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li>
<li><a href="/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means</a></li>
<li><a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation–maximization algorithm">Expectation–maximization (EM)</a></li>
<li><br/><a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>
<li><a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>
<li><a class="mw-redirect" href="/wiki/Mean-shift" title="Mean-shift">Mean-shift</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>
<li><a href="/wiki/Canonical_correlation" title="Canonical correlation">CCA</a></li>
<li><a href="/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>
<li><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>
<li><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>
<li><a href="/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>
<li><a href="/wiki/Proper_generalized_decomposition" title="Proper generalized decomposition">PGD</a></li>
<li><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Graphical_model" title="Graphical model">Graphical models</a>
<ul><li><a href="/wiki/Bayesian_network" title="Bayesian network">Bayes net</a></li>
<li><a href="/wiki/Conditional_random_field" title="Conditional random field">Conditional random field</a></li>
<li><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov</a></li></ul></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a class="mw-redirect" href="/wiki/K-nearest_neighbors_classification" title="K-nearest neighbors classification"><i>k</i>-NN</a></li>
<li><a href="/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural network</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="/wiki/DeepDream" title="DeepDream">DeepDream</a></li>
<li><a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron</a></li>
<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">RNN</a>
<ul><li><a class="mw-selflink selflink">LSTM</a></li>
<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">GRU</a></li></ul></li>
<li><a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted Boltzmann machine</a></li>
<li><a href="/wiki/Generative_adversarial_network" title="Generative adversarial network">GAN</a></li>
<li><a href="/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>
<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a>
<ul><li><a href="/wiki/U-Net" title="U-Net">U-Net</a></li></ul></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>
<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">SARSA</a></li>
<li><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference (TD)</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Theory</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a class="mw-redirect" href="/wiki/Bias%E2%80%93variance_dilemma" title="Bias–variance dilemma">Bias–variance dilemma</a></li>
<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>
<li><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>
<li><a href="/wiki/Occam_learning" title="Occam learning">Occam learning</a></li>
<li><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>
<li><a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>
<li><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik–Chervonenkis theory">VC theory</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Machine-learning venues</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NeurIPS</a></li>
<li><a href="/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a></li>
<li><a href="/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">ML</a></li>
<li><a href="/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">JMLR</a></li>
<li><a class="external text" href="https://arxiv.org/list/cs.LG/recent" rel="nofollow">ArXiv:cs.LG</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Related articles</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/List_of_datasets_for_machine-learning_research" title="List of datasets for machine-learning research">List of datasets for machine-learning research</a></li>
<li><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">Outline of machine learning</a></li></ul>
</div></div></div></td>
</tr><tr><td style="text-align:right;font-size:115%;padding-top: 0.6em;"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Machine_learning_bar" title="Template:Machine learning bar"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Machine_learning_bar" title="Template talk:Machine learning bar"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&action=edit"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>
<div class="thumb tright"><div class="thumbinner" style="width:302px;"><a class="image" href="/wiki/File:The_LSTM_cell.png"><img alt="" class="thumbimage" data-file-height="1322" data-file-width="2014" decoding="async" height="197" src="//upload.wikimedia.org/wikipedia/commons/thumb/3/3b/The_LSTM_cell.png/300px-The_LSTM_cell.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/3/3b/The_LSTM_cell.png/450px-The_LSTM_cell.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/3/3b/The_LSTM_cell.png/600px-The_LSTM_cell.png 2x" width="300"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:The_LSTM_cell.png" title="Enlarge"></a></div>The Long Short-Term Memory (LSTM) cell can process data sequentially and keep its hidden state through time.</div></div></div>
<p><b>Long short-term memory</b> (<b>LSTM</b>) is an artificial <a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">recurrent neural network</a> (RNN) architecture<sup class="reference" id="cite_ref-lstm1997_1-0"><a href="#cite_note-lstm1997-1">[1]</a></sup> used in the field of <a href="/wiki/Deep_learning" title="Deep learning">deep learning</a>. Unlike standard <a href="/wiki/Feedforward_neural_network" title="Feedforward neural network">feedforward neural networks</a>, LSTM has feedback connections. It can not only process single data points (such as images), but also entire sequences of data (such as speech or video). For example, LSTM is applicable to tasks such as unsegmented, connected <a href="/wiki/Handwriting_recognition" title="Handwriting recognition">handwriting recognition</a>,<sup class="reference" id="cite_ref-2"><a href="#cite_note-2">[2]</a></sup> <a href="/wiki/Speech_recognition" title="Speech recognition">speech recognition</a><sup class="reference" id="cite_ref-sak2014_3-0"><a href="#cite_note-sak2014-3">[3]</a></sup><sup class="reference" id="cite_ref-liwu2015_4-0"><a href="#cite_note-liwu2015-4">[4]</a></sup> and anomaly detection in network traffic or IDSs (intrusion detection systems).
</p><p>A common LSTM unit is composed of a <b>cell</b>, an <b>input gate</b>, an <b>output gate</b> and a <b>forget gate</b>. The cell remembers values over arbitrary time intervals and the three <i>gates</i> regulate the flow of information into and out of the cell.
</p><p>LSTM networks are well-suited to <a class="mw-redirect" href="/wiki/Classification_in_machine_learning" title="Classification in machine learning">classifying</a>, <a class="mw-redirect" href="/wiki/Computer_data_processing" title="Computer data processing">processing</a> and <a class="mw-redirect" href="/wiki/Predict" title="Predict">making predictions</a> based on <a href="/wiki/Time_series" title="Time series">time series</a> data, since there can be lags of unknown duration between important events in a time series. LSTMs were developed to deal with the <a href="/wiki/Vanishing_gradient_problem" title="Vanishing gradient problem">vanishing gradient problem</a> that can be encountered when training traditional RNNs. Relative insensitivity to gap length is an advantage of LSTM over RNNs, <a class="mw-redirect" href="/wiki/Hidden_Markov_models" title="Hidden Markov models">hidden Markov models</a> and other sequence learning methods in numerous applications.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (October 2017)">citation needed</span></a></i>]</sup>
</p>
<div aria-labelledby="mw-toc-heading" class="toc" id="toc" role="navigation"><input class="toctogglecheckbox" id="toctogglecheckbox" role="button" style="display:none" type="checkbox"/><div class="toctitle" dir="ltr" lang="en"><h2 id="mw-toc-heading">Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#History"><span class="tocnumber">1</span> <span class="toctext">History</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Idea"><span class="tocnumber">2</span> <span class="toctext">Idea</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="#Architecture"><span class="tocnumber">3</span> <span class="toctext">Architecture</span></a></li>
<li class="toclevel-1 tocsection-4"><a href="#Variants"><span class="tocnumber">4</span> <span class="toctext">Variants</span></a>
<ul>
<li class="toclevel-2 tocsection-5"><a href="#LSTM_with_a_forget_gate"><span class="tocnumber">4.1</span> <span class="toctext">LSTM with a forget gate</span></a>
<ul>
<li class="toclevel-3 tocsection-6"><a href="#Variables"><span class="tocnumber">4.1.1</span> <span class="toctext">Variables</span></a></li>
<li class="toclevel-3 tocsection-7"><a href="#Activation_functions"><span class="tocnumber">4.1.2</span> <span class="toctext">Activation functions</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-8"><a href="#Peephole_LSTM"><span class="tocnumber">4.2</span> <span class="toctext">Peephole LSTM</span></a></li>
<li class="toclevel-2 tocsection-9"><a href="#Peephole_convolutional_LSTM"><span class="tocnumber">4.3</span> <span class="toctext">Peephole convolutional LSTM</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-10"><a href="#Training"><span class="tocnumber">5</span> <span class="toctext">Training</span></a>
<ul>
<li class="toclevel-2 tocsection-11"><a href="#CTC_score_function"><span class="tocnumber">5.1</span> <span class="toctext">CTC score function</span></a></li>
<li class="toclevel-2 tocsection-12"><a href="#Alternatives"><span class="tocnumber">5.2</span> <span class="toctext">Alternatives</span></a>
<ul>
<li class="toclevel-3 tocsection-13"><a href="#Success"><span class="tocnumber">5.2.1</span> <span class="toctext">Success</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-14"><a href="#Applications"><span class="tocnumber">6</span> <span class="toctext">Applications</span></a></li>
<li class="toclevel-1 tocsection-15"><a href="#See_also"><span class="tocnumber">7</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-16"><a href="#References"><span class="tocnumber">8</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1 tocsection-17"><a href="#External_links"><span class="tocnumber">9</span> <span class="toctext">External links</span></a></li>
</ul>
</div>
<h2><span class="mw-headline" id="History">History</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&action=edit&section=1" title="Edit section: History">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p><b>1997:</b> LSTM was proposed by <a href="/wiki/Sepp_Hochreiter" title="Sepp Hochreiter">Sepp Hochreiter</a> and <a href="/wiki/J%C3%BCrgen_Schmidhuber" title="Jürgen Schmidhuber">Jürgen Schmidhuber</a>.<sup class="reference" id="cite_ref-lstm1997_1-1"><a href="#cite_note-lstm1997-1">[1]</a></sup> By introducing Constant Error Carousel (CEC) units, LSTM deals with the <a href="/wiki/Vanishing_gradient_problem" title="Vanishing gradient problem">vanishing gradient problem</a>. The initial version of LSTM block included cells, input and output gates.<sup class="reference" id="cite_ref-ASearchSpaceOdyssey_5-0"><a href="#cite_note-ASearchSpaceOdyssey-5">[5]</a></sup>
</p><p><b>1999:</b> <a href="/wiki/Felix_Gers" title="Felix Gers">Felix Gers</a> and his advisor <a href="/wiki/J%C3%BCrgen_Schmidhuber" title="Jürgen Schmidhuber">Jürgen Schmidhuber</a> and Fred Cummins introduced the forget gate (also called “keep gate”) into LSTM architecture,<sup class="reference" id="cite_ref-lstm1999_6-0"><a href="#cite_note-lstm1999-6">[6]</a></sup> 
enabling the LSTM to reset its own state.<sup class="reference" id="cite_ref-ASearchSpaceOdyssey_5-1"><a href="#cite_note-ASearchSpaceOdyssey-5">[5]</a></sup>
</p><p><b>2000:</b> Gers & Schmidhuber & Cummins added peephole connections (connections from the cell to the gates) into the architecture.<sup class="reference" id="cite_ref-lstm2000_7-0"><a href="#cite_note-lstm2000-7">[7]</a></sup> Additionally, the output activation function was omitted.<sup class="reference" id="cite_ref-ASearchSpaceOdyssey_5-2"><a href="#cite_note-ASearchSpaceOdyssey-5">[5]</a></sup>
</p><p><b>2009:</b> An LSTM based model won the <a class="mw-redirect" href="/wiki/ICDAR" title="ICDAR">ICDAR</a> connected handwriting recognition competition. Three such models were submitted by a team lead by <a href="/wiki/Alex_Graves_(computer_scientist)" title="Alex Graves (computer scientist)">Alex Graves</a>.<sup class="reference" id="cite_ref-8"><a href="#cite_note-8">[8]</a></sup> One was the most accurate model in the competition and another was the fastest.<sup class="reference" id="cite_ref-9"><a href="#cite_note-9">[9]</a></sup>
</p><p><b>2013:</b> LSTM networks were a major component of a network that achieved a record 17.7% <a href="/wiki/Phoneme" title="Phoneme">phoneme</a> error rate on the classic <a href="/wiki/TIMIT" title="TIMIT">TIMIT</a> natural speech dataset.<sup class="reference" id="cite_ref-10"><a href="#cite_note-10">[10]</a></sup>
</p><p><b>2014:</b> Kyunghyun Cho et al. put forward a simplified variant called <a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">Gated recurrent unit</a> (GRU).<sup class="reference" id="cite_ref-11"><a href="#cite_note-11">[11]</a></sup>
</p><p><b>2015:</b> Google started using an LSTM for speech recognition on Google Voice.<sup class="reference" id="cite_ref-Beau15_12-0"><a href="#cite_note-Beau15-12">[12]</a></sup><sup class="reference" id="cite_ref-GoogleVoiceSearch_13-0"><a href="#cite_note-GoogleVoiceSearch-13">[13]</a></sup> According to the official blog post, the new model cut transcription errors by 49%. <sup class="reference" id="cite_ref-14"><a href="#cite_note-14">[14]</a></sup>
</p><p><b>2016:</b> Google started using an LSTM to suggest messages in the Allo conversation app.<sup class="reference" id="cite_ref-GoogleAllo_15-0"><a href="#cite_note-GoogleAllo-15">[15]</a></sup> In the same year, Google released the <a href="/wiki/Google_Neural_Machine_Translation" title="Google Neural Machine Translation">Google Neural Machine Translation</a> system for Google Translate which used LSTMs to reduce translation errors by 60%.<sup class="reference" id="cite_ref-GoogleTranslate_16-0"><a href="#cite_note-GoogleTranslate-16">[16]</a></sup><sup class="reference" id="cite_ref-WiredGoogleTranslate_17-0"><a href="#cite_note-WiredGoogleTranslate-17">[17]</a></sup><sup class="reference" id="cite_ref-18"><a href="#cite_note-18">[18]</a></sup>
</p><p>Apple announced in its <a href="/wiki/Apple_Worldwide_Developers_Conference" title="Apple Worldwide Developers Conference">Worldwide Developers Conference</a> that it would start using the LSTM for quicktype<sup class="reference" id="cite_ref-AppleQuicktype_19-0"><a href="#cite_note-AppleQuicktype-19">[19]</a></sup><sup class="reference" id="cite_ref-AppleQuicktype2_20-0"><a href="#cite_note-AppleQuicktype2-20">[20]</a></sup><sup class="reference" id="cite_ref-21"><a href="#cite_note-21">[21]</a></sup> in the iPhone and for Siri.<sup class="reference" id="cite_ref-AppleSiri_22-0"><a href="#cite_note-AppleSiri-22">[22]</a></sup><sup class="reference" id="cite_ref-23"><a href="#cite_note-23">[23]</a></sup>
</p><p>Amazon released <a href="/wiki/Amazon_Polly" title="Amazon Polly">Polly</a>, which generates the voices behind Alexa, using a bidirectional LSTM for the text-to-speech technology.<sup class="reference" id="cite_ref-AmazonAlexa_24-0"><a href="#cite_note-AmazonAlexa-24">[24]</a></sup>
</p><p><b>2017:</b>  Facebook performed some 4.5 billion automatic translations every day using long short-term memory networks.<sup class="reference" id="cite_ref-FacebookTranslate_25-0"><a href="#cite_note-FacebookTranslate-25">[25]</a></sup>
</p><p>Researchers from <a href="/wiki/Michigan_State_University" title="Michigan State University">Michigan State University</a>, <a href="/wiki/IBM_Research" title="IBM Research">IBM Research</a>, and <a href="/wiki/Cornell_University" title="Cornell University">Cornell University</a> published a study in the Knowledge Discovery and Data Mining (KDD) conference.<sup class="reference" id="cite_ref-26"><a href="#cite_note-26">[26]</a></sup><sup class="reference" id="cite_ref-27"><a href="#cite_note-27">[27]</a></sup><sup class="reference" id="cite_ref-28"><a href="#cite_note-28">[28]</a></sup> Their study describes a novel neural network that performs better on certain data sets than the widely used long short-term memory neural network.
</p><p>Microsoft reported reaching 94.9% recognition accuracy on the <a class="new" href="/w/index.php?title=Switchboard_corpus&action=edit&redlink=1" title="Switchboard corpus (page does not exist)">Switchboard corpus</a>, incorporating a vocabulary of 165,000 words. The approach used "dialog session-based long-short-term memory".<sup class="reference" id="cite_ref-29"><a href="#cite_note-29">[29]</a></sup>
</p><p><b>2019:</b> Researchers from the <a href="/wiki/University_of_Waterloo" title="University of Waterloo">University of Waterloo</a> proposed a related RNN architecture which represents continuous windows of time. It was derived using the <a href="/wiki/Legendre_polynomials" title="Legendre polynomials">Legendre polynomials</a> and outperforms the LSTM on some memory-related benchmarks.<sup class="reference" id="cite_ref-30"><a href="#cite_note-30">[30]</a></sup>
</p><p>An LSTM model climbed to third place on the in Large Text Compression Benchmark.<sup class="reference" id="cite_ref-31"><a href="#cite_note-31">[31]</a></sup><sup class="reference" id="cite_ref-32"><a href="#cite_note-32">[32]</a></sup>
</p>
<h2><span class="mw-headline" id="Idea">Idea</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&action=edit&section=2" title="Edit section: Idea">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>In theory, classic (or "vanilla") <a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">RNNs</a> can keep track of arbitrary long-term dependencies in the input sequences. The problem of vanilla RNNs is computational (or practical) in nature: when training a vanilla RNN using <a class="mw-redirect" href="/wiki/Back-propagation" title="Back-propagation">back-propagation</a>, the gradients which are back-propagated can <a href="/wiki/Vanishing_gradient_problem" title="Vanishing gradient problem">"vanish" (that is, they can tend to zero) or "explode" (that is, they can tend to infinity)</a>, because of the computations involved in the process, which use <a href="/wiki/Round-off_error" title="Round-off error">finite-precision numbers</a>. RNNs using LSTM units partially solve the <a href="/wiki/Vanishing_gradient_problem" title="Vanishing gradient problem">vanishing gradient problem</a>, because LSTM units allow gradients to also flow <i>unchanged</i>. However, LSTM networks can still suffer from the exploding gradient problem.<sup class="reference" id="cite_ref-33"><a href="#cite_note-33">[33]</a></sup>
</p>
<h2><span class="mw-headline" id="Architecture">Architecture</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&action=edit&section=3" title="Edit section: Architecture">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>There are several architectures of LSTM units. A common architecture is composed of a <b>cell</b> (the memory part of the LSTM unit) and three "regulators", usually called gates, of the flow of information inside the LSTM unit: an <b>input gate</b>, an <b>output gate</b> and a <b>forget gate</b>. Some variations of the LSTM unit do not have one or more of these gates or maybe have other gates. For example, <a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">gated recurrent units</a> (GRUs) do not have an output gate.
</p><p>Intuitively, the <i>cell</i> is responsible for keeping track of the dependencies between the elements in the input sequence. The <i>input gate</i> controls the extent to which a new value flows into the cell, the <i>forget gate</i> controls the extent to which a value remains in the cell and the <i>output gate</i> controls the extent to which the value in the cell is used to compute the output activation of the LSTM unit. The activation function of the LSTM <i>gates</i> is often the <a class="mw-redirect" href="/wiki/Logistic_sigmoid_function" title="Logistic sigmoid function">logistic sigmoid function</a>.
</p><p>There are connections into and out of the LSTM <i>gates</i>, a few of which are recurrent. The weights of these connections, which need to be learned during <a href="/wiki/Supervised_learning" title="Supervised learning">training</a>, determine how the gates operate.
</p>
<h2><span class="mw-headline" id="Variants">Variants</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&action=edit&section=4" title="Edit section: Variants">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>In the equations below, the lowercase variables represent vectors. Matrices <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle W_{q}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>q</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle W_{q}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle W_{q}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d16355ad959593cf720b24fffe62d99af53d15d9" style="vertical-align: -1.005ex; width:3.182ex; height:2.843ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle U_{q}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>U</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>q</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle U_{q}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle U_{q}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/05e27486afb11613504d6d6b9f6bd72e322607c8" style="vertical-align: -1.005ex; width:2.576ex; height:2.843ex;"/></span> contain, respectively, the weights of the input and recurrent connections, where the subscript <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle _{q}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>q</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle _{q}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle _{q}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e333a711146f91533eb5a030accc0e90948e4f92" style="vertical-align: -1.005ex; width:0.989ex; height:1.676ex;"/></span> can either be the input gate <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle i}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>i</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle i}</annotation>
</semantics>
</math></span><img alt="i" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" style="vertical-align: -0.338ex; width:0.802ex; height:2.176ex;"/></span>, output gate <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle o}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>o</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle o}</annotation>
</semantics>
</math></span><img alt="o" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0c1031f61947aa3d1cf3a70ec3e4904df2c3675d" style="vertical-align: -0.338ex; width:1.128ex; height:1.676ex;"/></span>, the forget gate <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle f}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>f</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle f}</annotation>
</semantics>
</math></span><img alt="f" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61" style="vertical-align: -0.671ex; width:1.279ex; height:2.509ex;"/></span> or the memory cell <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle c}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>c</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle c}</annotation>
</semantics>
</math></span><img alt="c" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/86a67b81c2de995bd608d5b2df50cd8cd7d92455" style="vertical-align: -0.338ex; width:1.007ex; height:1.676ex;"/></span>, depending on the activation being calculated. In this section, we are thus using a "vector notation". So, for example, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle c_{t}\in \mathbb {R} ^{h}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">R</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>h</mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle c_{t}\in \mathbb {R} ^{h}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle c_{t}\in \mathbb {R} ^{h}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/65d6f2af820422ed59a0f14af91eee7498ebc4a2" style="vertical-align: -0.671ex; width:7.531ex; height:3.009ex;"/></span> is not just one cell of one LSTM unit, but contains <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle h}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>h</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle h}</annotation>
</semantics>
</math></span><img alt="h" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b26be3e694314bc90c3215047e4a2010c6ee184a" style="vertical-align: -0.338ex; width:1.339ex; height:2.176ex;"/></span> LSTM unit's cells.
</p>
<h3><span class="mw-headline" id="LSTM_with_a_forget_gate">LSTM with a forget gate</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&action=edit&section=5" title="Edit section: LSTM with a forget gate">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The compact forms of the equations for the forward pass of an LSTM unit with a forget gate are:<sup class="reference" id="cite_ref-lstm1997_1-2"><a href="#cite_note-lstm1997-1">[1]</a></sup><sup class="reference" id="cite_ref-lstm2000_7-1"><a href="#cite_note-lstm2000-7">[7]</a></sup>
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\begin{aligned}f_{t}&=\sigma _{g}(W_{f}x_{t}+U_{f}h_{t-1}+b_{f})\\i_{t}&=\sigma _{g}(W_{i}x_{t}+U_{i}h_{t-1}+b_{i})\\o_{t}&=\sigma _{g}(W_{o}x_{t}+U_{o}h_{t-1}+b_{o})\\{\tilde {c}}_{t}&=\sigma _{c}(W_{c}x_{t}+U_{c}h_{t-1}+b_{c})\\c_{t}&=f_{t}\circ c_{t-1}+i_{t}\circ {\tilde {c}}_{t}\\h_{t}&=o_{t}\circ \sigma _{h}(c_{t})\end{aligned}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt">
<mtr>
<mtd>
<msub>
<mi>f</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>g</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>f</mi>
</mrow>
</msub>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>U</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>f</mi>
</mrow>
</msub>
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>f</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mi>i</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>g</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>U</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mi>o</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>g</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>o</mi>
</mrow>
</msub>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>U</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>o</mi>
</mrow>
</msub>
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>o</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>c</mi>
<mo stretchy="false">~<!-- ~ --></mo>
</mover>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
</mrow>
</msub>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>U</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
</mrow>
</msub>
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>f</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∘<!-- ∘ --></mo>
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>i</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∘<!-- ∘ --></mo>
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>c</mi>
<mo stretchy="false">~<!-- ~ --></mo>
</mover>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>o</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∘<!-- ∘ --></mo>
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>h</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
</mtable>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\begin{aligned}f_{t}&=\sigma _{g}(W_{f}x_{t}+U_{f}h_{t-1}+b_{f})\\i_{t}&=\sigma _{g}(W_{i}x_{t}+U_{i}h_{t-1}+b_{i})\\o_{t}&=\sigma _{g}(W_{o}x_{t}+U_{o}h_{t-1}+b_{o})\\{\tilde {c}}_{t}&=\sigma _{c}(W_{c}x_{t}+U_{c}h_{t-1}+b_{c})\\c_{t}&=f_{t}\circ c_{t-1}+i_{t}\circ {\tilde {c}}_{t}\\h_{t}&=o_{t}\circ \sigma _{h}(c_{t})\end{aligned}}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\begin{aligned}f_{t}&=\sigma _{g}(W_{f}x_{t}+U_{f}h_{t-1}+b_{f})\\i_{t}&=\sigma _{g}(W_{i}x_{t}+U_{i}h_{t-1}+b_{i})\\o_{t}&=\sigma _{g}(W_{o}x_{t}+U_{o}h_{t-1}+b_{o})\\{\tilde {c}}_{t}&=\sigma _{c}(W_{c}x_{t}+U_{c}h_{t-1}+b_{c})\\c_{t}&=f_{t}\circ c_{t-1}+i_{t}\circ {\tilde {c}}_{t}\\h_{t}&=o_{t}\circ \sigma _{h}(c_{t})\end{aligned}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7dee414820d5c0162ae1fff1899e58b08923944f" style="vertical-align: -8.838ex; width:30.463ex; height:18.843ex;"/></span></dd></dl>
<p>where the initial values are <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle c_{0}=0}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>0</mn>
</mrow>
</msub>
<mo>=</mo>
<mn>0</mn>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle c_{0}=0}</annotation>
</semantics>
</math></span><img alt="c_{0}=0" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/29af3d4e887815bb3b9b9eab4f7540a376fccd73" style="vertical-align: -0.671ex; width:6.322ex; height:2.509ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle h_{0}=0}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>0</mn>
</mrow>
</msub>
<mo>=</mo>
<mn>0</mn>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle h_{0}=0}</annotation>
</semantics>
</math></span><img alt="{\displaystyle h_{0}=0}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/14a294b6cf9cbde4c37efd966913a63d316e615c" style="vertical-align: -0.671ex; width:6.654ex; height:2.509ex;"/></span> and the operator <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \circ }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mo>∘<!-- ∘ --></mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \circ }</annotation>
</semantics>
</math></span><img alt="\circ " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/99add39d2b681e2de7ff62422c32704a05c7ec31" style="vertical-align: 0.125ex; margin-bottom: -0.297ex; width:1.162ex; height:1.509ex;"/></span> denotes the <a href="/wiki/Hadamard_product_(matrices)" title="Hadamard product (matrices)">Hadamard product</a> (element-wise product). The subscript <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle t}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>t</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle t}</annotation>
</semantics>
</math></span><img alt="t" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/65658b7b223af9e1acc877d848888ecdb4466560" style="vertical-align: -0.338ex; width:0.84ex; height:2.009ex;"/></span> indexes the time step.
</p>
<h4><span class="mw-headline" id="Variables">Variables</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&action=edit&section=6" title="Edit section: Variables">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<ul><li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle x_{t}\in \mathbb {R} ^{d}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">R</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>d</mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle x_{t}\in \mathbb {R} ^{d}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle x_{t}\in \mathbb {R} ^{d}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d528d57c5517e90795e0a6d6760463564236fee7" style="vertical-align: -0.671ex; width:7.766ex; height:3.009ex;"/></span>: input vector to the LSTM unit</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle f_{t}\in \mathbb {R} ^{h}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>f</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">R</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>h</mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle f_{t}\in \mathbb {R} ^{h}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle f_{t}\in \mathbb {R} ^{h}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/02547735a64d79c553b23eaf3aeaaaf2fcde6eba" style="vertical-align: -0.671ex; width:7.663ex; height:3.009ex;"/></span>: forget gate's activation vector</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle i_{t}\in \mathbb {R} ^{h}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>i</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">R</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>h</mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle i_{t}\in \mathbb {R} ^{h}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle i_{t}\in \mathbb {R} ^{h}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/abb830b2d64edaa2aa92edaeccdcb027afa7344e" style="vertical-align: -0.671ex; width:7.326ex; height:3.009ex;"/></span>: input/update gate's activation vector</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle o_{t}\in \mathbb {R} ^{h}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>o</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">R</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>h</mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle o_{t}\in \mathbb {R} ^{h}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle o_{t}\in \mathbb {R} ^{h}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/25b8c014b13f62fbe82f9a277a71f6a10c2ae330" style="vertical-align: -0.671ex; width:7.651ex; height:3.009ex;"/></span>: output gate's activation vector</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle h_{t}\in \mathbb {R} ^{h}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">R</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>h</mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle h_{t}\in \mathbb {R} ^{h}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle h_{t}\in \mathbb {R} ^{h}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cf6b05b4bd0106b70d036400f1ddd3ae54be2689" style="vertical-align: -0.671ex; width:7.863ex; height:3.009ex;"/></span>: hidden state vector also known as output vector of the LSTM unit</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\tilde {c}}_{t}\in \mathbb {R} ^{h}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>c</mi>
<mo stretchy="false">~<!-- ~ --></mo>
</mover>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">R</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>h</mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\tilde {c}}_{t}\in \mathbb {R} ^{h}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\tilde {c}}_{t}\in \mathbb {R} ^{h}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/017c48f0975511aaac8e4ea5b5cecd664283d016" style="vertical-align: -0.671ex; width:7.815ex; height:3.009ex;"/></span>: cell input activation vector</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle c_{t}\in \mathbb {R} ^{h}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">R</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>h</mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle c_{t}\in \mathbb {R} ^{h}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle c_{t}\in \mathbb {R} ^{h}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/65d6f2af820422ed59a0f14af91eee7498ebc4a2" style="vertical-align: -0.671ex; width:7.531ex; height:3.009ex;"/></span>: cell state vector</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle W\in \mathbb {R} ^{h\times d}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>W</mi>
<mo>∈<!-- ∈ --></mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">R</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>h</mi>
<mo>×<!-- × --></mo>
<mi>d</mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle W\in \mathbb {R} ^{h\times d}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle W\in \mathbb {R} ^{h\times d}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/925afdb42f13f1d912db87ecda65135eb9fe6352" style="vertical-align: -0.338ex; width:10.271ex; height:2.676ex;"/></span>, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle U\in \mathbb {R} ^{h\times h}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>U</mi>
<mo>∈<!-- ∈ --></mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">R</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>h</mi>
<mo>×<!-- × --></mo>
<mi>h</mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle U\in \mathbb {R} ^{h\times h}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle U\in \mathbb {R} ^{h\times h}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0ff9bb53a5409a1e51f6130b3dfbcdad63324880" style="vertical-align: -0.338ex; width:9.706ex; height:2.676ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle b\in \mathbb {R} ^{h}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>b</mi>
<mo>∈<!-- ∈ --></mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">R</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>h</mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle b\in \mathbb {R} ^{h}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle b\in \mathbb {R} ^{h}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/289515b23d7df7e2e09f2ee38951abf345e60080" style="vertical-align: -0.338ex; width:6.695ex; height:2.676ex;"/></span>: weight matrices and bias vector parameters which need to be learned during training</li></ul>
<p>where the superscripts <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle d}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>d</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle d}</annotation>
</semantics>
</math></span><img alt="d" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e85ff03cbe0c7341af6b982e47e9f90d235c66ab" style="vertical-align: -0.338ex; width:1.216ex; height:2.176ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle h}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>h</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle h}</annotation>
</semantics>
</math></span><img alt="h" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b26be3e694314bc90c3215047e4a2010c6ee184a" style="vertical-align: -0.338ex; width:1.339ex; height:2.176ex;"/></span> refer to the number of input features and number of hidden units, respectively.
</p>
<h4><span class="mw-headline" id="Activation_functions"><a href="/wiki/Activation_function" title="Activation function">Activation functions</a></span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&action=edit&section=7" title="Edit section: Activation functions">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<ul><li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \sigma _{g}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>g</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \sigma _{g}}</annotation>
</semantics>
</math></span><img alt="\sigma _{g}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/086f92de077f853afd7f5d22fb3d305cbf5e0ac3" style="vertical-align: -1.005ex; width:2.349ex; height:2.343ex;"/></span>: <a href="/wiki/Sigmoid_function" title="Sigmoid function">sigmoid function</a>.</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \sigma _{c}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \sigma _{c}}</annotation>
</semantics>
</math></span><img alt="\sigma_c" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b436b43abda74fce1a6859e03d34c914c6a240f4" style="vertical-align: -0.671ex; width:2.272ex; height:2.009ex;"/></span>: <a class="mw-redirect" href="/wiki/Hyperbolic_tangent" title="Hyperbolic tangent">hyperbolic tangent</a> function.</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \sigma _{h}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>h</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \sigma _{h}}</annotation>
</semantics>
</math></span><img alt="\sigma _{h}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8a7f19495f5a65d26570b54b7ca332956d27b27b" style="vertical-align: -0.671ex; width:2.506ex; height:2.009ex;"/></span>: hyperbolic tangent function or, as the peephole LSTM paper<sup class="reference" id="cite_ref-peepholeLSTM_34-0"><a href="#cite_note-peepholeLSTM-34">[34]</a></sup><sup class="reference" id="cite_ref-peephole2002_35-0"><a href="#cite_note-peephole2002-35">[35]</a></sup> suggests, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \sigma _{h}(x)=x}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>h</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
<mo>=</mo>
<mi>x</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \sigma _{h}(x)=x}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \sigma _{h}(x)=x}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/98d34299f4e04f10f7c22e1219bf182712c3e0fc" style="vertical-align: -0.838ex; width:10.074ex; height:2.843ex;"/></span>.</li></ul>
<h3><span class="mw-headline" id="Peephole_LSTM">Peephole LSTM</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&action=edit&section=8" title="Edit section: Peephole LSTM">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="thumb tright"><div class="thumbinner" style="width:302px;"><a class="image" href="/wiki/File:Peephole_Long_Short-Term_Memory.svg"><img alt="" class="thumbimage" data-file-height="298" data-file-width="542" decoding="async" height="165" src="//upload.wikimedia.org/wikipedia/commons/thumb/5/53/Peephole_Long_Short-Term_Memory.svg/300px-Peephole_Long_Short-Term_Memory.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/5/53/Peephole_Long_Short-Term_Memory.svg/450px-Peephole_Long_Short-Term_Memory.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/5/53/Peephole_Long_Short-Term_Memory.svg/600px-Peephole_Long_Short-Term_Memory.svg.png 2x" width="300"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:Peephole_Long_Short-Term_Memory.svg" title="Enlarge"></a></div>A <a href="#Peephole_LSTM">peephole LSTM</a> unit with input (i.e. <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle i}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>i</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle i}</annotation>
</semantics>
</math></span><img alt="i" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" style="vertical-align: -0.338ex; width:0.802ex; height:2.176ex;"/></span>), output (i.e. <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle o}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>o</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle o}</annotation>
</semantics>
</math></span><img alt="o" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0c1031f61947aa3d1cf3a70ec3e4904df2c3675d" style="vertical-align: -0.338ex; width:1.128ex; height:1.676ex;"/></span>), and forget (i.e. <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle f}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>f</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle f}</annotation>
</semantics>
</math></span><img alt="f" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61" style="vertical-align: -0.671ex; width:1.279ex; height:2.509ex;"/></span>) gates. Each of these gates can be thought as a "standard" neuron in a feed-forward (or multi-layer) neural network: that is, they compute an activation (using an activation function) of a weighted sum. <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle i_{t},o_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>i</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>,</mo>
<msub>
<mi>o</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle i_{t},o_{t}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle i_{t},o_{t}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cf0b0dee7b12fd921a114101ff11c83e1606a1f8" style="vertical-align: -0.671ex; width:4.616ex; height:2.509ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle f_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>f</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle f_{t}}</annotation>
</semantics>
</math></span><img alt="f_{t}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/874c306411e808e8191e8aeb95e3440e1c68d6e9" style="vertical-align: -0.671ex; width:1.965ex; height:2.509ex;"/></span> represent the activations of respectively the input, output and forget gates, at time step <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle t}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>t</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle t}</annotation>
</semantics>
</math></span><img alt="t" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/65658b7b223af9e1acc877d848888ecdb4466560" style="vertical-align: -0.338ex; width:0.84ex; height:2.009ex;"/></span>.  The 3 exit arrows from the memory cell <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle c}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>c</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle c}</annotation>
</semantics>
</math></span><img alt="c" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/86a67b81c2de995bd608d5b2df50cd8cd7d92455" style="vertical-align: -0.338ex; width:1.007ex; height:1.676ex;"/></span> to the 3 gates <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle i,o}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>i</mi>
<mo>,</mo>
<mi>o</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle i,o}</annotation>
</semantics>
</math></span><img alt="{\displaystyle i,o}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4697b39f565cd54942b9f81d5de46dcdd1174528" style="vertical-align: -0.671ex; width:2.964ex; height:2.509ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle f}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>f</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle f}</annotation>
</semantics>
</math></span><img alt="f" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61" style="vertical-align: -0.671ex; width:1.279ex; height:2.509ex;"/></span> represent the <i>peephole</i> connections. These peephole connections actually denote the contributions of the activation of the memory cell <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle c}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>c</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle c}</annotation>
</semantics>
</math></span><img alt="c" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/86a67b81c2de995bd608d5b2df50cd8cd7d92455" style="vertical-align: -0.338ex; width:1.007ex; height:1.676ex;"/></span> at time step <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle t-1}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle t-1}</annotation>
</semantics>
</math></span><img alt="t-1" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a215d9553945bb84b3b5a79cc796fb7d6e0629f0" style="vertical-align: -0.505ex; width:4.842ex; height:2.343ex;"/></span>, i.e. the contribution of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle c_{t-1}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle c_{t-1}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle c_{t-1}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0b5dbc0177993c2ebd927aee23d88bd263770532" style="vertical-align: -0.671ex; width:3.933ex; height:2.009ex;"/></span> (and not <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle c_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle c_{t}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle c_{t}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/93578e37f3234419a34df79845836bc0ec5ef76c" style="vertical-align: -0.671ex; width:1.833ex; height:2.009ex;"/></span>, as the picture may suggest). In other words, the gates <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle i,o}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>i</mi>
<mo>,</mo>
<mi>o</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle i,o}</annotation>
</semantics>
</math></span><img alt="{\displaystyle i,o}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4697b39f565cd54942b9f81d5de46dcdd1174528" style="vertical-align: -0.671ex; width:2.964ex; height:2.509ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle f}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>f</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle f}</annotation>
</semantics>
</math></span><img alt="f" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61" style="vertical-align: -0.671ex; width:1.279ex; height:2.509ex;"/></span> calculate their activations at time step <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle t}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>t</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle t}</annotation>
</semantics>
</math></span><img alt="t" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/65658b7b223af9e1acc877d848888ecdb4466560" style="vertical-align: -0.338ex; width:0.84ex; height:2.009ex;"/></span> (i.e., respectively, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle i_{t},o_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>i</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>,</mo>
<msub>
<mi>o</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle i_{t},o_{t}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle i_{t},o_{t}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cf0b0dee7b12fd921a114101ff11c83e1606a1f8" style="vertical-align: -0.671ex; width:4.616ex; height:2.509ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle f_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>f</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle f_{t}}</annotation>
</semantics>
</math></span><img alt="f_{t}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/874c306411e808e8191e8aeb95e3440e1c68d6e9" style="vertical-align: -0.671ex; width:1.965ex; height:2.509ex;"/></span>) also considering the activation of the memory cell <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle c}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>c</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle c}</annotation>
</semantics>
</math></span><img alt="c" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/86a67b81c2de995bd608d5b2df50cd8cd7d92455" style="vertical-align: -0.338ex; width:1.007ex; height:1.676ex;"/></span> at time step <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle t-1}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle t-1}</annotation>
</semantics>
</math></span><img alt="t-1" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a215d9553945bb84b3b5a79cc796fb7d6e0629f0" style="vertical-align: -0.505ex; width:4.842ex; height:2.343ex;"/></span>, i.e. <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle c_{t-1}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle c_{t-1}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle c_{t-1}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0b5dbc0177993c2ebd927aee23d88bd263770532" style="vertical-align: -0.671ex; width:3.933ex; height:2.009ex;"/></span>.  The single left-to-right arrow exiting the memory cell is <i>not</i> a peephole connection and denotes <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle c_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle c_{t}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle c_{t}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/93578e37f3234419a34df79845836bc0ec5ef76c" style="vertical-align: -0.671ex; width:1.833ex; height:2.009ex;"/></span>.  The little circles containing a <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \times }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mo>×<!-- × --></mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \times }</annotation>
</semantics>
</math></span><img alt="\times " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0ffafff1ad26cbe49045f19a67ce532116a32703" style="vertical-align: 0.019ex; margin-bottom: -0.19ex; width:1.808ex; height:1.509ex;"/></span> symbol represent an element-wise multiplication between its inputs. The big circles containing an <i>S</i>-like curve represent the application of a differentiable function (like the sigmoid function) to a weighted sum.  There are many other kinds of LSTMs as well.<sup class="reference" id="cite_ref-ASearchSpaceOdyssey_5-3"><a href="#cite_note-ASearchSpaceOdyssey-5">[5]</a></sup></div></div></div>
<p>The figure on the right is a graphical representation of an LSTM unit with peephole connections (i.e. a peephole LSTM).<sup class="reference" id="cite_ref-peepholeLSTM_34-1"><a href="#cite_note-peepholeLSTM-34">[34]</a></sup><sup class="reference" id="cite_ref-peephole2002_35-1"><a href="#cite_note-peephole2002-35">[35]</a></sup> Peephole connections allow the gates to access the constant error carousel (CEC), whose activation is the cell state.<sup class="reference" id="cite_ref-36"><a href="#cite_note-36">[36]</a></sup> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle h_{t-1}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle h_{t-1}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle h_{t-1}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cf56fc7e1114417475762546403f3d66460975d0" style="vertical-align: -0.671ex; width:4.265ex; height:2.509ex;"/></span> is not used, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle c_{t-1}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle c_{t-1}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle c_{t-1}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0b5dbc0177993c2ebd927aee23d88bd263770532" style="vertical-align: -0.671ex; width:3.933ex; height:2.009ex;"/></span> is used instead in most places.
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\begin{aligned}f_{t}&=\sigma _{g}(W_{f}x_{t}+U_{f}c_{t-1}+b_{f})\\i_{t}&=\sigma _{g}(W_{i}x_{t}+U_{i}c_{t-1}+b_{i})\\o_{t}&=\sigma _{g}(W_{o}x_{t}+U_{o}c_{t-1}+b_{o})\\c_{t}&=f_{t}\circ c_{t-1}+i_{t}\circ \sigma _{c}(W_{c}x_{t}+b_{c})\\h_{t}&=o_{t}\circ \sigma _{h}(c_{t})\end{aligned}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt">
<mtr>
<mtd>
<msub>
<mi>f</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>g</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>f</mi>
</mrow>
</msub>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>U</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>f</mi>
</mrow>
</msub>
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>f</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mi>i</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>g</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>U</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mi>o</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>g</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>o</mi>
</mrow>
</msub>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>U</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>o</mi>
</mrow>
</msub>
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>o</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>f</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∘<!-- ∘ --></mo>
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>i</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∘<!-- ∘ --></mo>
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
</mrow>
</msub>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>o</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∘<!-- ∘ --></mo>
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>h</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
</mtable>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\begin{aligned}f_{t}&=\sigma _{g}(W_{f}x_{t}+U_{f}c_{t-1}+b_{f})\\i_{t}&=\sigma _{g}(W_{i}x_{t}+U_{i}c_{t-1}+b_{i})\\o_{t}&=\sigma _{g}(W_{o}x_{t}+U_{o}c_{t-1}+b_{o})\\c_{t}&=f_{t}\circ c_{t-1}+i_{t}\circ \sigma _{c}(W_{c}x_{t}+b_{c})\\h_{t}&=o_{t}\circ \sigma _{h}(c_{t})\end{aligned}}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\begin{aligned}f_{t}&=\sigma _{g}(W_{f}x_{t}+U_{f}c_{t-1}+b_{f})\\i_{t}&=\sigma _{g}(W_{i}x_{t}+U_{i}c_{t-1}+b_{i})\\o_{t}&=\sigma _{g}(W_{o}x_{t}+U_{o}c_{t-1}+b_{o})\\c_{t}&=f_{t}\circ c_{t-1}+i_{t}\circ \sigma _{c}(W_{c}x_{t}+b_{c})\\h_{t}&=o_{t}\circ \sigma _{h}(c_{t})\end{aligned}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c989b6299ec9d1166cc09573bda543824237119d" style="vertical-align: -7.338ex; width:34.928ex; height:15.843ex;"/></span></dd></dl>
<h3><span class="mw-headline" id="Peephole_convolutional_LSTM">Peephole convolutional LSTM</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&action=edit&section=9" title="Edit section: Peephole convolutional LSTM">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Peephole <a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">convolutional</a> LSTM.<sup class="reference" id="cite_ref-37"><a href="#cite_note-37">[37]</a></sup> The <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle *}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mo>∗<!-- ∗ --></mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle *}</annotation>
</semantics>
</math></span><img alt="*" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8e9972f426d9e07855984f73ee195a21dbc21755" style="vertical-align: 0.079ex; margin-bottom: -0.25ex; width:1.162ex; height:1.509ex;"/></span> denotes the <a href="/wiki/Convolution" title="Convolution">convolution</a> operator.
</p>
<dl><dd><span class="mwe-math-element" id="Page 4, formula 4 in [33] reference (Ot is calculated for ''C''(''t'') intead of ''C''(''t'' − 1)): https://arxiv.org/abs/1506.04214v2"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\begin{aligned}f_{t}&=\sigma _{g}(W_{f}*x_{t}+U_{f}*h_{t-1}+V_{f}\circ c_{t-1}+b_{f})\\i_{t}&=\sigma _{g}(W_{i}*x_{t}+U_{i}*h_{t-1}+V_{i}\circ c_{t-1}+b_{i})\\c_{t}&=f_{t}\circ c_{t-1}+i_{t}\circ \sigma _{c}(W_{c}*x_{t}+U_{c}*h_{t-1}+b_{c})\\o_{t}&=\sigma _{g}(W_{o}*x_{t}+U_{o}*h_{t-1}+V_{o}\circ c_{t}+b_{o})\\h_{t}&=o_{t}\circ \sigma _{h}(c_{t})\end{aligned}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt">
<mtr>
<mtd>
<msub>
<mi>f</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>g</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>f</mi>
</mrow>
</msub>
<mo>∗<!-- ∗ --></mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>U</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>f</mi>
</mrow>
</msub>
<mo>∗<!-- ∗ --></mo>
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>V</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>f</mi>
</mrow>
</msub>
<mo>∘<!-- ∘ --></mo>
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>f</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mi>i</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>g</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo>∗<!-- ∗ --></mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>U</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo>∗<!-- ∗ --></mo>
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>V</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo>∘<!-- ∘ --></mo>
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>f</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∘<!-- ∘ --></mo>
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>i</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∘<!-- ∘ --></mo>
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
</mrow>
</msub>
<mo>∗<!-- ∗ --></mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>U</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
</mrow>
</msub>
<mo>∗<!-- ∗ --></mo>
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mi>o</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>g</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>o</mi>
</mrow>
</msub>
<mo>∗<!-- ∗ --></mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>U</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>o</mi>
</mrow>
</msub>
<mo>∗<!-- ∗ --></mo>
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>V</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>o</mi>
</mrow>
</msub>
<mo>∘<!-- ∘ --></mo>
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>+</mo>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>o</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mi>h</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<msub>
<mi>o</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∘<!-- ∘ --></mo>
<msub>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>h</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>c</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
</mtable>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\begin{aligned}f_{t}&=\sigma _{g}(W_{f}*x_{t}+U_{f}*h_{t-1}+V_{f}\circ c_{t-1}+b_{f})\\i_{t}&=\sigma _{g}(W_{i}*x_{t}+U_{i}*h_{t-1}+V_{i}\circ c_{t-1}+b_{i})\\c_{t}&=f_{t}\circ c_{t-1}+i_{t}\circ \sigma _{c}(W_{c}*x_{t}+U_{c}*h_{t-1}+b_{c})\\o_{t}&=\sigma _{g}(W_{o}*x_{t}+U_{o}*h_{t-1}+V_{o}\circ c_{t}+b_{o})\\h_{t}&=o_{t}\circ \sigma _{h}(c_{t})\end{aligned}}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\begin{aligned}f_{t}&=\sigma _{g}(W_{f}*x_{t}+U_{f}*h_{t-1}+V_{f}\circ c_{t-1}+b_{f})\\i_{t}&=\sigma _{g}(W_{i}*x_{t}+U_{i}*h_{t-1}+V_{i}\circ c_{t-1}+b_{i})\\c_{t}&=f_{t}\circ c_{t-1}+i_{t}\circ \sigma _{c}(W_{c}*x_{t}+U_{c}*h_{t-1}+b_{c})\\o_{t}&=\sigma _{g}(W_{o}*x_{t}+U_{o}*h_{t-1}+V_{o}\circ c_{t}+b_{o})\\h_{t}&=o_{t}\circ \sigma _{h}(c_{t})\end{aligned}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" id="Page_4,_formula_4_in_[33]_reference_(Ot_is_calculated_for_''C''(''t'')_intead_of_''C''(''t'' − 1)):_https://arxiv.org/abs/1506.04214v2" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1edbece2559479959fe829e9c6657efb380debe7" style="vertical-align: -7.338ex; width:48.955ex; height:15.843ex;"/></span></dd></dl>
<h2><span class="mw-headline" id="Training">Training</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&action=edit&section=10" title="Edit section: Training">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>An RNN using LSTM units can be trained in a supervised fashion, on a set of training sequences, using an optimization algorithm, like <a href="/wiki/Gradient_descent" title="Gradient descent">gradient descent</a>, combined with <a href="/wiki/Backpropagation_through_time" title="Backpropagation through time">backpropagation through time</a> to compute the gradients needed during the optimization process, in order to change each weight of the LSTM network in proportion to the derivative of the error (at the output layer of the LSTM network) with respect to corresponding weight.
</p><p>A problem with using <a href="/wiki/Gradient_descent" title="Gradient descent">gradient descent</a> for standard RNNs is that error gradients <a href="/wiki/Vanishing_gradient_problem" title="Vanishing gradient problem">vanish</a> exponentially quickly with the size of the time lag between important events. This is due to <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \lim _{n\to \infty }W^{n}=0}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<munder>
<mo form="prefix" movablelimits="true">lim</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
<mo stretchy="false">→<!-- → --></mo>
<mi mathvariant="normal">∞<!-- ∞ --></mi>
</mrow>
</munder>
<msup>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msup>
<mo>=</mo>
<mn>0</mn>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \lim _{n\to \infty }W^{n}=0}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \lim _{n\to \infty }W^{n}=0}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4f21d24f36ac54c2e3826fe618891ce17b19e12d" style="vertical-align: -1.838ex; width:12.647ex; height:3.843ex;"/></span> if the <a href="/wiki/Spectral_radius" title="Spectral radius">spectral radius</a> of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle W}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>W</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle W}</annotation>
</semantics>
</math></span><img alt="W" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/54a9c4c547f4d6111f81946cad242b18298d70b7" style="vertical-align: -0.338ex; width:2.435ex; height:2.176ex;"/></span> is smaller than 1.<sup class="reference" id="cite_ref-38"><a href="#cite_note-38">[38]</a></sup><sup class="reference" id="cite_ref-gradf_39-0"><a href="#cite_note-gradf-39">[39]</a></sup>
</p><p>However, with LSTM units, when error values are back-propagated from the output layer, the error remains in the LSTM unit's cell. This "error carousel" continuously feeds error back to each of the LSTM unit's gates, until they learn to cut off the value.
</p>
<h3><span class="mw-headline" id="CTC_score_function">CTC score function</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&action=edit&section=11" title="Edit section: CTC score function">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Many applications use stacks of LSTM RNNs<sup class="reference" id="cite_ref-fernandez2007_40-0"><a href="#cite_note-fernandez2007-40">[40]</a></sup> and train them by <a class="mw-redirect" href="/wiki/Connectionist_temporal_classification_(CTC)" title="Connectionist temporal classification (CTC)">connectionist temporal classification (CTC)</a><sup class="reference" id="cite_ref-graves2006_41-0"><a href="#cite_note-graves2006-41">[41]</a></sup> to find an RNN weight matrix that maximizes the probability of the label sequences in a training set, given the corresponding input sequences. CTC achieves both alignment and recognition.
</p>
<h3><span class="mw-headline" id="Alternatives">Alternatives</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&action=edit&section=12" title="Edit section: Alternatives">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Sometimes, it can be advantageous to train (parts of) an LSTM by <a href="/wiki/Neuroevolution" title="Neuroevolution">neuroevolution</a><sup class="reference" id="cite_ref-wierstra2005_42-0"><a href="#cite_note-wierstra2005-42">[42]</a></sup> or by policy gradient methods, especially when there is no "teacher" (that is, training labels).
</p>
<h4><span class="mw-headline" id="Success">Success</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&action=edit&section=13" title="Edit section: Success">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>There have been several successful stories of training, in a non-supervised fashion, RNNs with LSTM units.
</p><p>In 2018, <a href="/wiki/Bill_Gates" title="Bill Gates">Bill Gates</a> called it a “huge milestone in advancing artificial intelligence” when bots developed by <a href="/wiki/OpenAI" title="OpenAI">OpenAI</a> were able to beat humans in the game of Dota 2.<sup class="reference" id="cite_ref-OpenAIfive_43-0"><a href="#cite_note-OpenAIfive-43">[43]</a></sup> OpenAI Five consists of five independent but coordinated neural networks. Each network is trained by a policy gradient method without supervising teacher and contains a single-layer, 1024-unit Long-Short-Term-Memory that sees the current game state and emits actions through several possible action heads.<sup class="reference" id="cite_ref-OpenAIfive_43-1"><a href="#cite_note-OpenAIfive-43">[43]</a></sup>
</p><p>In 2018, <a href="/wiki/OpenAI" title="OpenAI">OpenAI</a> also trained a similar LSTM by policy gradients to control a human-like robot hand that manipulates physical objects with unprecedented dexterity.<sup class="reference" id="cite_ref-OpenAIhand_44-0"><a href="#cite_note-OpenAIhand-44">[44]</a></sup>
</p><p>In 2019, <a href="/wiki/DeepMind" title="DeepMind">DeepMind</a>'s program AlphaStar used a deep LSTM core to excel at the complex video game <a class="mw-redirect" href="/wiki/Starcraft_II" title="Starcraft II">Starcraft II</a>.<sup class="reference" id="cite_ref-alphastar_45-0"><a href="#cite_note-alphastar-45">[45]</a></sup> This was viewed as significant progress towards Artificial General Intelligence.<sup class="reference" id="cite_ref-alphastar_45-1"><a href="#cite_note-alphastar-45">[45]</a></sup>
</p>
<h2><span class="mw-headline" id="Applications">Applications</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&action=edit&section=14" title="Edit section: Applications">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Applications of LSTM include:
</p>
<ul><li><a href="/wiki/Robot_control" title="Robot control">Robot control</a><sup class="reference" id="cite_ref-46"><a href="#cite_note-46">[46]</a></sup></li>
<li><a class="mw-redirect" href="/wiki/Time_series_prediction" title="Time series prediction">Time series prediction</a><sup class="reference" id="cite_ref-wierstra2005_42-1"><a href="#cite_note-wierstra2005-42">[42]</a></sup></li>
<li><a href="/wiki/Speech_recognition" title="Speech recognition">Speech recognition</a><sup class="reference" id="cite_ref-47"><a href="#cite_note-47">[47]</a></sup><sup class="reference" id="cite_ref-48"><a href="#cite_note-48">[48]</a></sup><sup class="reference" id="cite_ref-ReferenceA_49-0"><a href="#cite_note-ReferenceA-49">[49]</a></sup></li>
<li>Rhythm learning<sup class="reference" id="cite_ref-peephole2002_35-2"><a href="#cite_note-peephole2002-35">[35]</a></sup></li>
<li>Music composition<sup class="reference" id="cite_ref-50"><a href="#cite_note-50">[50]</a></sup></li>
<li>Grammar learning<sup class="reference" id="cite_ref-51"><a href="#cite_note-51">[51]</a></sup><sup class="reference" id="cite_ref-peepholeLSTM_34-2"><a href="#cite_note-peepholeLSTM-34">[34]</a></sup><sup class="reference" id="cite_ref-52"><a href="#cite_note-52">[52]</a></sup></li>
<li><a href="/wiki/Handwriting_recognition" title="Handwriting recognition">Handwriting recognition</a><sup class="reference" id="cite_ref-53"><a href="#cite_note-53">[53]</a></sup><sup class="reference" id="cite_ref-54"><a href="#cite_note-54">[54]</a></sup></li>
<li>Human action recognition<sup class="reference" id="cite_ref-55"><a href="#cite_note-55">[55]</a></sup></li>
<li><a href="/wiki/Sign_language" title="Sign language">Sign language translation</a><sup class="reference" id="cite_ref-56"><a href="#cite_note-56">[56]</a></sup></li>
<li>Protein homology detection<sup class="reference" id="cite_ref-57"><a href="#cite_note-57">[57]</a></sup></li>
<li>Predicting subcellular localization of proteins<sup class="reference" id="cite_ref-58"><a href="#cite_note-58">[58]</a></sup></li>
<li>Time series anomaly detection<sup class="reference" id="cite_ref-59"><a href="#cite_note-59">[59]</a></sup></li>
<li>Several prediction tasks in the area of business process management<sup class="reference" id="cite_ref-60"><a href="#cite_note-60">[60]</a></sup></li>
<li>Prediction in medical care pathways<sup class="reference" id="cite_ref-61"><a href="#cite_note-61">[61]</a></sup></li>
<li><a href="/wiki/Semantic_parsing" title="Semantic parsing">Semantic parsing</a><sup class="reference" id="cite_ref-62"><a href="#cite_note-62">[62]</a></sup></li>
<li><a href="/wiki/Object_co-segmentation" title="Object co-segmentation">Object co-segmentation</a><sup class="reference" id="cite_ref-Wang_Duan_Zhang_Niu_p=1657_63-0"><a href="#cite_note-Wang_Duan_Zhang_Niu_p=1657-63">[63]</a></sup><sup class="reference" id="cite_ref-Duan_Wang_Zhai_Zheng_2018_p._64-0"><a href="#cite_note-Duan_Wang_Zhai_Zheng_2018_p.-64">[64]</a></sup></li>
<li>Airport passenger management<sup class="reference" id="cite_ref-65"><a href="#cite_note-65">[65]</a></sup></li>
<li>Short-term <a class="mw-redirect" href="/wiki/Traffic_forecast" title="Traffic forecast">traffic forecast</a><sup class="reference" id="cite_ref-66"><a href="#cite_note-66">[66]</a></sup></li></ul>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&action=edit&section=15" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">Recurrent neural network</a></li>
<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">Gated recurrent unit</a></li>
<li><a href="/wiki/Differentiable_neural_computer" title="Differentiable neural computer">Differentiable neural computer</a></li>
<li><a href="/wiki/Long-term_potentiation" title="Long-term potentiation">Long-term potentiation</a></li>
<li><a href="/wiki/Prefrontal_cortex_basal_ganglia_working_memory" title="Prefrontal cortex basal ganglia working memory">Prefrontal cortex basal ganglia working memory</a></li>
<li><a href="/wiki/Time_series" title="Time series">Time series</a></li>
<li><a href="/wiki/Seq2seq" title="Seq2seq">Seq2seq</a></li>
<li><a href="/wiki/Highway_network" title="Highway network">Highway network</a></li></ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&action=edit&section=16" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist" style="list-style-type: decimal;">
<div class="mw-references-wrap mw-references-columns"><ol class="references">
<li id="cite_note-lstm1997-1"><span class="mw-cite-backlink">^ <a href="#cite_ref-lstm1997_1-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-lstm1997_1-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-lstm1997_1-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFSepp_HochreiterJürgen_Schmidhuber1997"><a href="/wiki/Sepp_Hochreiter" title="Sepp Hochreiter">Sepp Hochreiter</a>; <a href="/wiki/J%C3%BCrgen_Schmidhuber" title="Jürgen Schmidhuber">Jürgen Schmidhuber</a> (1997). <a class="external text" href="https://www.researchgate.net/publication/13853244" rel="nofollow">"Long short-term memory"</a>. <i><a href="/wiki/Neural_Computation_(journal)" title="Neural Computation (journal)">Neural Computation</a></i>. <b>9</b> (8): 1735–1780. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1162%2Fneco.1997.9.8.1735" rel="nofollow">10.1162/neco.1997.9.8.1735</a>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/9377276" rel="nofollow">9377276</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Neural+Computation&rft.atitle=Long+short-term+memory&rft.volume=9&rft.issue=8&rft.pages=1735-1780&rft.date=1997&rft_id=info%3Adoi%2F10.1162%2Fneco.1997.9.8.1735&rft_id=info%3Apmid%2F9377276&rft.au=Sepp+Hochreiter&rft.au=J%C3%BCrgen+Schmidhuber&rft_id=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F13853244&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><style data-mw-deduplicate="TemplateStyles:r951705291">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background-image:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png");background-image:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg");background-repeat:no-repeat;background-size:9px;background-position:right .1em center}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background-image:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png");background-image:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg");background-repeat:no-repeat;background-size:9px;background-position:right .1em center}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background-image:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png");background-image:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg");background-repeat:no-repeat;background-size:9px;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background-image:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png");background-image:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg");background-repeat:no-repeat;background-size:12px;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}</style></span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFGravesLiwickiFernandezBertolami2009">Graves, A.; Liwicki, M.; Fernandez, S.; Bertolami, R.; Bunke, H.; <a href="/wiki/J%C3%BCrgen_Schmidhuber" title="Jürgen Schmidhuber">Schmidhuber, J.</a> (2009). <a class="external text" href="http://www.idsia.ch/~juergen/tpami_2008.pdf" rel="nofollow">"A Novel Connectionist System for Improved Unconstrained Handwriting Recognition"</a> <span class="cs1-format">(PDF)</span>. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>. <b>31</b> (5): 855–868. <a class="mw-redirect" href="/wiki/CiteSeerX_(identifier)" title="CiteSeerX (identifier)">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.139.4502" rel="nofollow">10.1.1.139.4502</a></span>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1109%2Ftpami.2008.137" rel="nofollow">10.1109/tpami.2008.137</a>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/19299860" rel="nofollow">19299860</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=IEEE+Transactions+on+Pattern+Analysis+and+Machine+Intelligence&rft.atitle=A+Novel+Connectionist+System+for+Improved+Unconstrained+Handwriting+Recognition&rft.volume=31&rft.issue=5&rft.pages=855-868&rft.date=2009&rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.139.4502&rft_id=info%3Apmid%2F19299860&rft_id=info%3Adoi%2F10.1109%2Ftpami.2008.137&rft.aulast=Graves&rft.aufirst=A.&rft.au=Liwicki%2C+M.&rft.au=Fernandez%2C+S.&rft.au=Bertolami%2C+R.&rft.au=Bunke%2C+H.&rft.au=Schmidhuber%2C+J.&rft_id=http%3A%2F%2Fwww.idsia.ch%2F~juergen%2Ftpami_2008.pdf&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-sak2014-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-sak2014_3-0">^</a></b></span> <span class="reference-text"><cite class="citation web cs1" id="CITEREFSakSeniorBeaufays2014">Sak, Hasim; Senior, Andrew; Beaufays, Francoise (2014). <a class="external text" href="https://web.archive.org/web/20180424203806/https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43905.pdf" rel="nofollow">"Long Short-Term Memory recurrent neural network architectures for large scale acoustic modeling"</a> <span class="cs1-format">(PDF)</span>. Archived from <a class="external text" href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43905.pdf" rel="nofollow">the original</a> <span class="cs1-format">(PDF)</span> on 2018-04-24.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&rft.genre=unknown&rft.btitle=Long+Short-Term+Memory+recurrent+neural+network+architectures+for+large+scale+acoustic+modeling&rft.date=2014&rft.aulast=Sak&rft.aufirst=Hasim&rft.au=Senior%2C+Andrew&rft.au=Beaufays%2C+Francoise&rft_id=https%3A%2F%2Fstatic.googleusercontent.com%2Fmedia%2Fresearch.google.com%2Fen%2F%2Fpubs%2Farchive%2F43905.pdf&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-liwu2015-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-liwu2015_4-0">^</a></b></span> <span class="reference-text"><cite class="citation arxiv cs1" id="CITEREFLiWu2014">Li, Xiangang; Wu, Xihong (2014-10-15). "Constructing Long Short-Term Memory based Deep Recurrent Neural Networks for Large Vocabulary Speech Recognition". <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1410.4281" rel="nofollow">1410.4281</a></span> [<a class="external text" href="//arxiv.org/archive/cs.CL" rel="nofollow">cs.CL</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=preprint&rft.jtitle=arXiv&rft.atitle=Constructing+Long+Short-Term+Memory+based+Deep+Recurrent+Neural+Networks+for+Large+Vocabulary+Speech+Recognition&rft.date=2014-10-15&rft_id=info%3Aarxiv%2F1410.4281&rft.aulast=Li&rft.aufirst=Xiangang&rft.au=Wu%2C+Xihong&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-ASearchSpaceOdyssey-5"><span class="mw-cite-backlink">^ <a href="#cite_ref-ASearchSpaceOdyssey_5-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-ASearchSpaceOdyssey_5-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-ASearchSpaceOdyssey_5-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-ASearchSpaceOdyssey_5-3"><sup><i><b>d</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFKlaus_GreffRupesh_Kumar_SrivastavaJan_KoutníkBas_R._Steunebrink2015">Klaus Greff; Rupesh Kumar Srivastava; Jan Koutník; Bas R. Steunebrink; Jürgen Schmidhuber (2015). "LSTM: A Search Space Odyssey". <i>IEEE Transactions on Neural Networks and Learning Systems</i>. <b>28</b> (10): 2222–2232. <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1503.04069" rel="nofollow">1503.04069</a></span>. <a class="mw-redirect" href="/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a class="external text" href="https://ui.adsabs.harvard.edu/abs/2015arXiv150304069G" rel="nofollow">2015arXiv150304069G</a>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1109%2FTNNLS.2016.2582924" rel="nofollow">10.1109/TNNLS.2016.2582924</a>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/27411231" rel="nofollow">27411231</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=IEEE+Transactions+on+Neural+Networks+and+Learning+Systems&rft.atitle=LSTM%3A+A+Search+Space+Odyssey&rft.volume=28&rft.issue=10&rft.pages=2222-2232&rft.date=2015&rft_id=info%3Aarxiv%2F1503.04069&rft_id=info%3Apmid%2F27411231&rft_id=info%3Adoi%2F10.1109%2FTNNLS.2016.2582924&rft_id=info%3Abibcode%2F2015arXiv150304069G&rft.au=Klaus+Greff&rft.au=Rupesh+Kumar+Srivastava&rft.au=Jan+Koutn%C3%ADk&rft.au=Bas+R.+Steunebrink&rft.au=J%C3%BCrgen+Schmidhuber&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-lstm1999-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-lstm1999_6-0">^</a></b></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFGers1999">Gers, F.A. (1999). "Learning to forget: Continual prediction with LSTM". <i>9th International Conference on Artificial Neural Networks: ICANN '99</i>. <b>1999</b>. pp. 850–855. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1049%2Fcp%3A19991218" rel="nofollow">10.1049/cp:19991218</a>. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a> <a href="/wiki/Special:BookSources/0-85296-721-7" title="Special:BookSources/0-85296-721-7"><bdi>0-85296-721-7</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&rft.genre=bookitem&rft.atitle=Learning+to+forget%3A+Continual+prediction+with+LSTM&rft.btitle=9th+International+Conference+on+Artificial+Neural+Networks%3A+ICANN+%2799&rft.pages=850-855&rft.date=1999&rft_id=info%3Adoi%2F10.1049%2Fcp%3A19991218&rft.isbn=0-85296-721-7&rft.aulast=Gers&rft.aufirst=F.A.&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-lstm2000-7"><span class="mw-cite-backlink">^ <a href="#cite_ref-lstm2000_7-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-lstm2000_7-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFFelix_A._GersJürgen_SchmidhuberFred_Cummins2000">Felix A. Gers; Jürgen Schmidhuber; Fred Cummins (2000). "Learning to Forget: Continual Prediction with LSTM". <i><a href="/wiki/Neural_Computation_(journal)" title="Neural Computation (journal)">Neural Computation</a></i>. <b>12</b> (10): 2451–2471. <a class="mw-redirect" href="/wiki/CiteSeerX_(identifier)" title="CiteSeerX (identifier)">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.5709" rel="nofollow">10.1.1.55.5709</a></span>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1162%2F089976600300015015" rel="nofollow">10.1162/089976600300015015</a>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/11032042" rel="nofollow">11032042</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Neural+Computation&rft.atitle=Learning+to+Forget%3A+Continual+Prediction+with+LSTM&rft.volume=12&rft.issue=10&rft.pages=2451-2471&rft.date=2000&rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.55.5709&rft_id=info%3Apmid%2F11032042&rft_id=info%3Adoi%2F10.1162%2F089976600300015015&rft.au=Felix+A.+Gers&rft.au=J%C3%BCrgen+Schmidhuber&rft.au=Fred+Cummins&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFGravesLiwickiFernándezBertolami2009">Graves, A.; Liwicki, M.; Fernández, S.; Bertolami, R.; Bunke, H.; Schmidhuber, J. (May 2009). "A Novel Connectionist System for Unconstrained Handwriting Recognition". <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>. <b>31</b> (5): 855–868. <a class="mw-redirect" href="/wiki/CiteSeerX_(identifier)" title="CiteSeerX (identifier)">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.139.4502" rel="nofollow">10.1.1.139.4502</a></span>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1109%2Ftpami.2008.137" rel="nofollow">10.1109/tpami.2008.137</a>. <a class="mw-redirect" href="/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a> <a class="external text" href="//www.worldcat.org/issn/0162-8828" rel="nofollow">0162-8828</a>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/19299860" rel="nofollow">19299860</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=IEEE+Transactions+on+Pattern+Analysis+and+Machine+Intelligence&rft.atitle=A+Novel+Connectionist+System+for+Unconstrained+Handwriting+Recognition&rft.volume=31&rft.issue=5&rft.pages=855-868&rft.date=2009-05&rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.139.4502&rft.issn=0162-8828&rft_id=info%3Apmid%2F19299860&rft_id=info%3Adoi%2F10.1109%2Ftpami.2008.137&rft.aulast=Graves&rft.aufirst=A.&rft.au=Liwicki%2C+M.&rft.au=Fern%C3%A1ndez%2C+S.&rft.au=Bertolami%2C+R.&rft.au=Bunke%2C+H.&rft.au=Schmidhuber%2C+J.&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFMärgnerAbed2009">Märgner, Volker; Abed, Haikal El (July 2009). <a class="external text" href="https://ieeexplore.ieee.org/document/5277771/" rel="nofollow">"ICDAR 2009 Arabic Handwriting Recognition Competition"</a>. <i>2009 10th International Conference on Document Analysis and Recognition</i>: 1383–1387. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1109%2FICDAR.2009.256" rel="nofollow">10.1109/ICDAR.2009.256</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=2009+10th+International+Conference+on+Document+Analysis+and+Recognition&rft.atitle=ICDAR+2009+Arabic+Handwriting+Recognition+Competition&rft.pages=1383-1387&rft.date=2009-07&rft_id=info%3Adoi%2F10.1109%2FICDAR.2009.256&rft.aulast=M%C3%A4rgner&rft.aufirst=Volker&rft.au=Abed%2C+Haikal+El&rft_id=https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F5277771%2F&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text"><cite class="citation arxiv cs1" id="CITEREFGravesMohamedHinton2013">Graves, Alex; Mohamed, Abdel-rahman; Hinton, Geoffrey (2013-03-22). "Speech Recognition with Deep Recurrent Neural Networks". <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1303.5778" rel="nofollow">1303.5778</a></span> [<a class="external text" href="//arxiv.org/archive/cs.NE" rel="nofollow">cs.NE</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=preprint&rft.jtitle=arXiv&rft.atitle=Speech+Recognition+with+Deep+Recurrent+Neural+Networks&rft.date=2013-03-22&rft_id=info%3Aarxiv%2F1303.5778&rft.aulast=Graves&rft.aufirst=Alex&rft.au=Mohamed%2C+Abdel-rahman&rft.au=Hinton%2C+Geoffrey&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text"><cite class="citation arxiv cs1" id="CITEREFChovan_MerrienboerGulcehreBahdanau2014">Cho, Kyunghyun; van Merrienboer, Bart; Gulcehre, Caglar; Bahdanau, Dzmitry; Bougares, Fethi; Schwenk, Holger; Bengio, Yoshua (2014). "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation". <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1406.1078" rel="nofollow">1406.1078</a></span> [<a class="external text" href="//arxiv.org/archive/cs.CL" rel="nofollow">cs.CL</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=preprint&rft.jtitle=arXiv&rft.atitle=Learning+Phrase+Representations+using+RNN+Encoder-Decoder+for+Statistical+Machine+Translation&rft.date=2014&rft_id=info%3Aarxiv%2F1406.1078&rft.aulast=Cho&rft.aufirst=Kyunghyun&rft.au=van+Merrienboer%2C+Bart&rft.au=Gulcehre%2C+Caglar&rft.au=Bahdanau%2C+Dzmitry&rft.au=Bougares%2C+Fethi&rft.au=Schwenk%2C+Holger&rft.au=Bengio%2C+Yoshua&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-Beau15-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-Beau15_12-0">^</a></b></span> <span class="reference-text"><cite class="citation news cs1" id="CITEREFBeaufays2015">Beaufays, Françoise (August 11, 2015). <a class="external text" href="http://googleresearch.blogspot.co.at/2015/08/the-neural-networks-behind-google-voice.html" rel="nofollow">"The neural networks behind Google Voice transcription"</a>. <i>Research Blog</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-06-27</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Research+Blog&rft.atitle=The+neural+networks+behind+Google+Voice+transcription&rft.date=2015-08-11&rft.aulast=Beaufays&rft.aufirst=Fran%C3%A7oise&rft_id=http%3A%2F%2Fgoogleresearch.blogspot.co.at%2F2015%2F08%2Fthe-neural-networks-behind-google-voice.html&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-GoogleVoiceSearch-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-GoogleVoiceSearch_13-0">^</a></b></span> <span class="reference-text"><cite class="citation news cs1" id="CITEREFSakSeniorRaoBeaufays2015">Sak, Haşim; Senior, Andrew; Rao, Kanishka; Beaufays, Françoise; Schalkwyk, Johan (September 24, 2015). <a class="external text" href="http://googleresearch.blogspot.co.uk/2015/09/google-voice-search-faster-and-more.html" rel="nofollow">"Google voice search: faster and more accurate"</a>. <i>Research Blog</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-06-27</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Research+Blog&rft.atitle=Google+voice+search%3A+faster+and+more+accurate&rft.date=2015-09-24&rft.aulast=Sak&rft.aufirst=Ha%C5%9Fim&rft.au=Senior%2C+Andrew&rft.au=Rao%2C+Kanishka&rft.au=Beaufays%2C+Fran%C3%A7oise&rft.au=Schalkwyk%2C+Johan&rft_id=http%3A%2F%2Fgoogleresearch.blogspot.co.uk%2F2015%2F09%2Fgoogle-voice-search-faster-and-more.html&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14">^</a></b></span> <span class="reference-text"><cite class="citation web cs1"><a class="external text" href="https://googleblog.blogspot.com/2015/07/neon-prescription-or-rather-new.html" rel="nofollow">"Neon prescription... or rather, New transcription for Google Voice"</a>. <i>Official Google Blog</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2020-04-25</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=unknown&rft.jtitle=Official+Google+Blog&rft.atitle=Neon+prescription...+or+rather%2C+New+transcription+for+Google+Voice&rft_id=https%3A%2F%2Fgoogleblog.blogspot.com%2F2015%2F07%2Fneon-prescription-or-rather-new.html&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-GoogleAllo-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-GoogleAllo_15-0">^</a></b></span> <span class="reference-text"><cite class="citation news cs1" id="CITEREFKhaitan2016">Khaitan, Pranav (May 18, 2016). <a class="external text" href="http://googleresearch.blogspot.co.at/2016/05/chat-smarter-with-allo.html" rel="nofollow">"Chat Smarter with Allo"</a>. <i>Research Blog</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-06-27</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Research+Blog&rft.atitle=Chat+Smarter+with+Allo&rft.date=2016-05-18&rft.aulast=Khaitan&rft.aufirst=Pranav&rft_id=http%3A%2F%2Fgoogleresearch.blogspot.co.at%2F2016%2F05%2Fchat-smarter-with-allo.html&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-GoogleTranslate-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-GoogleTranslate_16-0">^</a></b></span> <span class="reference-text"><cite class="citation arxiv cs1" id="CITEREFWuSchusterChenLe2016">Wu, Yonghui; Schuster, Mike; Chen, Zhifeng; Le, Quoc V.; Norouzi, Mohammad; Macherey, Wolfgang; Krikun, Maxim; Cao, Yuan; Gao, Qin (2016-09-26). "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation". <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1609.08144" rel="nofollow">1609.08144</a></span> [<a class="external text" href="//arxiv.org/archive/cs.CL" rel="nofollow">cs.CL</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=preprint&rft.jtitle=arXiv&rft.atitle=Google%27s+Neural+Machine+Translation+System%3A+Bridging+the+Gap+between+Human+and+Machine+Translation&rft.date=2016-09-26&rft_id=info%3Aarxiv%2F1609.08144&rft.aulast=Wu&rft.aufirst=Yonghui&rft.au=Schuster%2C+Mike&rft.au=Chen%2C+Zhifeng&rft.au=Le%2C+Quoc+V.&rft.au=Norouzi%2C+Mohammad&rft.au=Macherey%2C+Wolfgang&rft.au=Krikun%2C+Maxim&rft.au=Cao%2C+Yuan&rft.au=Gao%2C+Qin&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-WiredGoogleTranslate-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-WiredGoogleTranslate_17-0">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFMetz2016">Metz, Cade (September 27, 2016). <a class="external text" href="https://www.wired.com/2016/09/google-claims-ai-breakthrough-machine-translation/" rel="nofollow">"An Infusion of AI Makes Google Translate More Powerful Than Ever | WIRED"</a>. <i>Wired</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-06-27</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Wired&rft.atitle=An+Infusion+of+AI+Makes+Google+Translate+More+Powerful+Than+Ever+%7C+WIRED&rft.date=2016-09-27&rft.aulast=Metz&rft.aufirst=Cade&rft_id=https%3A%2F%2Fwww.wired.com%2F2016%2F09%2Fgoogle-claims-ai-breakthrough-machine-translation%2F&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-18">^</a></b></span> <span class="reference-text"><cite class="citation web cs1"><a class="external text" href="http://ai.googleblog.com/2016/09/a-neural-network-for-machine.html" rel="nofollow">"A Neural Network for Machine Translation, at Production Scale"</a>. <i>Google AI Blog</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2020-04-25</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=unknown&rft.jtitle=Google+AI+Blog&rft.atitle=A+Neural+Network+for+Machine+Translation%2C+at+Production+Scale&rft_id=http%3A%2F%2Fai.googleblog.com%2F2016%2F09%2Fa-neural-network-for-machine.html&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-AppleQuicktype-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-AppleQuicktype_19-0">^</a></b></span> <span class="reference-text"><cite class="citation web cs1" id="CITEREFEfrati2016">Efrati, Amir (June 13, 2016). <a class="external text" href="https://www.theinformation.com/apples-machines-can-learn-too" rel="nofollow">"Apple's Machines Can Learn Too"</a>. <i>The Information</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-06-27</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=unknown&rft.jtitle=The+Information&rft.atitle=Apple%27s+Machines+Can+Learn+Too&rft.date=2016-06-13&rft.aulast=Efrati&rft.aufirst=Amir&rft_id=https%3A%2F%2Fwww.theinformation.com%2Fapples-machines-can-learn-too&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-AppleQuicktype2-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-AppleQuicktype2_20-0">^</a></b></span> <span class="reference-text"><cite class="citation news cs1" id="CITEREFRanger2016">Ranger, Steve (June 14, 2016). <a class="external text" href="https://www.zdnet.com/article/ai-big-data-and-the-iphone-heres-how-apple-plans-to-protect-your-privacy" rel="nofollow">"iPhone, AI and big data: Here's how Apple plans to protect your privacy | ZDNet"</a>. <i>ZDNet</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-06-27</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=ZDNet&rft.atitle=iPhone%2C+AI+and+big+data%3A+Here%27s+how+Apple+plans+to+protect+your+privacy+%7C+ZDNet&rft.date=2016-06-14&rft.aulast=Ranger&rft.aufirst=Steve&rft_id=http%3A%2F%2Fwww.zdnet.com%2Farticle%2Fai-big-data-and-the-iphone-heres-how-apple-plans-to-protect-your-privacy&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text"><cite class="citation web cs1"><a class="external text" href="https://machinelearning.apple.com/2018/09/27/can-global-semantic-context-improve-neural-language-models.html" rel="nofollow">"Can Global Semantic Context Improve Neural Language Models? - Apple"</a>. <i>Apple Machine Learning Journal</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2020-04-30</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=unknown&rft.jtitle=Apple+Machine+Learning+Journal&rft.atitle=Can+Global+Semantic+Context+Improve+Neural+Language+Models%3F+-+Apple&rft_id=https%3A%2F%2Fmachinelearning.apple.com%2F2018%2F09%2F27%2Fcan-global-semantic-context-improve-neural-language-models.html&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-AppleSiri-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-AppleSiri_22-0">^</a></b></span> <span class="reference-text"><cite class="citation web cs1" id="CITEREFSmith2016">Smith, Chris (2016-06-13). <a class="external text" href="http://bgr.com/2016/06/13/ios-10-siri-third-party-apps/" rel="nofollow">"iOS 10: Siri now works in third-party apps, comes with extra AI features"</a>. <i>BGR</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-06-27</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=unknown&rft.jtitle=BGR&rft.atitle=iOS+10%3A+Siri+now+works+in+third-party+apps%2C+comes+with+extra+AI+features&rft.date=2016-06-13&rft.aulast=Smith&rft.aufirst=Chris&rft_id=http%3A%2F%2Fbgr.com%2F2016%2F06%2F13%2Fios-10-siri-third-party-apps%2F&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFCapesColesConkieGolipour2017">Capes, Tim; Coles, Paul; Conkie, Alistair; Golipour, Ladan; Hadjitarkhani, Abie; Hu, Qiong; Huddleston, Nancy; Hunt, Melvyn; Li, Jiangchuan; Neeracher, Matthias; Prahallad, Kishore (2017-08-20). <a class="external text" href="http://www.isca-speech.org/archive/Interspeech_2017/abstracts/1798.html" rel="nofollow">"Siri On-Device Deep Learning-Guided Unit Selection Text-to-Speech System"</a>. <i>Interspeech 2017</i>. ISCA: 4011–4015. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.21437%2FInterspeech.2017-1798" rel="nofollow">10.21437/Interspeech.2017-1798</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Interspeech+2017&rft.atitle=Siri+On-Device+Deep+Learning-Guided+Unit+Selection+Text-to-Speech+System&rft.pages=4011-4015&rft.date=2017-08-20&rft_id=info%3Adoi%2F10.21437%2FInterspeech.2017-1798&rft.aulast=Capes&rft.aufirst=Tim&rft.au=Coles%2C+Paul&rft.au=Conkie%2C+Alistair&rft.au=Golipour%2C+Ladan&rft.au=Hadjitarkhani%2C+Abie&rft.au=Hu%2C+Qiong&rft.au=Huddleston%2C+Nancy&rft.au=Hunt%2C+Melvyn&rft.au=Li%2C+Jiangchuan&rft.au=Neeracher%2C+Matthias&rft.au=Prahallad%2C+Kishore&rft_id=http%3A%2F%2Fwww.isca-speech.org%2Farchive%2FInterspeech_2017%2Fabstracts%2F1798.html&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-AmazonAlexa-24"><span class="mw-cite-backlink"><b><a href="#cite_ref-AmazonAlexa_24-0">^</a></b></span> <span class="reference-text"><cite class="citation web cs1" id="CITEREFVogels2016">Vogels, Werner (30 November 2016). <a class="external text" href="http://www.allthingsdistributed.com/2016/11/amazon-ai-and-alexa-for-all-aws-apps.html" rel="nofollow">"Bringing the Magic of Amazon AI and Alexa to Apps on AWS. – All Things Distributed"</a>. <i>www.allthingsdistributed.com</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-06-27</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=unknown&rft.jtitle=www.allthingsdistributed.com&rft.atitle=Bringing+the+Magic+of+Amazon+AI+and+Alexa+to+Apps+on+AWS.+%E2%80%93+All+Things+Distributed&rft.date=2016-11-30&rft.aulast=Vogels&rft.aufirst=Werner&rft_id=http%3A%2F%2Fwww.allthingsdistributed.com%2F2016%2F11%2Famazon-ai-and-alexa-for-all-aws-apps.html&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-FacebookTranslate-25"><span class="mw-cite-backlink"><b><a href="#cite_ref-FacebookTranslate_25-0">^</a></b></span> <span class="reference-text"><cite class="citation web cs1" id="CITEREFOng2017">Ong, Thuy (4 August 2017). <a class="external text" href="https://www.theverge.com/2017/8/4/16093872/facebook-ai-translations-artificial-intelligence" rel="nofollow">"Facebook's translations are now powered completely by AI"</a>. <i>www.allthingsdistributed.com</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2019-02-15</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=unknown&rft.jtitle=www.allthingsdistributed.com&rft.atitle=Facebook%27s+translations+are+now+powered+completely+by+AI&rft.date=2017-08-04&rft.aulast=Ong&rft.aufirst=Thuy&rft_id=https%3A%2F%2Fwww.theverge.com%2F2017%2F8%2F4%2F16093872%2Ffacebook-ai-translations-artificial-intelligence&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-26"><span class="mw-cite-backlink"><b><a href="#cite_ref-26">^</a></b></span> <span class="reference-text"><cite class="citation web cs1"><a class="external text" href="http://biometrics.cse.msu.edu/Publications/MachineLearning/Baytasetal_PatientSubtypingViaTimeAwareLSTMNetworks.pdf" rel="nofollow">"Patient Subtyping via Time-Aware LSTM Networks"</a> <span class="cs1-format">(PDF)</span>. <i>msu.edu</i><span class="reference-accessdate">. Retrieved <span class="nowrap">21 Nov</span> 2018</span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=unknown&rft.jtitle=msu.edu&rft.atitle=Patient+Subtyping+via+Time-Aware+LSTM+Networks&rft_id=http%3A%2F%2Fbiometrics.cse.msu.edu%2FPublications%2FMachineLearning%2FBaytasetal_PatientSubtypingViaTimeAwareLSTMNetworks.pdf&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-27"><span class="mw-cite-backlink"><b><a href="#cite_ref-27">^</a></b></span> <span class="reference-text"><cite class="citation web cs1"><a class="external text" href="http://www.kdd.org/kdd2017/papers/view/patient-subtyping-via-time-aware-lstm-networks" rel="nofollow">"Patient Subtyping via Time-Aware LSTM Networks"</a>. <i>Kdd.org</i><span class="reference-accessdate">. Retrieved <span class="nowrap">24 May</span> 2018</span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=unknown&rft.jtitle=Kdd.org&rft.atitle=Patient+Subtyping+via+Time-Aware+LSTM+Networks&rft_id=http%3A%2F%2Fwww.kdd.org%2Fkdd2017%2Fpapers%2Fview%2Fpatient-subtyping-via-time-aware-lstm-networks&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-28"><span class="mw-cite-backlink"><b><a href="#cite_ref-28">^</a></b></span> <span class="reference-text"><cite class="citation web cs1"><a class="external text" href="http://www.kdd.org" rel="nofollow">"SIGKDD"</a>. <i>Kdd.org</i><span class="reference-accessdate">. Retrieved <span class="nowrap">24 May</span> 2018</span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=unknown&rft.jtitle=Kdd.org&rft.atitle=SIGKDD&rft_id=http%3A%2F%2Fwww.kdd.org&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-29"><span class="mw-cite-backlink"><b><a href="#cite_ref-29">^</a></b></span> <span class="reference-text"><cite class="citation web cs1" id="CITEREFHaridy2017">Haridy, Rich (August 21, 2017). <a class="external text" href="http://newatlas.com/microsoft-speech-recognition-equals-humans/50999" rel="nofollow">"Microsoft's speech recognition system is now as good as a human"</a>. <i>newatlas.com</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-08-27</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=unknown&rft.jtitle=newatlas.com&rft.atitle=Microsoft%27s+speech+recognition+system+is+now+as+good+as+a+human&rft.date=2017-08-21&rft.aulast=Haridy&rft.aufirst=Rich&rft_id=http%3A%2F%2Fnewatlas.com%2Fmicrosoft-speech-recognition-equals-humans%2F50999&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-30"><span class="mw-cite-backlink"><b><a href="#cite_ref-30">^</a></b></span> <span class="reference-text"><cite class="citation conference cs1" id="CITEREFVoelkerKajićEliasmith2019">Voelker, Aaron R.; Kajić, Ivana; Eliasmith, Chris (2019). <a class="external text" href="http://compneuro.uwaterloo.ca/files/publications/voelker.2019.lmu.pdf" rel="nofollow"><i>Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks</i></a> <span class="cs1-format">(PDF)</span>. <a class="external text" href="https://neurips.cc" rel="nofollow">Advances in Neural Information Processing Systems</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&rft.genre=conference&rft.btitle=Legendre+Memory+Units%3A+Continuous-Time+Representation+in+Recurrent+Neural+Networks&rft.date=2019&rft.aulast=Voelker&rft.aufirst=Aaron+R.&rft.au=Kaji%C4%87%2C+Ivana&rft.au=Eliasmith%2C+Chris&rft_id=http%3A%2F%2Fcompneuro.uwaterloo.ca%2Ffiles%2Fpublications%2Fvoelker.2019.lmu.pdf&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-31"><span class="mw-cite-backlink"><b><a href="#cite_ref-31">^</a></b></span> <span class="reference-text"><cite class="citation web cs1"><a class="external text" href="http://www.mattmahoney.net/dc/text.html#1218" rel="nofollow">"The Large Text Compression Benchmark"</a><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-01-13</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&rft.genre=unknown&rft.btitle=The+Large+Text+Compression+Benchmark&rft_id=http%3A%2F%2Fwww.mattmahoney.net%2Fdc%2Ftext.html%231218&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-32"><span class="mw-cite-backlink"><b><a href="#cite_ref-32">^</a></b></span> <span class="reference-text">Fabrice Bellard (2019), "<a class="external text" href="https://bellard.org/nncp/nncp.pdf" rel="nofollow">Lossless Data Compression with Neural Networks</a>"</span>
</li>
<li id="cite_note-33"><span class="mw-cite-backlink"><b><a href="#cite_ref-33">^</a></b></span> <span class="reference-text"><cite class="citation web cs1" id="CITEREFbro">bro, n. <a class="external text" href="https://stats.stackexchange.com/q/320919/82135" rel="nofollow">"Why can RNNs with LSTM units also suffer from "exploding gradients"?"</a>. <i>Cross Validated</i><span class="reference-accessdate">. Retrieved <span class="nowrap">25 December</span> 2018</span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=unknown&rft.jtitle=Cross+Validated&rft.atitle=Why+can+RNNs+with+LSTM+units+also+suffer+from+%22exploding+gradients%22%3F&rft.aulast=bro&rft.aufirst=n&rft_id=https%3A%2F%2Fstats.stackexchange.com%2Fq%2F320919%2F82135&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-peepholeLSTM-34"><span class="mw-cite-backlink">^ <a href="#cite_ref-peepholeLSTM_34-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-peepholeLSTM_34-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-peepholeLSTM_34-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFGersSchmidhuber2001">Gers, F. A.; Schmidhuber, J. (2001). <a class="external text" href="ftp://ftp.idsia.ch/pub/juergen/L-IEEE.pdf" rel="nofollow">"LSTM Recurrent Networks Learn Simple Context Free and Context Sensitive Languages"</a> <span class="cs1-format">(PDF)</span>. <i>IEEE Transactions on Neural Networks</i>. <b>12</b> (6): 1333–1340. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1109%2F72.963769" rel="nofollow">10.1109/72.963769</a>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/18249962" rel="nofollow">18249962</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=IEEE+Transactions+on+Neural+Networks&rft.atitle=LSTM+Recurrent+Networks+Learn+Simple+Context+Free+and+Context+Sensitive+Languages&rft.volume=12&rft.issue=6&rft.pages=1333-1340&rft.date=2001&rft_id=info%3Adoi%2F10.1109%2F72.963769&rft_id=info%3Apmid%2F18249962&rft.aulast=Gers&rft.aufirst=F.+A.&rft.au=Schmidhuber%2C+J.&rft_id=ftp%3A%2F%2Fftp.idsia.ch%2Fpub%2Fjuergen%2FL-IEEE.pdf&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-peephole2002-35"><span class="mw-cite-backlink">^ <a href="#cite_ref-peephole2002_35-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-peephole2002_35-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-peephole2002_35-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFGersSchraudolphSchmidhuber2002">Gers, F.; Schraudolph, N.; Schmidhuber, J. (2002). <a class="external text" href="http://www.jmlr.org/papers/volume3/gers02a/gers02a.pdf" rel="nofollow">"Learning precise timing with LSTM recurrent networks"</a> <span class="cs1-format">(PDF)</span>. <i>Journal of Machine Learning Research</i>. <b>3</b>: 115–143.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Journal+of+Machine+Learning+Research&rft.atitle=Learning+precise+timing+with+LSTM+recurrent+networks&rft.volume=3&rft.pages=115-143&rft.date=2002&rft.aulast=Gers&rft.aufirst=F.&rft.au=Schraudolph%2C+N.&rft.au=Schmidhuber%2C+J.&rft_id=http%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fvolume3%2Fgers02a%2Fgers02a.pdf&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-36"><span class="mw-cite-backlink"><b><a href="#cite_ref-36">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFGersSchmidhuber2001">Gers, F. A.; Schmidhuber, E. (November 2001). <a class="external text" href="ftp://ftp.idsia.ch/pub/juergen/L-IEEE.pdf" rel="nofollow">"LSTM recurrent networks learn simple context-free and context-sensitive languages"</a> <span class="cs1-format">(PDF)</span>. <i>IEEE Transactions on Neural Networks</i>. <b>12</b> (6): 1333–1340. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1109%2F72.963769" rel="nofollow">10.1109/72.963769</a>. <a class="mw-redirect" href="/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a> <a class="external text" href="//www.worldcat.org/issn/1045-9227" rel="nofollow">1045-9227</a>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/18249962" rel="nofollow">18249962</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=IEEE+Transactions+on+Neural+Networks&rft.atitle=LSTM+recurrent+networks+learn+simple+context-free+and+context-sensitive+languages&rft.volume=12&rft.issue=6&rft.pages=1333-1340&rft.date=2001-11&rft.issn=1045-9227&rft_id=info%3Apmid%2F18249962&rft_id=info%3Adoi%2F10.1109%2F72.963769&rft.aulast=Gers&rft.aufirst=F.+A.&rft.au=Schmidhuber%2C+E.&rft_id=ftp%3A%2F%2Fftp.idsia.ch%2Fpub%2Fjuergen%2FL-IEEE.pdf&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-37"><span class="mw-cite-backlink"><b><a href="#cite_ref-37">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFXingjian_ShiZhourong_ChenHao_WangDit-Yan_Yeung2015">Xingjian Shi; Zhourong Chen; Hao Wang; Dit-Yan Yeung; Wai-kin Wong; Wang-chun Woo (2015). "Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting". <i>Proceedings of the 28th International Conference on Neural Information Processing Systems</i>: 802–810. <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1506.04214" rel="nofollow">1506.04214</a></span>. <a class="mw-redirect" href="/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a class="external text" href="https://ui.adsabs.harvard.edu/abs/2015arXiv150604214S" rel="nofollow">2015arXiv150604214S</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Proceedings+of+the+28th+International+Conference+on+Neural+Information+Processing+Systems&rft.atitle=Convolutional+LSTM+Network%3A+A+Machine+Learning+Approach+for+Precipitation+Nowcasting&rft.pages=802-810&rft.date=2015&rft_id=info%3Aarxiv%2F1506.04214&rft_id=info%3Abibcode%2F2015arXiv150604214S&rft.au=Xingjian+Shi&rft.au=Zhourong+Chen&rft.au=Hao+Wang&rft.au=Dit-Yan+Yeung&rft.au=Wai-kin+Wong&rft.au=Wang-chun+Woo&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-38"><span class="mw-cite-backlink"><b><a href="#cite_ref-38">^</a></b></span> <span class="reference-text">S. Hochreiter. Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut f. Informatik, Technische Univ. Munich, 1991.</span>
</li>
<li id="cite_note-gradf-39"><span class="mw-cite-backlink"><b><a href="#cite_ref-gradf_39-0">^</a></b></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFHochreiterBengioFrasconiSchmidhuber2001">Hochreiter, S.; Bengio, Y.; Frasconi, P.; Schmidhuber, J. (2001). <a class="external text" href="https://www.researchgate.net/publication/2839938" rel="nofollow">"Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies (PDF Download Available)"</a>.  In Kremer and, S. C.; Kolen, J. F. (eds.). <i>A Field Guide to Dynamical Recurrent Neural Networks</i>. IEEE Press.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&rft.genre=bookitem&rft.atitle=Gradient+Flow+in+Recurrent+Nets%3A+the+Difficulty+of+Learning+Long-Term+Dependencies+%28PDF+Download+Available%29&rft.btitle=A+Field+Guide+to+Dynamical+Recurrent+Neural+Networks.&rft.pub=IEEE+Press&rft.date=2001&rft.aulast=Hochreiter&rft.aufirst=S.&rft.au=Bengio%2C+Y.&rft.au=Frasconi%2C+P.&rft.au=Schmidhuber%2C+J.&rft_id=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F2839938&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-fernandez2007-40"><span class="mw-cite-backlink"><b><a href="#cite_ref-fernandez2007_40-0">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFFernándezGravesSchmidhuber2007">Fernández, Santiago; Graves, Alex; Schmidhuber, Jürgen (2007). "Sequence labelling in structured domains with hierarchical recurrent neural networks". <i>Proc. 20th Int. Joint Conf. On Artificial Intelligence, Ijcai 2007</i>: 774–779. <a class="mw-redirect" href="/wiki/CiteSeerX_(identifier)" title="CiteSeerX (identifier)">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.79.1887" rel="nofollow">10.1.1.79.1887</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Proc.+20th+Int.+Joint+Conf.+On+Artificial+Intelligence%2C+Ijcai+2007&rft.atitle=Sequence+labelling+in+structured+domains+with+hierarchical+recurrent+neural+networks&rft.pages=774-779&rft.date=2007&rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.79.1887&rft.aulast=Fern%C3%A1ndez&rft.aufirst=Santiago&rft.au=Graves%2C+Alex&rft.au=Schmidhuber%2C+J%C3%BCrgen&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-graves2006-41"><span class="mw-cite-backlink"><b><a href="#cite_ref-graves2006_41-0">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFGravesFernándezGomez2006">Graves, Alex; Fernández, Santiago; Gomez, Faustino (2006). "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks". <i>In Proceedings of the International Conference on Machine Learning, ICML 2006</i>: 369–376. <a class="mw-redirect" href="/wiki/CiteSeerX_(identifier)" title="CiteSeerX (identifier)">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.75.6306" rel="nofollow">10.1.1.75.6306</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=In+Proceedings+of+the+International+Conference+on+Machine+Learning%2C+ICML+2006&rft.atitle=Connectionist+temporal+classification%3A+Labelling+unsegmented+sequence+data+with+recurrent+neural+networks&rft.pages=369-376&rft.date=2006&rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.75.6306&rft.aulast=Graves&rft.aufirst=Alex&rft.au=Fern%C3%A1ndez%2C+Santiago&rft.au=Gomez%2C+Faustino&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-wierstra2005-42"><span class="mw-cite-backlink">^ <a href="#cite_ref-wierstra2005_42-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-wierstra2005_42-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFWierstraSchmidhuberGomez2005">Wierstra, Daan; Schmidhuber, J.; Gomez, F. J. (2005). <a class="external text" href="https://www.academia.edu/5830256" rel="nofollow">"Evolino: Hybrid Neuroevolution/Optimal Linear Search for Sequence Learning"</a>. <i>Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI), Edinburgh</i>: 853–858.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Proceedings+of+the+19th+International+Joint+Conference+on+Artificial+Intelligence+%28IJCAI%29%2C+Edinburgh&rft.atitle=Evolino%3A+Hybrid+Neuroevolution%2FOptimal+Linear+Search+for+Sequence+Learning&rft.pages=853-858&rft.date=2005&rft.aulast=Wierstra&rft.aufirst=Daan&rft.au=Schmidhuber%2C+J.&rft.au=Gomez%2C+F.+J.&rft_id=https%3A%2F%2Fwww.academia.edu%2F5830256&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-OpenAIfive-43"><span class="mw-cite-backlink">^ <a href="#cite_ref-OpenAIfive_43-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-OpenAIfive_43-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation news cs1" id="CITEREFRodriguez2018">Rodriguez, Jesus (July 2, 2018). <a class="external text" href="https://towardsdatascience.com/the-science-behind-openai-five-that-just-produced-one-of-the-greatest-breakthrough-in-the-history-b045bcdc2b69" rel="nofollow">"The Science Behind OpenAI Five that just Produced One of the Greatest Breakthrough in the History of AI"</a>. <i>Towards Data Science</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2019-01-15</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Towards+Data+Science&rft.atitle=The+Science+Behind+OpenAI+Five+that+just+Produced+One+of+the+Greatest+Breakthrough+in+the+History+of+AI&rft.date=2018-07-02&rft.aulast=Rodriguez&rft.aufirst=Jesus&rft_id=https%3A%2F%2Ftowardsdatascience.com%2Fthe-science-behind-openai-five-that-just-produced-one-of-the-greatest-breakthrough-in-the-history-b045bcdc2b69&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-OpenAIhand-44"><span class="mw-cite-backlink"><b><a href="#cite_ref-OpenAIhand_44-0">^</a></b></span> <span class="reference-text"><cite class="citation news cs1"><a class="external text" href="https://blog.openai.com/learning-dexterity/" rel="nofollow">"Learning Dexterity"</a>. <i>OpenAI Blog</i>. July 30, 2018<span class="reference-accessdate">. Retrieved <span class="nowrap">2019-01-15</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=OpenAI+Blog&rft.atitle=Learning+Dexterity&rft.date=2018-07-30&rft_id=https%3A%2F%2Fblog.openai.com%2Flearning-dexterity%2F&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-alphastar-45"><span class="mw-cite-backlink">^ <a href="#cite_ref-alphastar_45-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-alphastar_45-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation news cs1" id="CITEREFStanford2019">Stanford, Stacy (January 25, 2019). <a class="external text" href="https://medium.com/mlmemoirs/deepminds-ai-alphastar-showcases-significant-progress-towards-agi-93810c94fbe9" rel="nofollow">"DeepMind's AI, AlphaStar Showcases Significant Progress Towards AGI"</a>. <i>Medium ML Memoirs</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2019-01-15</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Medium+ML+Memoirs&rft.atitle=DeepMind%27s+AI%2C+AlphaStar+Showcases+Significant+Progress+Towards+AGI&rft.date=2019-01-25&rft.aulast=Stanford&rft.aufirst=Stacy&rft_id=https%3A%2F%2Fmedium.com%2Fmlmemoirs%2Fdeepminds-ai-alphastar-showcases-significant-progress-towards-agi-93810c94fbe9&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-46"><span class="mw-cite-backlink"><b><a href="#cite_ref-46">^</a></b></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFMayerGomezWierstraNagy2006">Mayer, H.; Gomez, F.; Wierstra, D.; Nagy, I.; Knoll, A.; Schmidhuber, J. (October 2006). <i>A System for Robotic Heart Surgery that Learns to Tie Knots Using Recurrent Neural Networks</i>. <i>2006 IEEE/RSJ International Conference on Intelligent Robots and Systems</i>. pp. 543–548. <a class="mw-redirect" href="/wiki/CiteSeerX_(identifier)" title="CiteSeerX (identifier)">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.218.3399" rel="nofollow">10.1.1.218.3399</a></span>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1109%2FIROS.2006.282190" rel="nofollow">10.1109/IROS.2006.282190</a>. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a> <a href="/wiki/Special:BookSources/978-1-4244-0258-8" title="Special:BookSources/978-1-4244-0258-8"><bdi>978-1-4244-0258-8</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&rft.genre=book&rft.btitle=A+System+for+Robotic+Heart+Surgery+that+Learns+to+Tie+Knots+Using+Recurrent+Neural+Networks&rft.pages=543-548&rft.date=2006-10&rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.218.3399&rft_id=info%3Adoi%2F10.1109%2FIROS.2006.282190&rft.isbn=978-1-4244-0258-8&rft.aulast=Mayer&rft.aufirst=H.&rft.au=Gomez%2C+F.&rft.au=Wierstra%2C+D.&rft.au=Nagy%2C+I.&rft.au=Knoll%2C+A.&rft.au=Schmidhuber%2C+J.&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-47"><span class="mw-cite-backlink"><b><a href="#cite_ref-47">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFGravesSchmidhuber2005">Graves, A.; Schmidhuber, J. (2005). "Framewise phoneme classification with bidirectional LSTM and other neural network architectures". <i>Neural Networks</i>. <b>18</b> (5–6): 602–610. <a class="mw-redirect" href="/wiki/CiteSeerX_(identifier)" title="CiteSeerX (identifier)">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.331.5800" rel="nofollow">10.1.1.331.5800</a></span>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1016%2Fj.neunet.2005.06.042" rel="nofollow">10.1016/j.neunet.2005.06.042</a>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/16112549" rel="nofollow">16112549</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Neural+Networks&rft.atitle=Framewise+phoneme+classification+with+bidirectional+LSTM+and+other+neural+network+architectures&rft.volume=18&rft.issue=5%E2%80%936&rft.pages=602-610&rft.date=2005&rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.331.5800&rft_id=info%3Apmid%2F16112549&rft_id=info%3Adoi%2F10.1016%2Fj.neunet.2005.06.042&rft.aulast=Graves&rft.aufirst=A.&rft.au=Schmidhuber%2C+J.&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-48"><span class="mw-cite-backlink"><b><a href="#cite_ref-48">^</a></b></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFFernándezGravesSchmidhuber2007">Fernández, Santiago; Graves, Alex; Schmidhuber, Jürgen (2007). <a class="external text" href="http://dl.acm.org/citation.cfm?id=1778066.1778092" rel="nofollow"><i>An Application of Recurrent Neural Networks to Discriminative Keyword Spotting</i></a>. <i>Proceedings of the 17th International Conference on Artificial Neural Networks</i>. ICANN'07. Berlin, Heidelberg: Springer-Verlag. pp. 220–229. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a> <a href="/wiki/Special:BookSources/978-3540746935" title="Special:BookSources/978-3540746935"><bdi>978-3540746935</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&rft.genre=book&rft.btitle=An+Application+of+Recurrent+Neural+Networks+to+Discriminative+Keyword+Spotting&rft.place=Berlin%2C+Heidelberg&rft.series=ICANN%2707&rft.pages=220-229&rft.pub=Springer-Verlag&rft.date=2007&rft.isbn=978-3540746935&rft.aulast=Fern%C3%A1ndez&rft.aufirst=Santiago&rft.au=Graves%2C+Alex&rft.au=Schmidhuber%2C+J%C3%BCrgen&rft_id=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D1778066.1778092&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-ReferenceA-49"><span class="mw-cite-backlink"><b><a href="#cite_ref-ReferenceA_49-0">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFGravesMohamedHinton2013">Graves, Alex; Mohamed, Abdel-rahman; Hinton, Geoffrey (2013). "Speech Recognition with Deep Recurrent Neural Networks". <i>Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</i>: 6645–6649. <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1303.5778" rel="nofollow">1303.5778</a></span>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1109%2FICASSP.2013.6638947" rel="nofollow">10.1109/ICASSP.2013.6638947</a>. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a> <a href="/wiki/Special:BookSources/978-1-4799-0356-6" title="Special:BookSources/978-1-4799-0356-6"><bdi>978-1-4799-0356-6</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Acoustics%2C+Speech+and+Signal+Processing+%28ICASSP%29%2C+2013+IEEE+International+Conference+on&rft.atitle=Speech+Recognition+with+Deep+Recurrent+Neural+Networks&rft.pages=6645-6649&rft.date=2013&rft_id=info%3Aarxiv%2F1303.5778&rft_id=info%3Adoi%2F10.1109%2FICASSP.2013.6638947&rft.isbn=978-1-4799-0356-6&rft.aulast=Graves&rft.aufirst=Alex&rft.au=Mohamed%2C+Abdel-rahman&rft.au=Hinton%2C+Geoffrey&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-50"><span class="mw-cite-backlink"><b><a href="#cite_ref-50">^</a></b></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFEckSchmidhuber2002">Eck, Douglas; Schmidhuber, Jürgen (2002-08-28). <i>Learning the Long-Term Structure of the Blues</i>. <i>Artificial Neural Networks — ICANN 2002</i>. Lecture Notes in Computer Science. <b>2415</b>. Springer, Berlin, Heidelberg. pp. 284–289. <a class="mw-redirect" href="/wiki/CiteSeerX_(identifier)" title="CiteSeerX (identifier)">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.116.3620" rel="nofollow">10.1.1.116.3620</a></span>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1007%2F3-540-46084-5_47" rel="nofollow">10.1007/3-540-46084-5_47</a>. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a> <a href="/wiki/Special:BookSources/978-3540460848" title="Special:BookSources/978-3540460848"><bdi>978-3540460848</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&rft.genre=book&rft.btitle=Learning+the+Long-Term+Structure+of+the+Blues&rft.series=Lecture+Notes+in+Computer+Science&rft.pages=284-289&rft.pub=Springer%2C+Berlin%2C+Heidelberg&rft.date=2002-08-28&rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.116.3620&rft_id=info%3Adoi%2F10.1007%2F3-540-46084-5_47&rft.isbn=978-3540460848&rft.aulast=Eck&rft.aufirst=Douglas&rft.au=Schmidhuber%2C+J%C3%BCrgen&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-51"><span class="mw-cite-backlink"><b><a href="#cite_ref-51">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFSchmidhuberGersEckSchmidhuber2002">Schmidhuber, J.; Gers, F.; Eck, D.; Schmidhuber, J.; Gers, F. (2002). "Learning nonregular languages: A comparison of simple recurrent networks and LSTM". <i>Neural Computation</i>. <b>14</b> (9): 2039–2041. <a class="mw-redirect" href="/wiki/CiteSeerX_(identifier)" title="CiteSeerX (identifier)">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.11.7369" rel="nofollow">10.1.1.11.7369</a></span>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1162%2F089976602320263980" rel="nofollow">10.1162/089976602320263980</a>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/12184841" rel="nofollow">12184841</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Neural+Computation&rft.atitle=Learning+nonregular+languages%3A+A+comparison+of+simple+recurrent+networks+and+LSTM&rft.volume=14&rft.issue=9&rft.pages=2039-2041&rft.date=2002&rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.11.7369&rft_id=info%3Apmid%2F12184841&rft_id=info%3Adoi%2F10.1162%2F089976602320263980&rft.aulast=Schmidhuber&rft.aufirst=J.&rft.au=Gers%2C+F.&rft.au=Eck%2C+D.&rft.au=Schmidhuber%2C+J.&rft.au=Gers%2C+F.&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-52"><span class="mw-cite-backlink"><b><a href="#cite_ref-52">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFPerez-OrtizGersEckSchmidhuber2003">Perez-Ortiz, J. A.; Gers, F. A.; Eck, D.; Schmidhuber, J. (2003). "Kalman filters improve LSTM network performance in problems unsolvable by traditional recurrent nets". <i>Neural Networks</i>. <b>16</b> (2): 241–250. <a class="mw-redirect" href="/wiki/CiteSeerX_(identifier)" title="CiteSeerX (identifier)">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.381.1992" rel="nofollow">10.1.1.381.1992</a></span>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1016%2Fs0893-6080%2802%2900219-8" rel="nofollow">10.1016/s0893-6080(02)00219-8</a>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/12628609" rel="nofollow">12628609</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Neural+Networks&rft.atitle=Kalman+filters+improve+LSTM+network+performance+in+problems+unsolvable+by+traditional+recurrent+nets&rft.volume=16&rft.issue=2&rft.pages=241-250&rft.date=2003&rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.381.1992&rft_id=info%3Apmid%2F12628609&rft_id=info%3Adoi%2F10.1016%2Fs0893-6080%2802%2900219-8&rft.aulast=Perez-Ortiz&rft.aufirst=J.+A.&rft.au=Gers%2C+F.+A.&rft.au=Eck%2C+D.&rft.au=Schmidhuber%2C+J.&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-53"><span class="mw-cite-backlink"><b><a href="#cite_ref-53">^</a></b></span> <span class="reference-text">A. Graves, J. Schmidhuber. Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks. Advances in Neural Information Processing Systems 22, NIPS'22, pp 545–552, Vancouver, MIT Press, 2009.</span>
</li>
<li id="cite_note-54"><span class="mw-cite-backlink"><b><a href="#cite_ref-54">^</a></b></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFGravesFernándezLiwickiBunke2007">Graves, Alex; Fernández, Santiago; Liwicki, Marcus; Bunke, Horst; Schmidhuber, Jürgen (2007). <a class="external text" href="http://dl.acm.org/citation.cfm?id=2981562.2981635" rel="nofollow"><i>Unconstrained Online Handwriting Recognition with Recurrent Neural Networks</i></a>. <i>Proceedings of the 20th International Conference on Neural Information Processing Systems</i>. NIPS'07. USA: Curran Associates Inc. pp. 577–584. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a> <a href="/wiki/Special:BookSources/9781605603520" title="Special:BookSources/9781605603520"><bdi>9781605603520</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&rft.genre=book&rft.btitle=Unconstrained+Online+Handwriting+Recognition+with+Recurrent+Neural+Networks&rft.place=USA&rft.series=NIPS%2707&rft.pages=577-584&rft.pub=Curran+Associates+Inc.&rft.date=2007&rft.isbn=9781605603520&rft.aulast=Graves&rft.aufirst=Alex&rft.au=Fern%C3%A1ndez%2C+Santiago&rft.au=Liwicki%2C+Marcus&rft.au=Bunke%2C+Horst&rft.au=Schmidhuber%2C+J%C3%BCrgen&rft_id=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D2981562.2981635&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-55"><span class="mw-cite-backlink"><b><a href="#cite_ref-55">^</a></b></span> <span class="reference-text">M. Baccouche, F. Mamalet, C Wolf, C. Garcia, A. Baskurt. Sequential Deep Learning for Human Action Recognition. 2nd International Workshop on Human Behavior Understanding (HBU), A.A. Salah, B. Lepri ed. Amsterdam, Netherlands. pp. 29–39. Lecture Notes in Computer Science 7065. Springer. 2011</span>
</li>
<li id="cite_note-56"><span class="mw-cite-backlink"><b><a href="#cite_ref-56">^</a></b></span> <span class="reference-text"><cite class="citation arxiv cs1" id="CITEREFHuangZhouZhangLi2018">Huang, Jie; Zhou, Wengang; Zhang, Qilin; Li, Houqiang; Li, Weiping (2018-01-30). "Video-based Sign Language Recognition without Temporal Segmentation". <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1801.10111" rel="nofollow">1801.10111</a></span> [<a class="external text" href="//arxiv.org/archive/cs.CV" rel="nofollow">cs.CV</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=preprint&rft.jtitle=arXiv&rft.atitle=Video-based+Sign+Language+Recognition+without+Temporal+Segmentation&rft.date=2018-01-30&rft_id=info%3Aarxiv%2F1801.10111&rft.aulast=Huang&rft.aufirst=Jie&rft.au=Zhou%2C+Wengang&rft.au=Zhang%2C+Qilin&rft.au=Li%2C+Houqiang&rft.au=Li%2C+Weiping&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-57"><span class="mw-cite-backlink"><b><a href="#cite_ref-57">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFHochreiterHeuselObermayer2007">Hochreiter, S.; Heusel, M.; Obermayer, K. (2007). <a class="external text" href="https://doi.org/10.1093/bioinformatics/btm247" rel="nofollow">"Fast model-based protein homology detection without alignment"</a>. <i>Bioinformatics</i>. <b>23</b> (14): 1728–1736. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="https://doi.org/10.1093%2Fbioinformatics%2Fbtm247" rel="nofollow">10.1093/bioinformatics/btm247</a></span>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/17488755" rel="nofollow">17488755</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Bioinformatics&rft.atitle=Fast+model-based+protein+homology+detection+without+alignment&rft.volume=23&rft.issue=14&rft.pages=1728-1736&rft.date=2007&rft_id=info%3Adoi%2F10.1093%2Fbioinformatics%2Fbtm247&rft_id=info%3Apmid%2F17488755&rft.aulast=Hochreiter&rft.aufirst=S.&rft.au=Heusel%2C+M.&rft.au=Obermayer%2C+K.&rft_id=%2F%2Fdoi.org%2F10.1093%2Fbioinformatics%2Fbtm247&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-58"><span class="mw-cite-backlink"><b><a href="#cite_ref-58">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFThireouReczko2007">Thireou, T.; Reczko, M. (2007). <a class="external text" href="https://www.semanticscholar.org/paper/2985b87fc977d8a893da6086b2a18298b9650d96" rel="nofollow">"Bidirectional Long Short-Term Memory Networks for predicting the subcellular localization of eukaryotic proteins"</a>. <i>IEEE/ACM Transactions on Computational Biology and Bioinformatics</i>. <b>4</b> (3): 441–446. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1109%2Ftcbb.2007.1015" rel="nofollow">10.1109/tcbb.2007.1015</a>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/17666763" rel="nofollow">17666763</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=IEEE%2FACM+Transactions+on+Computational+Biology+and+Bioinformatics&rft.atitle=Bidirectional+Long+Short-Term+Memory+Networks+for+predicting+the+subcellular+localization+of+eukaryotic+proteins&rft.volume=4&rft.issue=3&rft.pages=441-446&rft.date=2007&rft_id=info%3Adoi%2F10.1109%2Ftcbb.2007.1015&rft_id=info%3Apmid%2F17666763&rft.aulast=Thireou&rft.aufirst=T.&rft.au=Reczko%2C+M.&rft_id=https%3A%2F%2Fwww.semanticscholar.org%2Fpaper%2F2985b87fc977d8a893da6086b2a18298b9650d96&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-59"><span class="mw-cite-backlink"><b><a href="#cite_ref-59">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFMalhotraVigShroffAgarwal2015">Malhotra, Pankaj; Vig, Lovekesh; Shroff, Gautam; Agarwal, Puneet (April 2015). <a class="external text" href="https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2015-56.pdf" rel="nofollow">"Long Short Term Memory Networks for Anomaly Detection in Time Series"</a> <span class="cs1-format">(PDF)</span>. <i>European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning — ESANN 2015</i>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=European+Symposium+on+Artificial+Neural+Networks%2C+Computational+Intelligence+and+Machine+Learning+%E2%80%94+ESANN+2015&rft.atitle=Long+Short+Term+Memory+Networks+for+Anomaly+Detection+in+Time+Series&rft.date=2015-04&rft.aulast=Malhotra&rft.aufirst=Pankaj&rft.au=Vig%2C+Lovekesh&rft.au=Shroff%2C+Gautam&rft.au=Agarwal%2C+Puneet&rft_id=https%3A%2F%2Fwww.elen.ucl.ac.be%2FProceedings%2Fesann%2Fesannpdf%2Fes2015-56.pdf&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-60"><span class="mw-cite-backlink"><b><a href="#cite_ref-60">^</a></b></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFTaxVerenichLa_RosaDumas2017">Tax, N.; Verenich, I.; La Rosa, M.; Dumas, M. (2017). <i>Predictive Business Process Monitoring with LSTM neural networks</i>. <i>Proceedings of the International Conference on Advanced Information Systems Engineering (CAiSE)</i>. Lecture Notes in Computer Science. <b>10253</b>. pp. 477–492. <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1612.02130" rel="nofollow">1612.02130</a></span>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1007%2F978-3-319-59536-8_30" rel="nofollow">10.1007/978-3-319-59536-8_30</a>. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a> <a href="/wiki/Special:BookSources/978-3-319-59535-1" title="Special:BookSources/978-3-319-59535-1"><bdi>978-3-319-59535-1</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&rft.genre=book&rft.btitle=Predictive+Business+Process+Monitoring+with+LSTM+neural+networks&rft.series=Lecture+Notes+in+Computer+Science&rft.pages=477-492&rft.date=2017&rft_id=info%3Aarxiv%2F1612.02130&rft_id=info%3Adoi%2F10.1007%2F978-3-319-59536-8_30&rft.isbn=978-3-319-59535-1&rft.aulast=Tax&rft.aufirst=N.&rft.au=Verenich%2C+I.&rft.au=La+Rosa%2C+M.&rft.au=Dumas%2C+M.&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-61"><span class="mw-cite-backlink"><b><a href="#cite_ref-61">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFChoiBahadoriSchuetzStewart2016">Choi, E.; Bahadori, M.T.; Schuetz, E.; Stewart, W.; Sun, J. (2016). <a class="external text" href="http://proceedings.mlr.press/v56/Choi16.html" rel="nofollow">"Doctor AI: Predicting Clinical Events via Recurrent Neural Networks"</a>. <i>Proceedings of the 1st Machine Learning for Healthcare Conference</i>. <b>56</b>: 301–318. <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1511.05942" rel="nofollow">1511.05942</a></span>. <a class="mw-redirect" href="/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a class="external text" href="https://ui.adsabs.harvard.edu/abs/2015arXiv151105942C" rel="nofollow">2015arXiv151105942C</a>. <a class="mw-redirect" href="/wiki/PMC_(identifier)" title="PMC (identifier)">PMC</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC5341604" rel="nofollow">5341604</a></span>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/28286600" rel="nofollow">28286600</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Proceedings+of+the+1st+Machine+Learning+for+Healthcare+Conference&rft.atitle=Doctor+AI%3A+Predicting+Clinical+Events+via+Recurrent+Neural+Networks&rft.volume=56&rft.pages=301-318&rft.date=2016&rft_id=info%3Aarxiv%2F1511.05942&rft_id=info%3Apmid%2F28286600&rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC5341604&rft_id=info%3Abibcode%2F2015arXiv151105942C&rft.aulast=Choi&rft.aufirst=E.&rft.au=Bahadori%2C+M.T.&rft.au=Schuetz%2C+E.&rft.au=Stewart%2C+W.&rft.au=Sun%2C+J.&rft_id=http%3A%2F%2Fproceedings.mlr.press%2Fv56%2FChoi16.html&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-62"><span class="mw-cite-backlink"><b><a href="#cite_ref-62">^</a></b></span> <span class="reference-text">Jia, Robin; Liang, Percy (2016-06-11). <a class="extiw" href="https://arxiv.org/abs/1606.03622" title="arxiv:1606.03622">"Data Recombination for Neural Semantic Parsing"</a>. <i>arXiv:1606.03622 [cs]</i>.</span>
</li>
<li id="cite_note-Wang_Duan_Zhang_Niu_p=1657-63"><span class="mw-cite-backlink"><b><a href="#cite_ref-Wang_Duan_Zhang_Niu_p=1657_63-0">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFWangDuanZhangNiu2018">Wang, Le; Duan, Xuhuan; Zhang, Qilin; Niu, Zhenxing; Hua, Gang; Zheng, Nanning (2018-05-22). <a class="external text" href="https://qilin-zhang.github.io/_pages/pdfs/Segment-Tube_Spatio-Temporal_Action_Localization_in_Untrimmed_Videos_with_Per-Frame_Segmentation.pdf" rel="nofollow">"Segment-Tube: Spatio-Temporal Action Localization in Untrimmed Videos with Per-Frame Segmentation"</a> <span class="cs1-format">(PDF)</span>. <i>Sensors</i>. <b>18</b> (5): 1657. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.3390%2Fs18051657" rel="nofollow">10.3390/s18051657</a>. <a class="mw-redirect" href="/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a> <a class="external text" href="//www.worldcat.org/issn/1424-8220" rel="nofollow">1424-8220</a>. <a class="mw-redirect" href="/wiki/PMC_(identifier)" title="PMC (identifier)">PMC</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC5982167" rel="nofollow">5982167</a></span>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/29789447" rel="nofollow">29789447</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Sensors&rft.atitle=Segment-Tube%3A+Spatio-Temporal+Action+Localization+in+Untrimmed+Videos+with+Per-Frame+Segmentation&rft.volume=18&rft.issue=5&rft.pages=1657&rft.date=2018-05-22&rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC5982167&rft.issn=1424-8220&rft_id=info%3Apmid%2F29789447&rft_id=info%3Adoi%2F10.3390%2Fs18051657&rft.aulast=Wang&rft.aufirst=Le&rft.au=Duan%2C+Xuhuan&rft.au=Zhang%2C+Qilin&rft.au=Niu%2C+Zhenxing&rft.au=Hua%2C+Gang&rft.au=Zheng%2C+Nanning&rft_id=https%3A%2F%2Fqilin-zhang.github.io%2F_pages%2Fpdfs%2FSegment-Tube_Spatio-Temporal_Action_Localization_in_Untrimmed_Videos_with_Per-Frame_Segmentation.pdf&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-Duan_Wang_Zhai_Zheng_2018_p.-64"><span class="mw-cite-backlink"><b><a href="#cite_ref-Duan_Wang_Zhai_Zheng_2018_p._64-0">^</a></b></span> <span class="reference-text"><cite class="citation conference cs1" id="CITEREFDuanWangZhaiZheng2018">Duan, Xuhuan; Wang, Le; Zhai, Changbo; Zheng, Nanning; Zhang, Qilin; Niu, Zhenxing; Hua, Gang (2018). <i>Joint Spatio-Temporal Action Localization in Untrimmed Videos with Per-Frame Segmentation</i>. 25th IEEE International Conference on Image Processing (ICIP). <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1109%2Ficip.2018.8451692" rel="nofollow">10.1109/icip.2018.8451692</a>. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a> <a href="/wiki/Special:BookSources/978-1-4799-7061-2" title="Special:BookSources/978-1-4799-7061-2"><bdi>978-1-4799-7061-2</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&rft.genre=conference&rft.btitle=Joint+Spatio-Temporal+Action+Localization+in+Untrimmed+Videos+with+Per-Frame+Segmentation&rft.pub=25th+IEEE+International+Conference+on+Image+Processing+%28ICIP%29&rft.date=2018&rft_id=info%3Adoi%2F10.1109%2Ficip.2018.8451692&rft.isbn=978-1-4799-7061-2&rft.aulast=Duan&rft.aufirst=Xuhuan&rft.au=Wang%2C+Le&rft.au=Zhai%2C+Changbo&rft.au=Zheng%2C+Nanning&rft.au=Zhang%2C+Qilin&rft.au=Niu%2C+Zhenxing&rft.au=Hua%2C+Gang&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-65"><span class="mw-cite-backlink"><b><a href="#cite_ref-65">^</a></b></span> <span class="reference-text"><cite class="citation conference cs1" id="CITEREFOrsiniGastaldiMantecchiniRossi2019">Orsini, F.; Gastaldi, M.; Mantecchini, L.; Rossi, R. (2019). <i>Neural networks trained with WiFi traces to predict airport passenger behavior</i>. 6th International Conference on Models and Technologies for Intelligent Transportation Systems. Krakow: IEEE. <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1910.14026" rel="nofollow">1910.14026</a></span>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1109%2FMTITS.2019.8883365" rel="nofollow">10.1109/MTITS.2019.8883365</a>. 8883365.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&rft.genre=conference&rft.btitle=Neural+networks+trained+with+WiFi+traces+to+predict+airport+passenger+behavior&rft.place=Krakow&rft.pub=IEEE&rft.date=2019&rft_id=info%3Aarxiv%2F1910.14026&rft_id=info%3Adoi%2F10.1109%2FMTITS.2019.8883365&rft.aulast=Orsini&rft.aufirst=F.&rft.au=Gastaldi%2C+M.&rft.au=Mantecchini%2C+L.&rft.au=Rossi%2C+R.&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-66"><span class="mw-cite-backlink"><b><a href="#cite_ref-66">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFZhaoChenWuChen2017">Zhao, Z.; Chen, W.; Wu, X.; Chen, P.C.Y.; Liu, J. (2017). "LSTM network: A deep learning approach for Short-term traffic forecast". <i>IET Intelligent Transport Systems</i>. <b>11</b> (2): 68–75. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1049%2Fiet-its.2016.0208" rel="nofollow">10.1049/iet-its.2016.0208</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=IET+Intelligent+Transport+Systems&rft.atitle=LSTM+network%3A+A+deep+learning+approach+for+Short-term+traffic+forecast&rft.volume=11&rft.issue=2&rft.pages=68-75&rft.date=2017&rft_id=info%3Adoi%2F10.1049%2Fiet-its.2016.0208&rft.aulast=Zhao&rft.aufirst=Z.&rft.au=Chen%2C+W.&rft.au=Wu%2C+X.&rft.au=Chen%2C+P.C.Y.&rft.au=Liu%2C+J.&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
</ol></div></div>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Long_short-term_memory&action=edit&section=17" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a class="external text" href="http://www.idsia.ch/~juergen/rnn.html" rel="nofollow">Recurrent Neural Networks</a> with over 30 LSTM papers by <a href="/wiki/J%C3%BCrgen_Schmidhuber" title="Jürgen Schmidhuber">Jürgen Schmidhuber</a>'s group at <a class="mw-redirect" href="/wiki/IDSIA" title="IDSIA">IDSIA</a></li>
<li><cite class="citation web cs1" id="CITEREFGers2001">Gers, Felix (2001). <a class="external text" href="http://www.felixgers.de/papers/phd.pdf" rel="nofollow">"Long Short-Term Memory in Recurrent Neural Networks"</a> <span class="cs1-format">(PDF)</span>. <i>PhD thesis</i>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=unknown&rft.jtitle=PhD+thesis&rft.atitle=Long+Short-Term+Memory+in+Recurrent+Neural+Networks&rft.date=2001&rft.aulast=Gers&rft.aufirst=Felix&rft_id=http%3A%2F%2Fwww.felixgers.de%2Fpapers%2Fphd.pdf&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></li>
<li><cite class="citation journal cs1" id="CITEREFGersSchraudolphSchmidhuber2002">Gers, Felix A.; Schraudolph, Nicol N.; Schmidhuber, Jürgen (Aug 2002). <a class="external text" href="http://www.jmlr.org/papers/volume3/gers02a/gers02a.pdf" rel="nofollow">"Learning precise timing with LSTM recurrent networks"</a> <span class="cs1-format">(PDF)</span>. <i>Journal of Machine Learning Research</i>. <b>3</b>: 115–143.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Journal+of+Machine+Learning+Research&rft.atitle=Learning+precise+timing+with+LSTM+recurrent+networks&rft.volume=3&rft.pages=115-143&rft.date=2002-08&rft.aulast=Gers&rft.aufirst=Felix+A.&rft.au=Schraudolph%2C+Nicol+N.&rft.au=Schmidhuber%2C+J%C3%BCrgen&rft_id=http%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fvolume3%2Fgers02a%2Fgers02a.pdf&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></li>
<li><cite class="citation thesis cs1" id="CITEREFAbidogun2005">Abidogun, Olusola Adeniyi (2005). <a class="external text" href="http://etd.uwc.ac.za/xmlui/handle/11394/249" rel="nofollow"><i>Data Mining, Fraud Detection and Mobile Telecommunications: Call Pattern Analysis with Unsupervised Neural Networks</i></a>. <i>Master's Thesis</i> (Thesis). University of the Western Cape. <a class="mw-redirect" href="/wiki/Hdl_(identifier)" title="Hdl (identifier)">hdl</a>:<a class="external text" href="//hdl.handle.net/11394%2F249" rel="nofollow">11394/249</a>. <a class="external text" href="https://web.archive.org/web/20120522234026/http://etd.uwc.ac.za/usrfiles/modules/etd/docs/etd_init_3937_1174040706.pdf" rel="nofollow">Archived</a> <span class="cs1-format">(PDF)</span> from the original on May 22, 2012.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adissertation&rft.title=Data+Mining%2C+Fraud+Detection+and+Mobile+Telecommunications%3A+Call+Pattern+Analysis+with+Unsupervised+Neural+Networks&rft.inst=University+of+the+Western+Cape&rft.date=2005&rft_id=info%3Ahdl%2F11394%2F249&rft.aulast=Abidogun&rft.aufirst=Olusola+Adeniyi&rft_id=http%3A%2F%2Fetd.uwc.ac.za%2Fxmlui%2Fhandle%2F11394%2F249&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/>
<ul><li><a class="external text" href="http://etd.uwc.ac.za/bitstream/handle/11394/249/Abidogun_MSC_2005.pdf" rel="nofollow">original</a> with two chapters devoted to explaining recurrent neural networks, especially LSTM.</li></ul></li>
<li><cite class="citation journal cs1" id="CITEREFMonnerReggia2010">Monner, Derek D.; Reggia, James A. (2010). <a class="external text" href="http://www.cs.umd.edu/~dmonner/papers/nn2012.pdf" rel="nofollow">"A generalized LSTM-like training algorithm for second-order recurrent neural networks"</a> <span class="cs1-format">(PDF)</span>. <i>Neural Networks</i>. <b>25</b> (1): 70–83. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1016%2Fj.neunet.2011.07.003" rel="nofollow">10.1016/j.neunet.2011.07.003</a>. <a class="mw-redirect" href="/wiki/PMC_(identifier)" title="PMC (identifier)">PMC</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC3217173" rel="nofollow">3217173</a></span>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/21803542" rel="nofollow">21803542</a>. <q>High-performing extension of LSTM that has been simplified to a single node type and can train arbitrary architectures</q></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Neural+Networks&rft.atitle=A+generalized+LSTM-like+training+algorithm+for+second-order+recurrent+neural+networks&rft.volume=25&rft.issue=1&rft.pages=70-83&rft.date=2010&rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3217173&rft_id=info%3Apmid%2F21803542&rft_id=info%3Adoi%2F10.1016%2Fj.neunet.2011.07.003&rft.aulast=Monner&rft.aufirst=Derek+D.&rft.au=Reggia%2C+James+A.&rft_id=http%3A%2F%2Fwww.cs.umd.edu%2F~dmonner%2Fpapers%2Fnn2012.pdf&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></li>
<li><cite class="citation web cs1" id="CITEREFHerta">Herta, Christian. <a class="external text" href="http://christianherta.de/lehre/dataScience/machineLearning/neuralNetworks/LSTM.html" rel="nofollow">"How to implement LSTM in Python with Theano"</a>. <i>Tutorial</i>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=unknown&rft.jtitle=Tutorial&rft.atitle=How+to+implement+LSTM+in+Python+with+Theano&rft.aulast=Herta&rft.aufirst=Christian&rft_id=http%3A%2F%2Fchristianherta.de%2Flehre%2FdataScience%2FmachineLearning%2FneuralNetworks%2FLSTM.html&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALong+short-term+memory"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></li></ul>
<!-- 
NewPP limit report
Parsed by mw1358
Cached time: 20200820195624
Cache expiry: 2592000
Dynamic content: false
Complications: [vary‐revision‐sha1]
CPU time usage: 0.820 seconds
Real time usage: 1.766 seconds
Preprocessor visited node count: 4032/1000000
Post‐expand include size: 170989/2097152 bytes
Template argument size: 1737/2097152 bytes
Highest expansion depth: 12/40
Expensive parser function count: 1/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 278813/5000000 bytes
Lua time usage: 0.400/10.000 seconds
Lua memory usage: 5.01 MB/50 MB
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  696.416      1 -total
 67.94%  473.122      1 Template:Reflist
 34.75%  241.978     27 Template:Cite_journal
 10.39%   72.392      1 Template:Machine_learning_bar
 10.07%   70.109      1 Template:Citation_needed
  9.78%   68.099     16 Template:Cite_web
  9.62%   67.002      1 Template:Sidebar_with_collapsible_lists
  7.83%   54.527      1 Template:Fix
  6.76%   47.071      7 Template:Cite_book
  5.44%   37.857      2 Template:Category_handler
-->
<!-- Saved in parser cache with key enwiki:pcache:idhash:10711453-0!canonical!math=5 and timestamp 20200820195648 and revision id 974048165
 -->
</div><noscript><img alt="" height="1" src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" style="border: none; position: absolute;" title="" width="1"/></noscript>
<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Long_short-term_memory&oldid=974048165">https://en.wikipedia.org/w/index.php?title=Long_short-term_memory&oldid=974048165</a>"</div></div>
<div class="catlinks" data-mw="interface" id="catlinks"><div class="mw-normal-catlinks" id="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Artificial_neural_networks" title="Category:Artificial neural networks">Artificial neural networks</a></li></ul></div><div class="mw-hidden-catlinks mw-hidden-cats-hidden" id="mw-hidden-catlinks">Hidden categories: <ul><li><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_October_2017" title="Category:Articles with unsourced statements from October 2017">Articles with unsourced statements from October 2017</a></li></ul></div></div>
</div>
</div>
<div id="mw-data-after-content">
<div class="read-more-container"></div>
</div>
<div id="mw-navigation">
<h2>Navigation menu</h2>
<div id="mw-head">
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav aria-labelledby="p-personal-label" class="vector-menu" id="p-personal" role="navigation">
<h3 id="p-personal-label">
<span>Personal tools</span>
</h3>
<!-- Please do not use the .body class, it is deprecated. -->
<div class="body vector-menu-content">
<!-- Please do not use the .menu class, it is deprecated. -->
<ul class="vector-menu-content-list"><li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a accesskey="n" href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]">Talk</a></li><li id="pt-anoncontribs"><a accesskey="y" href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&returnto=Long+short-term+memory" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a accesskey="o" href="/w/index.php?title=Special:UserLogin&returnto=Long+short-term+memory" title="You're encouraged to log in; however, it's not mandatory. [o]">Log in</a></li></ul>
</div>
</nav>
<div id="left-navigation">
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav aria-labelledby="p-namespaces-label" class="vector-menu vector-menu-tabs vectorTabs" id="p-namespaces" role="navigation">
<h3 id="p-namespaces-label">
<span>Namespaces</span>
</h3>
<!-- Please do not use the .body class, it is deprecated. -->
<div class="body vector-menu-content">
<!-- Please do not use the .menu class, it is deprecated. -->
<ul class="vector-menu-content-list"><li class="selected" id="ca-nstab-main"><a accesskey="c" href="/wiki/Long_short-term_memory" title="View the content page [c]">Article</a></li><li id="ca-talk"><a accesskey="t" href="/wiki/Talk:Long_short-term_memory" rel="discussion" title="Discuss improvements to the content page [t]">Talk</a></li></ul>
</div>
</nav>
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav aria-labelledby="p-variants-label" class="vector-menu-empty emptyPortlet vector-menu vector-menu-dropdown vectorMenu" id="p-variants" role="navigation">
<input aria-labelledby="p-variants-label" class="vector-menu-checkbox vectorMenuCheckbox" type="checkbox"/>
<h3 id="p-variants-label">
<span>Variants</span>
</h3>
<!-- Please do not use the .body class, it is deprecated. -->
<div class="body vector-menu-content">
<!-- Please do not use the .menu class, it is deprecated. -->
<ul class="menu vector-menu-content-list"></ul>
</div>
</nav>
</div>
<div id="right-navigation">
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav aria-labelledby="p-views-label" class="vector-menu vector-menu-tabs vectorTabs" id="p-views" role="navigation">
<h3 id="p-views-label">
<span>Views</span>
</h3>
<!-- Please do not use the .body class, it is deprecated. -->
<div class="body vector-menu-content">
<!-- Please do not use the .menu class, it is deprecated. -->
<ul class="vector-menu-content-list"><li class="selected" id="ca-view"><a href="/wiki/Long_short-term_memory">Read</a></li><li id="ca-edit"><a accesskey="e" href="/w/index.php?title=Long_short-term_memory&action=edit" title="Edit this page [e]">Edit</a></li><li id="ca-history"><a accesskey="h" href="/w/index.php?title=Long_short-term_memory&action=history" title="Past revisions of this page [h]">View history</a></li></ul>
</div>
</nav>
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav aria-labelledby="p-cactions-label" class="vector-menu-empty emptyPortlet vector-menu vector-menu-dropdown vectorMenu" id="p-cactions" role="navigation">
<input aria-labelledby="p-cactions-label" class="vector-menu-checkbox vectorMenuCheckbox" type="checkbox"/>
<h3 id="p-cactions-label">
<span>More</span>
</h3>
<!-- Please do not use the .body class, it is deprecated. -->
<div class="body vector-menu-content">
<!-- Please do not use the .menu class, it is deprecated. -->
<ul class="menu vector-menu-content-list"></ul>
</div>
</nav>
<div id="p-search" role="search">
<h3>
<label for="searchInput">Search</label>
</h3>
<form action="/w/index.php" id="searchform">
<div id="simpleSearch">
<input accesskey="f" id="searchInput" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" type="search"/>
<input name="title" type="hidden" value="Special:Search"/>
<input class="searchButton mw-fallbackSearchButton" id="mw-searchButton" name="fulltext" title="Search Wikipedia for this text" type="submit" value="Search">
<input class="searchButton" id="searchButton" name="go" title="Go to a page with this exact name if it exists" type="submit" value="Go"/>
</input></div>
</form>
</div>
</div>
</div>
<div id="mw-panel">
<div id="p-logo" role="banner">
<a class="mw-wiki-logo" href="/wiki/Main_Page" title="Visit the main page"></a>
</div>
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav aria-labelledby="p-navigation-label" class="vector-menu vector-menu-portal portal portal-first" id="p-navigation" role="navigation">
<h3 id="p-navigation-label">
<span>Navigation</span>
</h3>
<!-- Please do not use the .body class, it is deprecated. -->
<div class="body vector-menu-content">
<!-- Please do not use the .menu class, it is deprecated. -->
<ul class="vector-menu-content-list"><li id="n-mainpage-description"><a accesskey="z" href="/wiki/Main_Page" title="Visit the main page [z]">Main page</a></li><li id="n-contents"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a accesskey="x" href="/wiki/Special:Random" title="Visit a randomly selected article [x]">Random article</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Learn about Wikipedia and how it works">About Wikipedia</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact us</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en" title="Support us by donating to the Wikimedia Foundation">Donate</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li></ul>
</div>
</nav>
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav aria-labelledby="p-interaction-label" class="vector-menu vector-menu-portal portal" id="p-interaction" role="navigation">
<h3 id="p-interaction-label">
<span>Contribute</span>
</h3>
<!-- Please do not use the .body class, it is deprecated. -->
<div class="body vector-menu-content">
<!-- Please do not use the .menu class, it is deprecated. -->
<ul class="vector-menu-content-list"><li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a accesskey="r" href="/wiki/Special:RecentChanges" title="A list of recent changes to Wikipedia [r]">Recent changes</a></li><li id="n-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Add images or other media for use on Wikipedia">Upload file</a></li></ul>
</div>
</nav>
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav aria-labelledby="p-tb-label" class="vector-menu vector-menu-portal portal" id="p-tb" role="navigation">
<h3 id="p-tb-label">
<span>Tools</span>
</h3>
<!-- Please do not use the .body class, it is deprecated. -->
<div class="body vector-menu-content">
<!-- Please do not use the .menu class, it is deprecated. -->
<ul class="vector-menu-content-list"><li id="t-whatlinkshere"><a accesskey="j" href="/wiki/Special:WhatLinksHere/Long_short-term_memory" title="List of all English Wikipedia pages containing links to this page [j]">What links here</a></li><li id="t-recentchangeslinked"><a accesskey="k" href="/wiki/Special:RecentChangesLinked/Long_short-term_memory" rel="nofollow" title="Recent changes in pages linked from this page [k]">Related changes</a></li><li id="t-upload"><a accesskey="u" href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]">Upload file</a></li><li id="t-specialpages"><a accesskey="q" href="/wiki/Special:SpecialPages" title="A list of all special pages [q]">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Long_short-term_memory&oldid=974048165" title="Permanent link to this revision of this page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Long_short-term_memory&action=info" title="More information about this page">Page information</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&page=Long_short-term_memory&id=974048165&wpFormIdentifier=titleform" title="Information on how to cite this page">Cite this page</a></li><li id="t-wikibase"><a accesskey="g" href="https://www.wikidata.org/wiki/Special:EntityPage/Q6673524" title="Structured data on this page hosted by Wikidata [g]">Wikidata item</a></li></ul>
</div>
</nav>
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav aria-labelledby="p-coll-print_export-label" class="vector-menu vector-menu-portal portal" id="p-coll-print_export" role="navigation">
<h3 id="p-coll-print_export-label">
<span>Print/export</span>
</h3>
<!-- Please do not use the .body class, it is deprecated. -->
<div class="body vector-menu-content">
<!-- Please do not use the .menu class, it is deprecated. -->
<ul class="vector-menu-content-list"><li id="coll-download-as-rl"><a href="/w/index.php?title=Special:DownloadAsPdf&page=Long_short-term_memory&action=show-download-screen" title="Download this page as a PDF file">Download as PDF</a></li><li id="t-print"><a accesskey="p" href="/w/index.php?title=Long_short-term_memory&printable=yes" title="Printable version of this page [p]">Printable version</a></li></ul>
</div>
</nav>
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav aria-labelledby="p-lang-label" class="vector-menu vector-menu-portal portal" id="p-lang" role="navigation">
<h3 id="p-lang-label">
<span>Languages</span>
</h3>
<!-- Please do not use the .body class, it is deprecated. -->
<div class="body vector-menu-content">
<!-- Please do not use the .menu class, it is deprecated. -->
<ul class="vector-menu-content-list"><li class="interlanguage-link interwiki-ca"><a class="interlanguage-link-target" href="https://ca.wikipedia.org/wiki/Long_short-term_memory" hreflang="ca" lang="ca" title="Long short-term memory – Catalan">Català</a></li><li class="interlanguage-link interwiki-de"><a class="interlanguage-link-target" href="https://de.wikipedia.org/wiki/Long_short-term_memory" hreflang="de" lang="de" title="Long short-term memory – German">Deutsch</a></li><li class="interlanguage-link interwiki-fa"><a class="interlanguage-link-target" href="https://fa.wikipedia.org/wiki/%D8%AD%D8%A7%D9%81%D8%B8%D9%87_%D8%B7%D9%88%D9%84%D8%A7%D9%86%DB%8C_%DA%A9%D9%88%D8%AA%D8%A7%D9%87-%D9%85%D8%AF%D8%AA" hreflang="fa" lang="fa" title="حافظه طولانی کوتاه-مدت – Persian">فارسی</a></li><li class="interlanguage-link interwiki-lv"><a class="interlanguage-link-target" href="https://lv.wikipedia.org/wiki/LSTM" hreflang="lv" lang="lv" title="LSTM – Latvian">Latviešu</a></li><li class="interlanguage-link interwiki-ja"><a class="interlanguage-link-target" href="https://ja.wikipedia.org/wiki/%E9%95%B7%E3%83%BB%E7%9F%AD%E6%9C%9F%E8%A8%98%E6%86%B6" hreflang="ja" lang="ja" title="長・短期記憶 – Japanese">日本語</a></li><li class="interlanguage-link interwiki-ru"><a class="interlanguage-link-target" href="https://ru.wikipedia.org/wiki/%D0%94%D0%BE%D0%BB%D0%B3%D0%B0%D1%8F_%D0%BA%D1%80%D0%B0%D1%82%D0%BA%D0%BE%D1%81%D1%80%D0%BE%D1%87%D0%BD%D0%B0%D1%8F_%D0%BF%D0%B0%D0%BC%D1%8F%D1%82%D1%8C" hreflang="ru" lang="ru" title="Долгая краткосрочная память – Russian">Русский</a></li><li class="interlanguage-link interwiki-tr"><a class="interlanguage-link-target" href="https://tr.wikipedia.org/wiki/Long_short-term_memory" hreflang="tr" lang="tr" title="Long short-term memory – Turkish">Türkçe</a></li><li class="interlanguage-link interwiki-uk"><a class="interlanguage-link-target" href="https://uk.wikipedia.org/wiki/%D0%94%D0%BE%D0%B2%D0%B3%D0%B0_%D0%BA%D0%BE%D1%80%D0%BE%D1%82%D0%BA%D0%BE%D1%87%D0%B0%D1%81%D0%BD%D0%B0_%D0%BF%D0%B0%D0%BC%27%D1%8F%D1%82%D1%8C" hreflang="uk" lang="uk" title="Довга короткочасна пам'ять – Ukrainian">Українська</a></li><li class="interlanguage-link interwiki-zh-yue"><a class="interlanguage-link-target" href="https://zh-yue.wikipedia.org/wiki/%E9%95%B7%E7%9F%AD%E6%9C%9F%E8%A8%98%E6%86%B6" hreflang="yue" lang="yue" title="長短期記憶 – Cantonese">粵語</a></li><li class="interlanguage-link interwiki-zh"><a class="interlanguage-link-target" href="https://zh.wikipedia.org/wiki/%E9%95%B7%E7%9F%AD%E6%9C%9F%E8%A8%98%E6%86%B6" hreflang="zh" lang="zh" title="長短期記憶 – Chinese">中文</a></li></ul>
<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a class="wbc-editpage" href="https://www.wikidata.org/wiki/Special:EntityPage/Q6673524#sitelinks-wikipedia" title="Edit interlanguage links">Edit links</a></span></div>
</div>
</nav>
</div>
</div>
<footer class="mw-footer" id="footer" role="contentinfo">
<ul id="footer-info">
<li id="footer-info-lastmod"> This page was last edited on 20 August 2020, at 19:56<span class="anonymous-show"> (UTC)</span>.</li>
<li id="footer-info-copyright">Text is available under the <a href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License" rel="license">Creative Commons Attribution-ShareAlike License</a><a href="//creativecommons.org/licenses/by-sa/3.0/" rel="license" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
</ul>
<ul id="footer-places">
<li id="footer-places-privacy"><a class="extiw" href="https://foundation.wikimedia.org/wiki/Privacy_policy" title="wmf:Privacy policy">Privacy policy</a></li>
<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
<li id="footer-places-mobileview"><a class="noprint stopMobileRedirectToggle" href="//en.m.wikipedia.org/w/index.php?title=Long_short-term_memory&mobileaction=toggle_view_mobile">Mobile view</a></li>
<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>
<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
</ul>
<ul class="noprint" id="footer-icons">
<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img alt="Wikimedia Foundation" height="31" loading="lazy" src="/static/images/footer/wikimedia-button.png" srcset="/static/images/footer/wikimedia-button-1.5x.png 1.5x, /static/images/footer/wikimedia-button-2x.png 2x" width="88"/></a></li>
<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img alt="Powered by MediaWiki" height="31" loading="lazy" src="/static/images/footer/poweredby_mediawiki_88x31.png" srcset="/static/images/footer/poweredby_mediawiki_132x47.png 1.5x, /static/images/footer/poweredby_mediawiki_176x62.png 2x" width="88"/></a></li>
</ul>
<div style="clear: both;"></div>
</footer>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.820","walltime":"1.766","ppvisitednodes":{"value":4032,"limit":1000000},"postexpandincludesize":{"value":170989,"limit":2097152},"templateargumentsize":{"value":1737,"limit":2097152},"expansiondepth":{"value":12,"limit":40},"expensivefunctioncount":{"value":1,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":278813,"limit":5000000},"entityaccesscount":{"value":0,"limit":400},"timingprofile":["100.00%  696.416      1 -total"," 67.94%  473.122      1 Template:Reflist"," 34.75%  241.978     27 Template:Cite_journal"," 10.39%   72.392      1 Template:Machine_learning_bar"," 10.07%   70.109      1 Template:Citation_needed","  9.78%   68.099     16 Template:Cite_web","  9.62%   67.002      1 Template:Sidebar_with_collapsible_lists","  7.83%   54.527      1 Template:Fix","  6.76%   47.071      7 Template:Cite_book","  5.44%   37.857      2 Template:Category_handler"]},"scribunto":{"limitreport-timeusage":{"value":"0.400","limit":"10.000"},"limitreport-memusage":{"value":5255694,"limit":52428800}},"cachereport":{"origin":"mw1358","timestamp":"20200820195624","ttl":2592000,"transientcontent":false}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"Long short-term memory","url":"https:\/\/en.wikipedia.org\/wiki\/Long_short-term_memory","sameAs":"http:\/\/www.wikidata.org\/entity\/Q6673524","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q6673524","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2007-04-16T20:18:38Z","dateModified":"2020-08-20T19:56:48Z","image":"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/3\/3b\/The_LSTM_cell.png","headline":"recurrent neural network architecture"}</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":112,"wgHostname":"mw1397"});});</script>
</body></html>