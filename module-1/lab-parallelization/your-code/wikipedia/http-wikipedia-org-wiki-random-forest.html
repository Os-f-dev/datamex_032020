<!DOCTYPE html>

<html class="client-nojs" dir="ltr" lang="en">
<head>
<meta charset="utf-8"/>
<title>Random forest - Wikipedia</title>
<script>document.documentElement.className="client-js";RLCONF={"wgBreakFrames":!1,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"fc07ce07-cc92-452d-a4f1-694494081767","wgCSPNonce":!1,"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":!1,"wgNamespaceNumber":0,"wgPageName":"Random_forest","wgTitle":"Random forest","wgCurRevisionId":972396951,"wgRevisionId":972396951,"wgArticleId":1363880,"wgIsArticle":!0,"wgIsRedirect":!1,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["CS1 maint: uses authors parameter","CS1 errors: missing periodical","All articles with dead external links","Articles with dead external links from May 2017","Articles with permanently dead external links","Articles with short description","Short description is different from Wikidata",
"Articles containing potentially dated statements from 2019","All articles containing potentially dated statements","Classification algorithms","Ensemble learning","Decision trees","Decision theory","Computational statistics","Machine learning"],"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"Random_forest","wgRelevantArticleId":1363880,"wgIsProbablyEditable":!0,"wgRelevantPageIsProbablyEditable":!0,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgMediaViewerOnClick":!0,"wgMediaViewerEnabledByDefault":!0,"wgPopupsReferencePreviews":!1,"wgPopupsConflictsWithNavPopupGadget":!1,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":!0,"nearby":!0,"watchlist":!0,"tagline":!1},"wgWMESchemaEditAttemptStepOversample":!1,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralAuthMobileDomain":!1,"wgEditSubmitButtonLabelPublish":
!0,"wgULSPosition":"interlanguage","wgWikibaseItemId":"Q245748"};RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"loading","ext.cite.styles":"ready","ext.math.styles":"ready","skins.vector.styles.legacy":"ready","mediawiki.toc.styles":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","wikibase.client.init":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements","ext.math.scripts","site","mediawiki.page.startup","mediawiki.page.ready","mediawiki.toc","skins.vector.legacy.js","ext.gadget.ReferenceTooltips","ext.gadget.charinsert","ext.gadget.extra-toolbar-buttons","ext.gadget.refToolbar","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging",
"ext.wikimediaEvents","ext.navigationTiming","ext.uls.compactlinks","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.options@1hzgi",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});});</script>
<link href="/w/load.php?lang=en&modules=ext.cite.styles%7Cext.math.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.toc.styles%7Cskins.vector.styles.legacy%7Cwikibase.client.init&only=styles&skin=vector" rel="stylesheet"/>
<script async="" src="/w/load.php?lang=en&modules=startup&only=scripts&raw=1&skin=vector"></script>
<meta content="" name="ResourceLoaderDynamicStyles"/>
<link href="/w/load.php?lang=en&modules=site.styles&only=styles&skin=vector" rel="stylesheet"/>
<meta content="MediaWiki 1.36.0-wmf.4" name="generator"/>
<meta content="origin" name="referrer"/>
<meta content="origin-when-crossorigin" name="referrer"/>
<meta content="origin-when-cross-origin" name="referrer"/>
<meta content="https://upload.wikimedia.org/wikipedia/commons/7/76/Random_forest_diagram_complete.png" property="og:image"/>
<link href="//en.m.wikipedia.org/wiki/Random_forest" media="only screen and (max-width: 720px)" rel="alternate"/>
<link href="/w/index.php?title=Random_forest&action=edit" rel="alternate" title="Edit this page" type="application/x-wiki"/>
<link href="/w/index.php?title=Random_forest&action=edit" rel="edit" title="Edit this page"/>
<link href="/static/apple-touch/wikipedia.png" rel="apple-touch-icon"/>
<link href="/static/favicon/wikipedia.ico" rel="shortcut icon"/>
<link href="/w/opensearch_desc.php" rel="search" title="Wikipedia (en)" type="application/opensearchdescription+xml"/>
<link href="//en.wikipedia.org/w/api.php?action=rsd" rel="EditURI" type="application/rsd+xml"/>
<link href="//creativecommons.org/licenses/by-sa/3.0/" rel="license"/>
<link href="https://en.wikipedia.org/wiki/Random_forest" rel="canonical"/>
<link href="//login.wikimedia.org" rel="dns-prefetch"/>
<link href="//meta.wikimedia.org" rel="dns-prefetch"/>
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Random_forest rootpage-Random_forest skin-vector action-view skin-vector-legacy"><div class="noprint" id="mw-page-base"></div>
<div class="noprint" id="mw-head-base"></div>
<div class="mw-body" id="content" role="main">
<a id="top"></a>
<div class="mw-body-content" id="siteNotice"><!-- CentralNotice --></div>
<div class="mw-indicators mw-body-content">
</div>
<h1 class="firstHeading" id="firstHeading" lang="en">Random forest</h1>
<div class="mw-body-content" id="bodyContent">
<div class="noprint" id="siteSub">From Wikipedia, the free encyclopedia</div>
<div id="contentSub"></div>
<div id="contentSub2"></div>
<div id="jump-to-nav"></div>
<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
<a class="mw-jump-link" href="#searchInput">Jump to search</a>
<div class="mw-content-ltr" dir="ltr" id="mw-content-text" lang="en"><div class="mw-parser-output"><div class="hatnote navigation-not-searchable" role="note">This article is about the machine learning technique. For other kinds of random tree, see <a href="/wiki/Random_tree" title="Random tree">Random tree</a>.</div>
<div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">An ensemble machine learning method</div>
<table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:22.0em;margin:0 0 1.0em 1.0em;background:#f8f9fa;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%"><tbody><tr><td style="padding-top:0.4em;line-height:1.2em">Part of a series on</td></tr><tr><th style="padding:0.2em 0.4em 0.2em;padding-top:0;font-size:145%;line-height:1.2em"><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a><br/>and<br/><a href="/wiki/Data_mining" title="Data mining">data mining</a></th></tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Problems</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>
<li><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>
<li><a href="/wiki/Automated_machine_learning" title="Automated machine learning">AutoML</a></li>
<li><a href="/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>
<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>
<li><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>
<li><a href="/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></li>
<li><a href="/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>
<li><a href="/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li>
<li><a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">Semi-supervised learning</a></li>
<li><a href="/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li>
<li><a href="/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></li>
<li><a href="/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><div style="display:inline-block; padding:0.1em 0;line-height:1.2em;"><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br/><style data-mw-deduplicate="TemplateStyles:r886047488">.mw-parser-output .nobold{font-weight:normal}</style><span class="nobold"><span style="font-size:85%;">(<b><a href="/wiki/Statistical_classification" title="Statistical classification">classification</a></b> • <b><a href="/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</span></span> </div></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>
<li><a href="/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a>
<ul><li><a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a></li>
<li><a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a></li>
<li><a class="mw-selflink selflink">Random forest</a></li></ul></li>
<li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>
<li><a href="/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>
<li><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>
<li><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural networks</a></li>
<li><a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>
<li><a href="/wiki/Perceptron" title="Perceptron">Perceptron</a></li>
<li><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li>
<li><a class="mw-redirect" href="/wiki/Support-vector_machine" title="Support-vector machine">Support vector machine (SVM)</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/BIRCH" title="BIRCH">BIRCH</a></li>
<li><a class="mw-redirect" href="/wiki/CURE_data_clustering_algorithm" title="CURE data clustering algorithm">CURE</a></li>
<li><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li>
<li><a href="/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means</a></li>
<li><a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation–maximization algorithm">Expectation–maximization (EM)</a></li>
<li><br/><a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>
<li><a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>
<li><a class="mw-redirect" href="/wiki/Mean-shift" title="Mean-shift">Mean-shift</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>
<li><a href="/wiki/Canonical_correlation" title="Canonical correlation">CCA</a></li>
<li><a href="/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>
<li><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>
<li><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>
<li><a href="/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>
<li><a href="/wiki/Proper_generalized_decomposition" title="Proper generalized decomposition">PGD</a></li>
<li><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Graphical_model" title="Graphical model">Graphical models</a>
<ul><li><a href="/wiki/Bayesian_network" title="Bayesian network">Bayes net</a></li>
<li><a href="/wiki/Conditional_random_field" title="Conditional random field">Conditional random field</a></li>
<li><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov</a></li></ul></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a class="mw-redirect" href="/wiki/K-nearest_neighbors_classification" title="K-nearest neighbors classification"><i>k</i>-NN</a></li>
<li><a href="/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural network</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="/wiki/DeepDream" title="DeepDream">DeepDream</a></li>
<li><a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron</a></li>
<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">RNN</a>
<ul><li><a href="/wiki/Long_short-term_memory" title="Long short-term memory">LSTM</a></li>
<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">GRU</a></li></ul></li>
<li><a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted Boltzmann machine</a></li>
<li><a href="/wiki/Generative_adversarial_network" title="Generative adversarial network">GAN</a></li>
<li><a href="/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>
<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a>
<ul><li><a href="/wiki/U-Net" title="U-Net">U-Net</a></li></ul></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>
<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">SARSA</a></li>
<li><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference (TD)</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Theory</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a class="mw-redirect" href="/wiki/Bias%E2%80%93variance_dilemma" title="Bias–variance dilemma">Bias–variance dilemma</a></li>
<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>
<li><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>
<li><a href="/wiki/Occam_learning" title="Occam learning">Occam learning</a></li>
<li><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>
<li><a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>
<li><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik–Chervonenkis theory">VC theory</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Machine-learning venues</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NeurIPS</a></li>
<li><a href="/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a></li>
<li><a href="/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">ML</a></li>
<li><a href="/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">JMLR</a></li>
<li><a class="external text" href="https://arxiv.org/list/cs.LG/recent" rel="nofollow">ArXiv:cs.LG</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Related articles</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/List_of_datasets_for_machine-learning_research" title="List of datasets for machine-learning research">List of datasets for machine-learning research</a></li>
<li><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">Outline of machine learning</a></li></ul>
</div></div></div></td>
</tr><tr><td style="text-align:right;font-size:115%;padding-top: 0.6em;"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Machine_learning_bar" title="Template:Machine learning bar"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Machine_learning_bar" title="Template talk:Machine learning bar"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&action=edit"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>
<div class="thumb tright"><div class="thumbinner" style="width:222px;"><a class="image" href="/wiki/File:Random_forest_diagram_complete.png"><img alt="" class="thumbimage" data-file-height="444" data-file-width="592" decoding="async" height="165" src="//upload.wikimedia.org/wikipedia/commons/thumb/7/76/Random_forest_diagram_complete.png/220px-Random_forest_diagram_complete.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/7/76/Random_forest_diagram_complete.png/330px-Random_forest_diagram_complete.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/7/76/Random_forest_diagram_complete.png/440px-Random_forest_diagram_complete.png 2x" width="220"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:Random_forest_diagram_complete.png" title="Enlarge"></a></div>Diagram of a random decision forest</div></div></div>
<p><b>Random forests</b> or <b>random decision forests</b> are an <a href="/wiki/Ensemble_learning" title="Ensemble learning">ensemble learning</a> method for <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a>, <a href="/wiki/Regression_analysis" title="Regression analysis">regression</a> and other tasks that operate by constructing a multitude of <a href="/wiki/Decision_tree_learning" title="Decision tree learning">decision trees</a> at training time and outputting the class that is the <a href="/wiki/Mode_(statistics)" title="Mode (statistics)">mode</a> of the classes (classification) or mean prediction (regression) of the individual trees.<sup class="reference" id="cite_ref-ho1995_1-0"><a href="#cite_note-ho1995-1">[1]</a></sup><sup class="reference" id="cite_ref-ho1998_2-0"><a href="#cite_note-ho1998-2">[2]</a></sup> Random decision forests correct for decision trees' habit of <a href="/wiki/Overfitting" title="Overfitting">overfitting</a> to their <a class="mw-redirect" href="/wiki/Test_set" title="Test set">training set</a>.<sup class="reference" id="cite_ref-elemstatlearn_3-0"><a href="#cite_note-elemstatlearn-3">[3]</a></sup><sup class="reference" style="white-space:nowrap;">:<span>587–588</span></sup>
</p><p>The first algorithm for random decision forests was created by <a href="/wiki/Tin_Kam_Ho" title="Tin Kam Ho">Tin Kam Ho</a><sup class="reference" id="cite_ref-ho1995_1-1"><a href="#cite_note-ho1995-1">[1]</a></sup> using the <a href="/wiki/Random_subspace_method" title="Random subspace method">random subspace method</a>,<sup class="reference" id="cite_ref-ho1998_2-1"><a href="#cite_note-ho1998-2">[2]</a></sup> which, in Ho's formulation, is a way to implement the "stochastic discrimination" approach to classification proposed by Eugene Kleinberg.<sup class="reference" id="cite_ref-kleinberg1990_4-0"><a href="#cite_note-kleinberg1990-4">[4]</a></sup><sup class="reference" id="cite_ref-kleinberg1996_5-0"><a href="#cite_note-kleinberg1996-5">[5]</a></sup><sup class="reference" id="cite_ref-kleinberg2000_6-0"><a href="#cite_note-kleinberg2000-6">[6]</a></sup>
</p><p>An extension of the algorithm was developed by <a href="/wiki/Leo_Breiman" title="Leo Breiman">Leo Breiman</a><sup class="reference" id="cite_ref-breiman2001_7-0"><a href="#cite_note-breiman2001-7">[7]</a></sup> and <a href="/wiki/Adele_Cutler" title="Adele Cutler">Adele Cutler</a>,<sup class="reference" id="cite_ref-rpackage_8-0"><a href="#cite_note-rpackage-8">[8]</a></sup> who registered<sup class="reference" id="cite_ref-9"><a href="#cite_note-9">[9]</a></sup> "Random Forests" as a <a href="/wiki/Trademark" title="Trademark">trademark</a> (as of 2019<sup class="plainlinks noexcerpt noprint asof-tag update" style="display:none;"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Random_forest&action=edit">[update]</a></sup>, owned by <a href="/wiki/Minitab" title="Minitab">Minitab, Inc.</a>).<sup class="reference" id="cite_ref-10"><a href="#cite_note-10">[10]</a></sup> The extension combines Breiman's "<a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">bagging</a>" idea and random selection of features, introduced first by Ho<sup class="reference" id="cite_ref-ho1995_1-2"><a href="#cite_note-ho1995-1">[1]</a></sup> and later independently by Amit and <a href="/wiki/Donald_Geman" title="Donald Geman">Geman</a><sup class="reference" id="cite_ref-amitgeman1997_11-0"><a href="#cite_note-amitgeman1997-11">[11]</a></sup> in order to construct a collection of decision trees with controlled variance.
</p>
<div aria-labelledby="mw-toc-heading" class="toc" id="toc" role="navigation"><input class="toctogglecheckbox" id="toctogglecheckbox" role="button" style="display:none" type="checkbox"/><div class="toctitle" dir="ltr" lang="en"><h2 id="mw-toc-heading">Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#History"><span class="tocnumber">1</span> <span class="toctext">History</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Algorithm"><span class="tocnumber">2</span> <span class="toctext">Algorithm</span></a>
<ul>
<li class="toclevel-2 tocsection-3"><a href="#Preliminaries:_decision_tree_learning"><span class="tocnumber">2.1</span> <span class="toctext">Preliminaries: decision tree learning</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="#Bagging"><span class="tocnumber">2.2</span> <span class="toctext">Bagging</span></a></li>
<li class="toclevel-2 tocsection-5"><a href="#From_bagging_to_random_forests"><span class="tocnumber">2.3</span> <span class="toctext">From bagging to random forests</span></a></li>
<li class="toclevel-2 tocsection-6"><a href="#ExtraTrees"><span class="tocnumber">2.4</span> <span class="toctext">ExtraTrees</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-7"><a href="#Properties"><span class="tocnumber">3</span> <span class="toctext">Properties</span></a>
<ul>
<li class="toclevel-2 tocsection-8"><a href="#Variable_importance"><span class="tocnumber">3.1</span> <span class="toctext">Variable importance</span></a></li>
<li class="toclevel-2 tocsection-9"><a href="#Relationship_to_nearest_neighbors"><span class="tocnumber">3.2</span> <span class="toctext">Relationship to nearest neighbors</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-10"><a href="#Unsupervised_learning_with_random_forests"><span class="tocnumber">4</span> <span class="toctext">Unsupervised learning with random forests</span></a></li>
<li class="toclevel-1 tocsection-11"><a href="#Variants"><span class="tocnumber">5</span> <span class="toctext">Variants</span></a></li>
<li class="toclevel-1 tocsection-12"><a href="#Kernel_random_forest"><span class="tocnumber">6</span> <span class="toctext">Kernel random forest</span></a>
<ul>
<li class="toclevel-2 tocsection-13"><a href="#History_2"><span class="tocnumber">6.1</span> <span class="toctext">History</span></a></li>
<li class="toclevel-2 tocsection-14"><a href="#Notations_and_definitions"><span class="tocnumber">6.2</span> <span class="toctext">Notations and definitions</span></a>
<ul>
<li class="toclevel-3 tocsection-15"><a href="#Preliminaries:_Centered_forests"><span class="tocnumber">6.2.1</span> <span class="toctext">Preliminaries: Centered forests</span></a></li>
<li class="toclevel-3 tocsection-16"><a href="#Uniform_forest"><span class="tocnumber">6.2.2</span> <span class="toctext">Uniform forest</span></a></li>
<li class="toclevel-3 tocsection-17"><a href="#From_random_forest_to_KeRF"><span class="tocnumber">6.2.3</span> <span class="toctext">From random forest to KeRF</span></a></li>
<li class="toclevel-3 tocsection-18"><a href="#Centered_KeRF"><span class="tocnumber">6.2.4</span> <span class="toctext">Centered KeRF</span></a></li>
<li class="toclevel-3 tocsection-19"><a href="#Uniform_KeRF"><span class="tocnumber">6.2.5</span> <span class="toctext">Uniform KeRF</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-20"><a href="#Properties_2"><span class="tocnumber">6.3</span> <span class="toctext">Properties</span></a>
<ul>
<li class="toclevel-3 tocsection-21"><a href="#Relation_between_KeRF_and_random_forest"><span class="tocnumber">6.3.1</span> <span class="toctext">Relation between KeRF and random forest</span></a></li>
<li class="toclevel-3 tocsection-22"><a href="#Relation_between_infinite_KeRF_and_infinite_random_forest"><span class="tocnumber">6.3.2</span> <span class="toctext">Relation between infinite KeRF and infinite random forest</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-23"><a href="#Consistency_results"><span class="tocnumber">6.4</span> <span class="toctext">Consistency results</span></a>
<ul>
<li class="toclevel-3 tocsection-24"><a href="#Consistency_of_centered_KeRF"><span class="tocnumber">6.4.1</span> <span class="toctext">Consistency of centered KeRF</span></a></li>
<li class="toclevel-3 tocsection-25"><a href="#Consistency_of_uniform_KeRF"><span class="tocnumber">6.4.2</span> <span class="toctext">Consistency of uniform KeRF</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-26"><a href="#See_also"><span class="tocnumber">7</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-27"><a href="#References"><span class="tocnumber">8</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1 tocsection-28"><a href="#Further_reading"><span class="tocnumber">9</span> <span class="toctext">Further reading</span></a></li>
<li class="toclevel-1 tocsection-29"><a href="#External_links"><span class="tocnumber">10</span> <span class="toctext">External links</span></a></li>
</ul>
</div>
<h2><span class="mw-headline" id="History">History</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Random_forest&action=edit&section=1" title="Edit section: History">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The general method of random decision forests was first proposed by Ho in 1995.<sup class="reference" id="cite_ref-ho1995_1-3"><a href="#cite_note-ho1995-1">[1]</a></sup> Ho established that forests of trees splitting with oblique hyperplanes can gain accuracy as they grow without suffering from overtraining, as long as the forests are randomly restricted to be sensitive to only selected <a href="/wiki/Feature_(machine_learning)" title="Feature (machine learning)">feature</a> dimensions.  A subsequent work along the same lines<sup class="reference" id="cite_ref-ho1998_2-2"><a href="#cite_note-ho1998-2">[2]</a></sup> concluded that other splitting methods behave similarly, as long as they are randomly forced to be insensitive to some feature dimensions.  Note that this observation of a more complex classifier (a larger forest) getting more accurate nearly monotonically is in sharp contrast to the common belief that the complexity of a classifier can only grow to a certain level of accuracy before being hurt by overfitting.  The explanation of the forest method's resistance to overtraining can be found in Kleinberg's theory of stochastic discrimination.<sup class="reference" id="cite_ref-kleinberg1990_4-1"><a href="#cite_note-kleinberg1990-4">[4]</a></sup><sup class="reference" id="cite_ref-kleinberg1996_5-1"><a href="#cite_note-kleinberg1996-5">[5]</a></sup><sup class="reference" id="cite_ref-kleinberg2000_6-1"><a href="#cite_note-kleinberg2000-6">[6]</a></sup>
</p><p>The early development of Breiman's notion of random forests was influenced by the work of Amit and
Geman<sup class="reference" id="cite_ref-amitgeman1997_11-1"><a href="#cite_note-amitgeman1997-11">[11]</a></sup> who introduced the idea of searching over a random subset of the
available decisions when splitting a node, in the context of growing a single
<a href="/wiki/Decision_tree" title="Decision tree">tree</a>.  The idea of random subspace selection from Ho<sup class="reference" id="cite_ref-ho1998_2-3"><a href="#cite_note-ho1998-2">[2]</a></sup> was also influential in the design of random forests.  In this method a forest of trees is grown,
and variation among the trees is introduced by projecting the training data
into a randomly chosen <a href="/wiki/Linear_subspace" title="Linear subspace">subspace</a> before fitting each tree or each node.  Finally, the idea of
randomized node optimization, where the decision at each node is selected by a
randomized procedure, rather than a deterministic optimization was first
introduced by Dietterich.<sup class="reference" id="cite_ref-12"><a href="#cite_note-12">[12]</a></sup>
</p><p>The introduction of random forests proper was first made in a paper
by <a href="/wiki/Leo_Breiman" title="Leo Breiman">Leo Breiman</a>.<sup class="reference" id="cite_ref-breiman2001_7-1"><a href="#cite_note-breiman2001-7">[7]</a></sup>  This paper describes a method of building a forest of
uncorrelated trees using a <a class="mw-redirect" href="/wiki/Classification_and_regression_tree" title="Classification and regression tree">CART</a> like procedure, combined with randomized node
optimization and <a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">bagging</a>.  In addition, this paper combines several
ingredients, some previously known and some novel, which form the basis of the
modern practice of random forests, in particular:
</p>
<ol><li>Using <a href="/wiki/Out-of-bag_error" title="Out-of-bag error">out-of-bag error</a> as an estimate of the <a href="/wiki/Generalization_error" title="Generalization error">generalization error</a>.</li>
<li>Measuring variable importance through permutation.</li></ol>
<p>The report also offers the first theoretical result for random forests in the
form of a bound on the <a href="/wiki/Generalization_error" title="Generalization error">generalization error</a> which depends on the strength of the
trees in the forest and their <a class="mw-redirect" href="/wiki/Correlation" title="Correlation">correlation</a>.
</p>
<h2><span class="mw-headline" id="Algorithm">Algorithm</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Random_forest&action=edit&section=2" title="Edit section: Algorithm">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Preliminaries:_decision_tree_learning">Preliminaries: decision tree learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Random_forest&action=edit&section=3" title="Edit section: Preliminaries: decision tree learning">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="hatnote navigation-not-searchable" role="note">Main article: <a href="/wiki/Decision_tree_learning" title="Decision tree learning">Decision tree learning</a></div>
<p>Decision trees are a popular method for various machine learning tasks. Tree learning "come[s] closest to meeting the requirements for serving as an off-the-shelf procedure for data mining", say <a href="/wiki/Trevor_Hastie" title="Trevor Hastie">Hastie</a> <i>et al.</i>, "because it is invariant under scaling and various other transformations of feature values, is robust to inclusion of irrelevant features, and produces inspectable models. However, they are seldom accurate".<sup class="reference" id="cite_ref-elemstatlearn_3-1"><a href="#cite_note-elemstatlearn-3">[3]</a></sup><sup class="reference" style="white-space:nowrap;">:<span>352</span></sup>
</p><p>In particular, trees that are grown very deep tend to learn highly irregular patterns: they <a href="/wiki/Overfitting" title="Overfitting">overfit</a> their training sets, i.e. have <a href="/wiki/Bias%E2%80%93variance_tradeoff" title="Bias–variance tradeoff">low bias, but very high variance</a>. Random forests are a way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance.<sup class="reference" id="cite_ref-elemstatlearn_3-2"><a href="#cite_note-elemstatlearn-3">[3]</a></sup><sup class="reference" style="white-space:nowrap;">:<span>587–588</span></sup> This comes at the expense of a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance in the final model.
</p><p>Forests are like the pulling together of decision tree algorithm efforts. Taking the teamwork of many trees thus improving the performance of a single random tree. Though not quite similar, forests give the effects of a K-fold cross validation.
</p>
<h3><span class="mw-headline" id="Bagging">Bagging</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Random_forest&action=edit&section=4" title="Edit section: Bagging">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="hatnote navigation-not-searchable" role="note">Main article: <a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bootstrap aggregating</a></div>
<p>The training algorithm for random forests applies the general technique of <a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">bootstrap aggregating</a>, or bagging, to tree learners. Given a training set <span class="texhtml mvar" style="font-style:italic;">X</span> = <span class="texhtml mvar" style="font-style:italic;">x<sub>1</sub></span>, ..., <span class="texhtml mvar" style="font-style:italic;">x<sub>n</sub></span> with responses <span class="texhtml mvar" style="font-style:italic;">Y</span> = <span class="texhtml mvar" style="font-style:italic;">y<sub>1</sub></span>, ..., <span class="texhtml mvar" style="font-style:italic;">y<sub>n</sub></span>, bagging repeatedly (<i>B</i> times) selects a <a href="/wiki/Sampling_(statistics)#Replacement_of_selected_units" title="Sampling (statistics)">random sample with replacement</a> of the training set and fits trees to these samples:
</p>
<dl><dd>For <span class="texhtml mvar" style="font-style:italic;">b</span> = 1, ..., <span class="texhtml mvar" style="font-style:italic;">B</span>:
<ol><li>Sample, with replacement, <span class="texhtml mvar" style="font-style:italic;">n</span> training examples from <span class="texhtml mvar" style="font-style:italic;">X</span>, <span class="texhtml mvar" style="font-style:italic;">Y</span>; call these <span class="texhtml mvar" style="font-style:italic;">X<sub>b</sub></span>, <span class="texhtml mvar" style="font-style:italic;">Y<sub>b</sub></span>.</li>
<li>Train a classification or regression tree <span class="texhtml mvar" style="font-style:italic;">f<sub>b</sub></span> on <span class="texhtml mvar" style="font-style:italic;">X<sub>b</sub></span>, <span class="texhtml mvar" style="font-style:italic;">Y<sub>b</sub></span>.</li></ol></dd></dl>
<p>After training, predictions for unseen samples <span class="texhtml mvar" style="font-style:italic;">x'</span> can be made by averaging the predictions from all the individual regression trees on <span class="texhtml mvar" style="font-style:italic;">x'</span>:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\hat {f}}={\frac {1}{B}}\sum _{b=1}^{B}f_{b}(x')}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo>=</mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>1</mn>
<mi>B</mi>
</mfrac>
</mrow>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>b</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>B</mi>
</mrow>
</munderover>
<msub>
<mi>f</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>b</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msup>
<mi>x</mi>
<mo>′</mo>
</msup>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\hat {f}}={\frac {1}{B}}\sum _{b=1}^{B}f_{b}(x')}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\hat {f}}={\frac {1}{B}}\sum _{b=1}^{B}f_{b}(x')}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b54befce12aefdb29442bfc71cb5ad452364e8d8" style="vertical-align: -3.005ex; width:17.427ex; height:7.343ex;"/></span></dd></dl>
<p>or by taking the majority vote in the case of classification trees.
</p><p>This bootstrapping procedure leads to better model performance because it decreases the <a class="mw-redirect" href="/wiki/Bias%E2%80%93variance_dilemma" title="Bias–variance dilemma">variance</a> of the model, without increasing the bias. This means that while the predictions of a single tree are highly sensitive to noise in its training set, the average of many trees is not, as long as the trees are not correlated. Simply training many trees on a single training set would give strongly correlated trees (or even the same tree many times, if the training algorithm is deterministic); bootstrap sampling is a way of de-correlating the trees by showing them different training sets.
</p><p>Additionally, an estimate of the uncertainty of the prediction can be made as the standard deviation of the predictions from all the individual regression trees on <span class="texhtml mvar" style="font-style:italic;">x'</span>:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \sigma ={\sqrt {\frac {\sum _{b=1}^{B}(f_{b}(x')-{\hat {f}})^{2}}{B-1}}}.}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>σ<!-- σ --></mi>
<mo>=</mo>
<mrow class="MJX-TeXAtom-ORD">
<msqrt>
<mfrac>
<mrow>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>b</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>B</mi>
</mrow>
</munderover>
<mo stretchy="false">(</mo>
<msub>
<mi>f</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>b</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msup>
<mi>x</mi>
<mo>′</mo>
</msup>
<mo stretchy="false">)</mo>
<mo>−<!-- − --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<msup>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
</mrow>
<mrow>
<mi>B</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</mfrac>
</msqrt>
</mrow>
<mo>.</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \sigma ={\sqrt {\frac {\sum _{b=1}^{B}(f_{b}(x')-{\hat {f}})^{2}}{B-1}}}.}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \sigma ={\sqrt {\frac {\sum _{b=1}^{B}(f_{b}(x')-{\hat {f}})^{2}}{B-1}}}.}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/11e64a440f8625c492d848b38785f833a5882432" style="vertical-align: -2.671ex; width:27.03ex; height:7.676ex;"/></span></dd></dl>
<p>The number of samples/trees, <span class="texhtml mvar" style="font-style:italic;">B</span>, is a free parameter. Typically, a few hundred to several thousand trees are used, depending on the size and nature of the training set. An optimal number of trees <span class="texhtml mvar" style="font-style:italic;">B</span> can be found using <a href="/wiki/Cross-validation_(statistics)" title="Cross-validation (statistics)">cross-validation</a>, or by observing the <i><a href="/wiki/Out-of-bag_error" title="Out-of-bag error">out-of-bag error</a></i>: the mean prediction error on each training sample <span class="texhtml mvar" style="font-style:italic;">xᵢ</span>, using only the trees that did not have <span class="texhtml mvar" style="font-style:italic;">xᵢ</span> in their bootstrap sample.<sup class="reference" id="cite_ref-islr_13-0"><a href="#cite_note-islr-13">[13]</a></sup>
The training and test error tend to level off after some number of trees have been fit.
</p>
<h3><span class="mw-headline" id="From_bagging_to_random_forests">From bagging to random forests</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Random_forest&action=edit&section=5" title="Edit section: From bagging to random forests">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="hatnote navigation-not-searchable" role="note">Main article: <a href="/wiki/Random_subspace_method" title="Random subspace method">Random subspace method</a></div>
<p>The above procedure describes the original bagging algorithm for trees. Random forests differ in only one way from this general scheme: they use a modified tree learning algorithm that selects, at each candidate split in the learning process, a <a href="/wiki/Random_subspace_method" title="Random subspace method">random subset of the features</a>. This process is sometimes called "feature bagging". The reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few <a href="/wiki/Feature_(machine_learning)" title="Feature (machine learning)">features</a> are very strong predictors for the response variable (target output), these features will be selected in many of the <span class="texhtml mvar" style="font-style:italic;">B</span> trees, causing them to become correlated. An analysis of how bagging and random subspace projection contribute to accuracy gains under different conditions is given by Ho.<sup class="reference" id="cite_ref-ho2002_14-0"><a href="#cite_note-ho2002-14">[14]</a></sup>
</p><p>Typically, for a classification problem with <span class="texhtml mvar" style="font-style:italic;">p</span> features, <span class="nowrap">√<span style="border-top:1px solid; padding:0 0.1em;"><span class="texhtml mvar" style="font-style:italic;">p</span></span></span> (rounded down) features are used in each split.<sup class="reference" id="cite_ref-elemstatlearn_3-3"><a href="#cite_note-elemstatlearn-3">[3]</a></sup><sup class="reference" style="white-space:nowrap;">:<span>592</span></sup>  For regression problems the inventors recommend <span class="texhtml mvar" style="font-style:italic;">p/3</span> (rounded down) with a minimum node size of 5 as the default.<sup class="reference" id="cite_ref-elemstatlearn_3-4"><a href="#cite_note-elemstatlearn-3">[3]</a></sup><sup class="reference" style="white-space:nowrap;">:<span>592</span></sup> In practice the best values for these parameters will depend on the problem, and they should be treated as tuning parameters.<sup class="reference" id="cite_ref-elemstatlearn_3-5"><a href="#cite_note-elemstatlearn-3">[3]</a></sup><sup class="reference" style="white-space:nowrap;">:<span>592</span></sup>
</p>
<h3><span class="mw-headline" id="ExtraTrees">ExtraTrees</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Random_forest&action=edit&section=6" title="Edit section: ExtraTrees">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Adding one further step of randomization yields <i>extremely randomized trees</i>, or ExtraTrees. While similar to ordinary random forests in that they are an ensemble of individual trees, there are two main differences: first, each tree is trained using the whole learning sample (rather than a bootstrap sample), and second, the top-down splitting in the tree learner is randomized. Instead of computing the locally <i>optimal</i> cut-point for each feature under consideration (based on, e.g., <a class="mw-redirect" href="/wiki/Information_gain" title="Information gain">information gain</a> or the <a class="mw-redirect" href="/wiki/Gini_impurity" title="Gini impurity">Gini impurity</a>), a <i>random</i> cut-point is selected. This value is selected from a uniform distribution within the feature's empirical range (in the tree's training set). Then, of all the randomly generated splits, the split that yields the highest score is chosen to split the node. Similar to ordinary random forests, the number of randomly selected features to be considered at each node can be specified. Default values for this parameter are <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\sqrt {p}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<msqrt>
<mi>p</mi>
</msqrt>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\sqrt {p}}}</annotation>
</semantics>
</math></span><img alt="{\sqrt {p}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0527785cd1ad7fa60789e172c720affdcdb28b7f" style="vertical-align: -1.171ex; width:3.105ex; height:3.009ex;"/></span> for classification and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle p}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>p</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle p}</annotation>
</semantics>
</math></span><img alt="p" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/81eac1e205430d1f40810df36a0edffdc367af36" style="vertical-align: -0.671ex; margin-left: -0.089ex; width:1.259ex; height:2.009ex;"/></span> for regression, where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle p}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>p</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle p}</annotation>
</semantics>
</math></span><img alt="p" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/81eac1e205430d1f40810df36a0edffdc367af36" style="vertical-align: -0.671ex; margin-left: -0.089ex; width:1.259ex; height:2.009ex;"/></span> is the number of features in the model.<sup class="reference" id="cite_ref-15"><a href="#cite_note-15">[15]</a></sup>
</p>
<h2><span class="mw-headline" id="Properties">Properties</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Random_forest&action=edit&section=7" title="Edit section: Properties">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Variable_importance">Variable importance</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Random_forest&action=edit&section=8" title="Edit section: Variable importance">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Random forests can be used to rank the importance of variables in a regression or classification problem in a natural way.  The following technique was described in Breiman's original paper<sup class="reference" id="cite_ref-breiman2001_7-2"><a href="#cite_note-breiman2001-7">[7]</a></sup> and is implemented in the <a href="/wiki/R_(programming_language)" title="R (programming language)">R</a> package randomForest.<sup class="reference" id="cite_ref-rpackage_8-1"><a href="#cite_note-rpackage-8">[8]</a></sup>
</p><p>The first step in measuring the variable importance in a data set <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\mathcal {D}}_{n}=\{(X_{i},Y_{i})\}_{i=1}^{n}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">D</mi>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo>=</mo>
<mo fence="false" stretchy="false">{</mo>
<mo stretchy="false">(</mo>
<msub>
<mi>X</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo>,</mo>
<msub>
<mi>Y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
<msubsup>
<mo fence="false" stretchy="false">}</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msubsup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\mathcal {D}}_{n}=\{(X_{i},Y_{i})\}_{i=1}^{n}}</annotation>
</semantics>
</math></span><img alt="{\mathcal {D}}_{n}=\{(X_{i},Y_{i})\}_{i=1}^{n}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ee98824c21c5539b16d0c560a1445101f74898ba" style="vertical-align: -1.005ex; width:19.051ex; height:3.009ex;"/></span> is to fit a random forest to the data.  During the fitting process the <a href="/wiki/Out-of-bag_error" title="Out-of-bag error">out-of-bag error</a> for each data point is recorded and averaged over the forest (errors on an independent test set can be substituted if bagging is not used during training).
</p><p>To measure the importance of the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle j}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>j</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle j}</annotation>
</semantics>
</math></span><img alt="j" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:0.985ex; height:2.509ex;"/></span>-th feature after training, the values of the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle j}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>j</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle j}</annotation>
</semantics>
</math></span><img alt="j" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:0.985ex; height:2.509ex;"/></span>-th feature are permuted among the training data and the out-of-bag error is again computed on this perturbed data set.  The importance score for the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle j}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>j</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle j}</annotation>
</semantics>
</math></span><img alt="j" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:0.985ex; height:2.509ex;"/></span>-th feature is computed by averaging the difference in out-of-bag error before and after the permutation over all trees.  The score is normalized by the standard deviation of these differences.
</p><p>Features which produce large values for this score are ranked as more important than features which produce small values. The statistical definition of the variable importance measure was given and analyzed by Zhu <i>et al.</i><sup class="reference" id="cite_ref-16"><a href="#cite_note-16">[16]</a></sup>
</p><p>This method of determining variable importance has some drawbacks.  For data including categorical variables with different number of levels, random forests are biased in favor of those attributes with more levels. Methods such as <a href="/wiki/Partial_permutation" title="Partial permutation">partial permutations</a><sup class="reference" id="cite_ref-17"><a href="#cite_note-17">[17]</a></sup><sup class="reference" id="cite_ref-18"><a href="#cite_note-18">[18]</a></sup>
and growing unbiased trees<sup class="reference" id="cite_ref-19"><a href="#cite_note-19">[19]</a></sup><sup class="reference" id="cite_ref-20"><a href="#cite_note-20">[20]</a></sup> can be used to solve the problem.  If the data contain groups of correlated features of similar relevance for the output, then smaller groups are favored over larger groups.<sup class="reference" id="cite_ref-21"><a href="#cite_note-21">[21]</a></sup>
</p>
<h3><span class="mw-headline" id="Relationship_to_nearest_neighbors">Relationship to nearest neighbors</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Random_forest&action=edit&section=9" title="Edit section: Relationship to nearest neighbors">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>A relationship between random forests and the <a class="mw-redirect" href="/wiki/K-nearest_neighbor_algorithm" title="K-nearest neighbor algorithm"><span class="texhtml mvar" style="font-style:italic;">k</span>-nearest neighbor algorithm</a> (<span class="texhtml mvar" style="font-style:italic;">k</span>-NN) was pointed out by Lin and Jeon in 2002.<sup class="reference" id="cite_ref-linjeon02_22-0"><a href="#cite_note-linjeon02-22">[22]</a></sup> It turns out that both can be viewed as so-called <i>weighted neighborhoods schemes</i>. These are models built from a training set <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \{(x_{i},y_{i})\}_{i=1}^{n}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mo fence="false" stretchy="false">{</mo>
<mo stretchy="false">(</mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo>,</mo>
<msub>
<mi>y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
<msubsup>
<mo fence="false" stretchy="false">}</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msubsup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \{(x_{i},y_{i})\}_{i=1}^{n}}</annotation>
</semantics>
</math></span><img alt="\{(x_{i},y_{i})\}_{i=1}^{n}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0a9e0a59cd73d8fb11a10caa787512bf93af04b4" style="vertical-align: -1.005ex; width:12.137ex; height:3.009ex;"/></span> that make predictions <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\hat {y}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>y</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\hat {y}}}</annotation>
</semantics>
</math></span><img alt="{\hat {y}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3dc8de3d8ea01304329ef9518fad7a6d196c4c01" style="vertical-align: -0.671ex; width:1.302ex; height:2.509ex;"/></span> for new points <span class="texhtml mvar" style="font-style:italic;">x'</span> by looking at the "neighborhood" of the point, formalized by a weight function <span class="texhtml mvar" style="font-style:italic;">W</span>:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\hat {y}}=\sum _{i=1}^{n}W(x_{i},x')\,y_{i}.}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>y</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo>=</mo>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</munderover>
<mi>W</mi>
<mo stretchy="false">(</mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo>,</mo>
<msup>
<mi>x</mi>
<mo>′</mo>
</msup>
<mo stretchy="false">)</mo>
<mspace width="thinmathspace"></mspace>
<msub>
<mi>y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo>.</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\hat {y}}=\sum _{i=1}^{n}W(x_{i},x')\,y_{i}.}</annotation>
</semantics>
</math></span><img alt="{\hat {y}}=\sum _{i=1}^{n}W(x_{i},x')\,y_{i}." aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f9cd87e3168f0200bd67d04530ab9124dcb8cafc" style="vertical-align: -3.005ex; width:20.538ex; height:6.843ex;"/></span></dd></dl>
<p>Here, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle W(x_{i},x')}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>W</mi>
<mo stretchy="false">(</mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo>,</mo>
<msup>
<mi>x</mi>
<mo>′</mo>
</msup>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle W(x_{i},x')}</annotation>
</semantics>
</math></span><img alt="W(x_{i},x')" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f87df356c5a2445516fca56d2df6eb73e64e48ce" style="vertical-align: -0.838ex; width:9.422ex; height:3.009ex;"/></span> is the non-negative weight of the <span class="texhtml mvar" style="font-style:italic;">i</span>'th training point relative to the new point <span class="texhtml mvar" style="font-style:italic;">x'</span> in the same tree. For any particular <span class="texhtml mvar" style="font-style:italic;">x'</span>, the weights for points <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle x_{i}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle x_{i}}</annotation>
</semantics>
</math></span><img alt="x_{i}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e87000dd6142b81d041896a30fe58f0c3acb2158" style="vertical-align: -0.671ex; width:2.129ex; height:2.009ex;"/></span> must sum to one. Weight functions are given as follows:
</p>
<ul><li>In <span class="texhtml mvar" style="font-style:italic;">k</span>-NN, the weights are <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle W(x_{i},x')={\frac {1}{k}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>W</mi>
<mo stretchy="false">(</mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo>,</mo>
<msup>
<mi>x</mi>
<mo>′</mo>
</msup>
<mo stretchy="false">)</mo>
<mo>=</mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>1</mn>
<mi>k</mi>
</mfrac>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle W(x_{i},x')={\frac {1}{k}}}</annotation>
</semantics>
</math></span><img alt="W(x_{i},x')={\frac {1}{k}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ae3098adcbde36fd1253a0ee85a3868d67b1861f" style="vertical-align: -2.005ex; width:14.568ex; height:5.343ex;"/></span> if <span class="texhtml mvar" style="font-style:italic;">x<sub>i</sub></span> is one of the <span class="texhtml mvar" style="font-style:italic;">k</span> points closest to <span class="texhtml mvar" style="font-style:italic;">x'</span>, and zero otherwise.</li>
<li>In a tree, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle W(x_{i},x')={\frac {1}{k'}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>W</mi>
<mo stretchy="false">(</mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo>,</mo>
<msup>
<mi>x</mi>
<mo>′</mo>
</msup>
<mo stretchy="false">)</mo>
<mo>=</mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>1</mn>
<msup>
<mi>k</mi>
<mo>′</mo>
</msup>
</mfrac>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle W(x_{i},x')={\frac {1}{k'}}}</annotation>
</semantics>
</math></span><img alt="W(x_{i},x')={\frac {1}{k'}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d80899c24f8d4488f56290ae99cfe61558667873" style="vertical-align: -2.005ex; width:15.253ex; height:5.343ex;"/></span> if <span class="texhtml mvar" style="font-style:italic;">x<sub>i</sub></span> is one of the <span class="texhtml mvar" style="font-style:italic;">k'</span> points in the same leaf as <span class="texhtml mvar" style="font-style:italic;">x'</span>, and zero otherwise.</li></ul>
<p>Since a forest averages the predictions of a set of <span class="texhtml mvar" style="font-style:italic;">m</span> trees with individual weight functions <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle W_{j}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle W_{j}}</annotation>
</semantics>
</math></span><img alt="W_{j}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fa98874e6beb16373e8d0e056ba550cf653676a1" style="vertical-align: -1.005ex; width:3.103ex; height:2.843ex;"/></span>, its predictions are
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\hat {y}}={\frac {1}{m}}\sum _{j=1}^{m}\sum _{i=1}^{n}W_{j}(x_{i},x')\,y_{i}=\sum _{i=1}^{n}\left({\frac {1}{m}}\sum _{j=1}^{m}W_{j}(x_{i},x')\right)\,y_{i}.}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>y</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo>=</mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>1</mn>
<mi>m</mi>
</mfrac>
</mrow>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>m</mi>
</mrow>
</munderover>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</munderover>
<msub>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo>,</mo>
<msup>
<mi>x</mi>
<mo>′</mo>
</msup>
<mo stretchy="false">)</mo>
<mspace width="thinmathspace"></mspace>
<msub>
<mi>y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo>=</mo>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</munderover>
<mrow>
<mo>(</mo>
<mrow>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>1</mn>
<mi>m</mi>
</mfrac>
</mrow>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>m</mi>
</mrow>
</munderover>
<msub>
<mi>W</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo>,</mo>
<msup>
<mi>x</mi>
<mo>′</mo>
</msup>
<mo stretchy="false">)</mo>
</mrow>
<mo>)</mo>
</mrow>
<mspace width="thinmathspace"></mspace>
<msub>
<mi>y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo>.</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\hat {y}}={\frac {1}{m}}\sum _{j=1}^{m}\sum _{i=1}^{n}W_{j}(x_{i},x')\,y_{i}=\sum _{i=1}^{n}\left({\frac {1}{m}}\sum _{j=1}^{m}W_{j}(x_{i},x')\right)\,y_{i}.}</annotation>
</semantics>
</math></span><img alt="{\hat {y}}={\frac {1}{m}}\sum _{j=1}^{m}\sum _{i=1}^{n}W_{j}(x_{i},x')\,y_{i}=\sum _{i=1}^{n}\left({\frac {1}{m}}\sum _{j=1}^{m}W_{j}(x_{i},x')\right)\,y_{i}." aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8b819eff4f3bb5ee472825d9996c3fd5f6b18ed7" style="vertical-align: -3.338ex; width:58.543ex; height:7.676ex;"/></span></dd></dl>
<p>This shows that the whole forest is again a weighted neighborhood scheme, with weights that average those of the individual trees. The neighbors of <span class="texhtml mvar" style="font-style:italic;">x'</span> in this interpretation are the points <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle x_{i}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle x_{i}}</annotation>
</semantics>
</math></span><img alt="x_{i}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e87000dd6142b81d041896a30fe58f0c3acb2158" style="vertical-align: -0.671ex; width:2.129ex; height:2.009ex;"/></span> sharing the same leaf in any tree <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle j}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>j</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle j}</annotation>
</semantics>
</math></span><img alt="j" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:0.985ex; height:2.509ex;"/></span>. In this way, the neighborhood of <span class="texhtml mvar" style="font-style:italic;">x'</span> depends in a complex way on the structure of the trees, and thus on the structure of the training set. Lin and Jeon show that the shape of the neighborhood used by a random forest adapts to the local importance of each feature.<sup class="reference" id="cite_ref-linjeon02_22-1"><a href="#cite_note-linjeon02-22">[22]</a></sup>
</p>
<h2><span class="mw-headline" id="Unsupervised_learning_with_random_forests">Unsupervised learning with random forests</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Random_forest&action=edit&section=10" title="Edit section: Unsupervised learning with random forests">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>As part of their construction, random forest predictors naturally lead to a dissimilarity measure among the observations. One can also define a random forest dissimilarity measure between unlabeled data: the idea is to construct a random forest predictor that distinguishes the “observed” data from suitably generated synthetic data.<sup class="reference" id="cite_ref-breiman2001_7-3"><a href="#cite_note-breiman2001-7">[7]</a></sup><sup class="reference" id="cite_ref-23"><a href="#cite_note-23">[23]</a></sup>
The observed data are the original unlabeled data and the synthetic data are drawn from a reference distribution. A random forest dissimilarity can be attractive because it handles mixed variable types very well, is invariant to monotonic transformations of the input variables, and is robust to outlying observations. The random forest dissimilarity easily deals with a large number of semi-continuous variables due to its intrinsic variable selection; for example, the "Addcl 1" random forest dissimilarity weighs the contribution of each variable according to how dependent it is on other variables. The random forest dissimilarity has been used in a variety of applications, e.g. to find clusters of patients based on tissue marker data.<sup class="reference" id="cite_ref-24"><a href="#cite_note-24">[24]</a></sup>
</p>
<h2><span class="mw-headline" id="Variants">Variants</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Random_forest&action=edit&section=11" title="Edit section: Variants">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Instead of decision trees, linear models have been proposed and evaluated as base estimators in random forests, in particular <a href="/wiki/Multinomial_logistic_regression" title="Multinomial logistic regression">multinomial logistic regression</a> and <a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">naive Bayes classifiers</a>.<sup class="reference" id="cite_ref-25"><a href="#cite_note-25">[25]</a></sup><sup class="reference" id="cite_ref-26"><a href="#cite_note-26">[26]</a></sup>
</p>
<h2><span class="mw-headline" id="Kernel_random_forest">Kernel random forest</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Random_forest&action=edit&section=12" title="Edit section: Kernel random forest">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>In machine learning, kernel random forests establish the connection between random forests and <a href="/wiki/Kernel_method" title="Kernel method">kernel methods</a>. By slightly modifying their definition, random forests can be rewritten as <a href="/wiki/Kernel_method" title="Kernel method">kernel methods</a>, which are more interpretable and easier to analyze.<sup class="reference" id="cite_ref-scornet2015random_27-0"><a href="#cite_note-scornet2015random-27">[27]</a></sup>
</p>
<h3><span class="mw-headline" id="History_2">History</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Random_forest&action=edit&section=13" title="Edit section: History">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="/wiki/Leo_Breiman" title="Leo Breiman">Leo Breiman</a><sup class="reference" id="cite_ref-breiman2000some_28-0"><a href="#cite_note-breiman2000some-28">[28]</a></sup> was the first person to notice the link between random forest and <a class="mw-redirect" href="/wiki/Kernel_methods" title="Kernel methods">kernel methods</a>. He pointed out that random forests which are grown using <a class="mw-redirect" href="/wiki/I.i.d." title="I.i.d.">i.i.d.</a> random vectors in the tree construction are equivalent to a kernel acting on the true margin. Lin and Jeon<sup class="reference" id="cite_ref-lin2006random_29-0"><a href="#cite_note-lin2006random-29">[29]</a></sup> established the connection between random forests and adaptive nearest neighbor, implying that random forests can be seen as adaptive kernel estimates. Davies and Ghahramani<sup class="reference" id="cite_ref-davies2014random_30-0"><a href="#cite_note-davies2014random-30">[30]</a></sup> proposed Random Forest Kernel and show that it can empirically outperform state-of-art kernel methods. Scornet<sup class="reference" id="cite_ref-scornet2015random_27-1"><a href="#cite_note-scornet2015random-27">[27]</a></sup> first defined KeRF estimates and gave the explicit link between KeRF estimates and random forest. He also gave explicit expressions for kernels based on centered random forest<sup class="reference" id="cite_ref-breiman2004consistency_31-0"><a href="#cite_note-breiman2004consistency-31">[31]</a></sup> and uniform random forest,<sup class="reference" id="cite_ref-arlot2014analysis_32-0"><a href="#cite_note-arlot2014analysis-32">[32]</a></sup> two simplified models of random forest. He named these two KeRFs Centered KeRF and Uniform KeRF, and proved upper bounds on their rates of consistency.
</p>
<h3><span class="mw-headline" id="Notations_and_definitions">Notations and definitions</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Random_forest&action=edit&section=14" title="Edit section: Notations and definitions">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<h4><span class="mw-headline" id="Preliminaries:_Centered_forests">Preliminaries: Centered forests</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Random_forest&action=edit&section=15" title="Edit section: Preliminaries: Centered forests">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Centered forest<sup class="reference" id="cite_ref-breiman2004consistency_31-1"><a href="#cite_note-breiman2004consistency-31">[31]</a></sup> is a simplified model for Breiman's original random forest, which uniformly selects an attribute among all attributes and performs splits at the center of the cell along the pre-chosen attribute. The algorithm stops when a fully binary tree of level <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle k}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>k</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle k}</annotation>
</semantics>
</math></span><img alt="k" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c3c9a2c7b599b37105512c5d570edc034056dd40" style="vertical-align: -0.338ex; width:1.211ex; height:2.176ex;"/></span> is built, where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle k\in \mathbb {N} }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>k</mi>
<mo>∈<!-- ∈ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">N</mi>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle k\in \mathbb {N} }</annotation>
</semantics>
</math></span><img alt="k\in {\mathbb  {N}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2a5bc4b7383031ba693b7433198ead7170954c1d" style="vertical-align: -0.338ex; width:5.73ex; height:2.176ex;"/></span> is a parameter of the algorithm.
</p>
<h4><span class="mw-headline" id="Uniform_forest">Uniform forest</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Random_forest&action=edit&section=16" title="Edit section: Uniform forest">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Uniform forest<sup class="reference" id="cite_ref-arlot2014analysis_32-1"><a href="#cite_note-arlot2014analysis-32">[32]</a></sup> is another simplified model for Breiman's original random forest, which uniformly selects a feature among all features and performs splits at a point uniformly drawn on the side of the cell, along the preselected feature.
</p>
<h4><span class="mw-headline" id="From_random_forest_to_KeRF">From random forest to KeRF</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Random_forest&action=edit&section=17" title="Edit section: From random forest to KeRF">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Given a training sample  <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\mathcal {D}}_{n}=\{(\mathbf {X} _{i},Y_{i})\}_{i=1}^{n}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">D</mi>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo>=</mo>
<mo fence="false" stretchy="false">{</mo>
<mo stretchy="false">(</mo>
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">X</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo>,</mo>
<msub>
<mi>Y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
<msubsup>
<mo fence="false" stretchy="false">}</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msubsup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\mathcal {D}}_{n}=\{(\mathbf {X} _{i},Y_{i})\}_{i=1}^{n}}</annotation>
</semantics>
</math></span><img alt="{\mathcal  {D}}_{n}=\{({\mathbf  {X}}_{i},Y_{i})\}_{{i=1}}^{n}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c67c8cf7d7485be2a566d02284d9bc27a3f61241" style="vertical-align: -1.005ex; width:19.147ex; height:3.009ex;"/></span> of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle [0,1]^{p}\times \mathbb {R} }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mo stretchy="false">[</mo>
<mn>0</mn>
<mo>,</mo>
<mn>1</mn>
<msup>
<mo stretchy="false">]</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>p</mi>
</mrow>
</msup>
<mo>×<!-- × --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">R</mi>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle [0,1]^{p}\times \mathbb {R} }</annotation>
</semantics>
</math></span><img alt="[0,1]^{p}\times {\mathbb  {R}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6724c5c806dd7cee08c1f2133fc89f94ed0c2e91" style="vertical-align: -0.838ex; width:10.23ex; height:2.843ex;"/></span>-valued independent random variables distributed as the independent prototype pair <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle (\mathbf {X} ,Y)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">X</mi>
</mrow>
<mo>,</mo>
<mi>Y</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle (\mathbf {X} ,Y)}</annotation>
</semantics>
</math></span><img alt="({\mathbf  {X}},Y)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f50eb05dacdb567fc019a3bbcd44b596a1c59a13" style="vertical-align: -0.838ex; width:6.636ex; height:2.843ex;"/></span>, where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \operatorname {E} [Y^{2}]<\infty }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<msup>
<mi>Y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo stretchy="false">]</mo>
<mo><</mo>
<mi mathvariant="normal">∞<!-- ∞ --></mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \operatorname {E} [Y^{2}]<\infty }</annotation>
</semantics>
</math></span><img alt="{\displaystyle \operatorname {E} [Y^{2}]<\infty }" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/07a863a618762e9fecf30fadce90c3f6484db196" style="vertical-align: -0.838ex; width:11.253ex; height:3.176ex;"/></span>. We aim at predicting the response <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle Y}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>Y</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle Y}</annotation>
</semantics>
</math></span><img alt="Y" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/961d67d6b454b4df2301ac571808a3538b3a6d3f" style="vertical-align: -0.171ex; width:1.773ex; height:2.009ex;"/></span>, associated with the random variable <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbf {X} }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">X</mi>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbf {X} }</annotation>
</semantics>
</math></span><img alt="\mathbf {X} " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9f75966a2f9d5672136fa9401ee1e75008f95ffd" style="vertical-align: -0.338ex; width:2.019ex; height:2.176ex;"/></span>, by estimating the regression function <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle m(\mathbf {x} )=\operatorname {E} [Y\mid \mathbf {X} =\mathbf {x} ]}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>m</mi>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo stretchy="false">)</mo>
<mo>=</mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mi>Y</mi>
<mo>∣<!-- ∣ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">X</mi>
</mrow>
<mo>=</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo stretchy="false">]</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle m(\mathbf {x} )=\operatorname {E} [Y\mid \mathbf {X} =\mathbf {x} ]}</annotation>
</semantics>
</math></span><img alt="{\displaystyle m(\mathbf {x} )=\operatorname {E} [Y\mid \mathbf {X} =\mathbf {x} ]}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0c097ce105048693746d5524ac0ef9fc514da53b" style="vertical-align: -0.838ex; width:21.475ex; height:2.843ex;"/></span>. A random regression forest is an ensemble of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle M}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>M</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle M}</annotation>
</semantics>
</math></span><img alt="M" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f82cade9898ced02fdd08712e5f0c0151758a0dd" style="vertical-align: -0.338ex; width:2.442ex; height:2.176ex;"/></span> randomized regression trees. Denote <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle m_{n}(\mathbf {x} ,\mathbf {\Theta } _{j})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>m</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">Θ<!-- Θ --></mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle m_{n}(\mathbf {x} ,\mathbf {\Theta } _{j})}</annotation>
</semantics>
</math></span><img alt="m_{n}({\mathbf  {x}},{\mathbf  {\Theta }}_{j})" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/95ad3dfa3a0ed6fc135df01b527d6be634ff9d07" style="vertical-align: -1.005ex; width:10.5ex; height:3.009ex;"/></span> the predicted value at point <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbf {x} }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbf {x} }</annotation>
</semantics>
</math></span><img alt="\mathbf {x} " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/32adf004df5eb0a8c7fd8c0b6b7405183c5a5ef2" style="vertical-align: -0.338ex; width:1.411ex; height:1.676ex;"/></span> by the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle j}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>j</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle j}</annotation>
</semantics>
</math></span><img alt="j" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:0.985ex; height:2.509ex;"/></span>-th tree, where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbf {\Theta } _{1},\ldots ,\mathbf {\Theta } _{M}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">Θ<!-- Θ --></mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">Θ<!-- Θ --></mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>M</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbf {\Theta } _{1},\ldots ,\mathbf {\Theta } _{M}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \mathbf {\Theta } _{1},\ldots ,\mathbf {\Theta } _{M}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8588793690df49c9b10f8c44499b1ee82106867f" style="vertical-align: -0.671ex; width:12.347ex; height:2.509ex;"/></span> are independent random variables, distributed as a generic random variable <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbf {\Theta } }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">Θ<!-- Θ --></mi>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbf {\Theta } }</annotation>
</semantics>
</math></span><img alt="{\mathbf  {\Theta }}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1a15b5576ec407e52c5fdfc21c5d7bf45402406a" style="vertical-align: -0.338ex; width:2.078ex; height:2.176ex;"/></span>, independent of the sample <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\mathcal {D}}_{n}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">D</mi>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\mathcal {D}}_{n}}</annotation>
</semantics>
</math></span><img alt="\mathcal{D}_n" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4ba7d8da2372a68d034e848dbd973b7f173769fe" style="vertical-align: -0.671ex; width:3.01ex; height:2.509ex;"/></span>. This random variable can be used to describe the randomness induced by node splitting and the sampling procedure for tree construction. The trees are combined to form the finite forest estimate <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle m_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})={\frac {1}{M}}\sum _{j=1}^{M}m_{n}(\mathbf {x} ,\Theta _{j})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>m</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>M</mi>
<mo>,</mo>
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<msub>
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<msub>
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>M</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
<mo>=</mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>1</mn>
<mi>M</mi>
</mfrac>
</mrow>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>M</mi>
</mrow>
</munderover>
<msub>
<mi>m</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<msub>
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle m_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})={\frac {1}{M}}\sum _{j=1}^{M}m_{n}(\mathbf {x} ,\Theta _{j})}</annotation>
</semantics>
</math></span><img alt="{\displaystyle m_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})={\frac {1}{M}}\sum _{j=1}^{M}m_{n}(\mathbf {x} ,\Theta _{j})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e1cd03b3dba46782bb60f083359fd8e77250024c" style="vertical-align: -3.338ex; width:42.242ex; height:7.676ex;"/></span>.
For regression trees, we have <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle m_{n}=\sum _{i=1}^{n}{\frac {Y_{i}\mathbf {1} _{\mathbf {X} _{i}\in A_{n}(\mathbf {x} ,\Theta _{j})}}{N_{n}(\mathbf {x} ,\Theta _{j})}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>m</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo>=</mo>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</munderover>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mrow>
<msub>
<mi>Y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mn mathvariant="bold">1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">X</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<msub>
<mi>A</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<msub>
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mrow>
</msub>
</mrow>
<mrow>
<msub>
<mi>N</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<msub>
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mrow>
</mfrac>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle m_{n}=\sum _{i=1}^{n}{\frac {Y_{i}\mathbf {1} _{\mathbf {X} _{i}\in A_{n}(\mathbf {x} ,\Theta _{j})}}{N_{n}(\mathbf {x} ,\Theta _{j})}}}</annotation>
</semantics>
</math></span><img alt="m_{n}=\sum _{{i=1}}^{n}{\frac  {Y_{i}{\mathbf  {1}}_{{{\mathbf  {X}}_{i}\in A_{n}({\mathbf  {x}},\Theta _{j})}}}{N_{n}({\mathbf  {x}},\Theta _{j})}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d1ad1f4a5e0e849bfbf10492b08e5e9b55b6c711" style="vertical-align: -3.005ex; width:24.729ex; height:7.009ex;"/></span>, where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle A_{n}(\mathbf {x} ,\Theta _{j})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>A</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<msub>
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle A_{n}(\mathbf {x} ,\Theta _{j})}</annotation>
</semantics>
</math></span><img alt="A_{n}({\mathbf  {x}},\Theta _{j})" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1811d33726c0ea70d83e6f5ee5228d82e6229300" style="vertical-align: -1.005ex; width:9.934ex; height:3.009ex;"/></span> is the cell containing <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbf {x} }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbf {x} }</annotation>
</semantics>
</math></span><img alt="\mathbf {x} " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/32adf004df5eb0a8c7fd8c0b6b7405183c5a5ef2" style="vertical-align: -0.338ex; width:1.411ex; height:1.676ex;"/></span>, designed with randomness <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \Theta _{j}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \Theta _{j}}</annotation>
</semantics>
</math></span><img alt="\Theta _{j}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/10b9d5bab99a44d98171cce92f04d0e700060512" style="vertical-align: -1.005ex; width:2.718ex; height:2.843ex;"/></span> and dataset <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\mathcal {D}}_{n}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">D</mi>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\mathcal {D}}_{n}}</annotation>
</semantics>
</math></span><img alt="\mathcal{D}_n" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4ba7d8da2372a68d034e848dbd973b7f173769fe" style="vertical-align: -0.671ex; width:3.01ex; height:2.509ex;"/></span>, and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle N_{n}(\mathbf {x} ,\Theta _{j})=\sum _{i=1}^{n}\mathbf {1} _{\mathbf {X} _{i}\in A_{n}(\mathbf {x} ,\Theta _{j})}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>N</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<msub>
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
<mo>=</mo>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</munderover>
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mn mathvariant="bold">1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">X</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<msub>
<mi>A</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<msub>
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle N_{n}(\mathbf {x} ,\Theta _{j})=\sum _{i=1}^{n}\mathbf {1} _{\mathbf {X} _{i}\in A_{n}(\mathbf {x} ,\Theta _{j})}}</annotation>
</semantics>
</math></span><img alt="N_{n}({\mathbf  {x}},\Theta _{j})=\sum _{{i=1}}^{n}{\mathbf  {1}}_{{{\mathbf  {X}}_{i}\in A_{n}({\mathbf  {x}},\Theta _{j})}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2c13629db1ca0ba725280a4f2bbf7adfc5c9f8a5" style="vertical-align: -3.005ex; width:28.54ex; height:6.843ex;"/></span>.
</p><p>Thus random forest estimates satisfy, for all <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbf {x} \in [0,1]^{d}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>∈<!-- ∈ --></mo>
<mo stretchy="false">[</mo>
<mn>0</mn>
<mo>,</mo>
<mn>1</mn>
<msup>
<mo stretchy="false">]</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>d</mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbf {x} \in [0,1]^{d}}</annotation>
</semantics>
</math></span><img alt="{\mathbf  {x}}\in [0,1]^{d}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/691c9727c059400e1d15031c7e0e08fbd6774282" style="vertical-align: -0.838ex; width:9.996ex; height:3.176ex;"/></span>, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle m_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})={\frac {1}{M}}\sum _{j=1}^{M}\left(\sum _{i=1}^{n}{\frac {Y_{i}\mathbf {1} _{\mathbf {X} _{i}\in A_{n}(\mathbf {x} ,\Theta _{j})}}{N_{n}(\mathbf {x} ,\Theta _{j})}}\right)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>m</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>M</mi>
<mo>,</mo>
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<msub>
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<msub>
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>M</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
<mo>=</mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>1</mn>
<mi>M</mi>
</mfrac>
</mrow>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>M</mi>
</mrow>
</munderover>
<mrow>
<mo>(</mo>
<mrow>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</munderover>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mrow>
<msub>
<mi>Y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mn mathvariant="bold">1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">X</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<msub>
<mi>A</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<msub>
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mrow>
</msub>
</mrow>
<mrow>
<msub>
<mi>N</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<msub>
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mrow>
</mfrac>
</mrow>
</mrow>
<mo>)</mo>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle m_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})={\frac {1}{M}}\sum _{j=1}^{M}\left(\sum _{i=1}^{n}{\frac {Y_{i}\mathbf {1} _{\mathbf {X} _{i}\in A_{n}(\mathbf {x} ,\Theta _{j})}}{N_{n}(\mathbf {x} ,\Theta _{j})}}\right)}</annotation>
</semantics>
</math></span><img alt="m_{{M,n}}({\mathbf  {x}},\Theta _{1},\ldots ,\Theta _{M})={\frac  {1}{M}}\sum _{{j=1}}^{M}\left(\sum _{{i=1}}^{n}{\frac  {Y_{i}{\mathbf  {1}}_{{{\mathbf  {X}}_{i}\in A_{n}({\mathbf  {x}},\Theta _{j})}}}{N_{n}({\mathbf  {x}},\Theta _{j})}}\right)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/671426d343ccaba3944706fd2517e5f7035f674c" style="vertical-align: -3.338ex; width:54.064ex; height:7.676ex;"/></span>. Random regression forest has two level of averaging, first over the samples in the target cell of a tree, then over all trees. Thus the contributions of observations that are in cells with a high density of data points are smaller than that of observations which belong to less populated cells. In order to improve the random forest methods and compensate the misestimation, Scornet<sup class="reference" id="cite_ref-scornet2015random_27-2"><a href="#cite_note-scornet2015random-27">[27]</a></sup> defined KeRF by
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\tilde {m}}_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})={\frac {1}{\sum _{j=1}^{M}N_{n}(\mathbf {x} ,\Theta _{j})}}\sum _{j=1}^{M}\sum _{i=1}^{n}Y_{i}\mathbf {1} _{\mathbf {X} _{i}\in A_{n}(\mathbf {x} ,\Theta _{j})},}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>m</mi>
<mo stretchy="false">~<!-- ~ --></mo>
</mover>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>M</mi>
<mo>,</mo>
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<msub>
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<msub>
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>M</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
<mo>=</mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>1</mn>
<mrow>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>M</mi>
</mrow>
</munderover>
<msub>
<mi>N</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<msub>
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mrow>
</mfrac>
</mrow>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>M</mi>
</mrow>
</munderover>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</munderover>
<msub>
<mi>Y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mn mathvariant="bold">1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">X</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<msub>
<mi>A</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<msub>
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mrow>
</msub>
<mo>,</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\tilde {m}}_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})={\frac {1}{\sum _{j=1}^{M}N_{n}(\mathbf {x} ,\Theta _{j})}}\sum _{j=1}^{M}\sum _{i=1}^{n}Y_{i}\mathbf {1} _{\mathbf {X} _{i}\in A_{n}(\mathbf {x} ,\Theta _{j})},}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\tilde {m}}_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})={\frac {1}{\sum _{j=1}^{M}N_{n}(\mathbf {x} ,\Theta _{j})}}\sum _{j=1}^{M}\sum _{i=1}^{n}Y_{i}\mathbf {1} _{\mathbf {X} _{i}\in A_{n}(\mathbf {x} ,\Theta _{j})},}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/565e75beaa1dd3b92753899bfa390abba19af5fc" style="vertical-align: -3.505ex; width:63.659ex; height:7.843ex;"/></span></dd></dl>
<p>which is equal to the mean of the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle Y_{i}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>Y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle Y_{i}}</annotation>
</semantics>
</math></span><img alt="Y_{i}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d57be496fff95ee2a97ee43c7f7fe244b4dbf8ae" style="vertical-align: -0.671ex; width:2.15ex; height:2.509ex;"/></span>'s falling in the cells containing <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbf {x} }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbf {x} }</annotation>
</semantics>
</math></span><img alt="\mathbf {x} " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/32adf004df5eb0a8c7fd8c0b6b7405183c5a5ef2" style="vertical-align: -0.338ex; width:1.411ex; height:1.676ex;"/></span> in the forest. If we define the connection function of the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle M}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>M</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle M}</annotation>
</semantics>
</math></span><img alt="M" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f82cade9898ced02fdd08712e5f0c0151758a0dd" style="vertical-align: -0.338ex; width:2.442ex; height:2.176ex;"/></span> finite forest as <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle K_{M,n}(\mathbf {x} ,\mathbf {z} )={\frac {1}{M}}\sum _{j=1}^{M}\mathbf {1} _{\mathbf {z} \in A_{n}(\mathbf {x} ,\Theta _{j})}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>K</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>M</mi>
<mo>,</mo>
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">z</mi>
</mrow>
<mo stretchy="false">)</mo>
<mo>=</mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>1</mn>
<mi>M</mi>
</mfrac>
</mrow>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>M</mi>
</mrow>
</munderover>
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mn mathvariant="bold">1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">z</mi>
</mrow>
<mo>∈<!-- ∈ --></mo>
<msub>
<mi>A</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<msub>
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle K_{M,n}(\mathbf {x} ,\mathbf {z} )={\frac {1}{M}}\sum _{j=1}^{M}\mathbf {1} _{\mathbf {z} \in A_{n}(\mathbf {x} ,\Theta _{j})}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle K_{M,n}(\mathbf {x} ,\mathbf {z} )={\frac {1}{M}}\sum _{j=1}^{M}\mathbf {1} _{\mathbf {z} \in A_{n}(\mathbf {x} ,\Theta _{j})}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7723dd64358a146a8bc819a79f9c15266b4d37dc" style="vertical-align: -3.338ex; width:31.754ex; height:7.676ex;"/></span>, i.e. the proportion of cells shared between <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbf {x} }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbf {x} }</annotation>
</semantics>
</math></span><img alt="\mathbf {x} " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/32adf004df5eb0a8c7fd8c0b6b7405183c5a5ef2" style="vertical-align: -0.338ex; width:1.411ex; height:1.676ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbf {z} }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">z</mi>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbf {z} }</annotation>
</semantics>
</math></span><img alt="\mathbf {z} " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/82eca5d0928078d5a61b9e7e98cc73db31070909" style="vertical-align: -0.338ex; width:1.188ex; height:1.676ex;"/></span>, then almost surely we have <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\tilde {m}}_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})={\frac {\sum _{i=1}^{n}Y_{i}K_{M,n}(\mathbf {x} ,\mathbf {x} _{i})}{\sum _{\ell =1}^{n}K_{M,n}(\mathbf {x} ,\mathbf {x} _{\ell })}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>m</mi>
<mo stretchy="false">~<!-- ~ --></mo>
</mover>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>M</mi>
<mo>,</mo>
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<msub>
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<msub>
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>M</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
<mo>=</mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mrow>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</munderover>
<msub>
<mi>Y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<msub>
<mi>K</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>M</mi>
<mo>,</mo>
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mrow>
<mrow>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>ℓ<!-- ℓ --></mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</munderover>
<msub>
<mi>K</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>M</mi>
<mo>,</mo>
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>ℓ<!-- ℓ --></mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mrow>
</mfrac>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\tilde {m}}_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})={\frac {\sum _{i=1}^{n}Y_{i}K_{M,n}(\mathbf {x} ,\mathbf {x} _{i})}{\sum _{\ell =1}^{n}K_{M,n}(\mathbf {x} ,\mathbf {x} _{\ell })}}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\tilde {m}}_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})={\frac {\sum _{i=1}^{n}Y_{i}K_{M,n}(\mathbf {x} ,\mathbf {x} _{i})}{\sum _{\ell =1}^{n}K_{M,n}(\mathbf {x} ,\mathbf {x} _{\ell })}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7ea4edc4c4ad2b30acfe0b5898862397424612eb" style="vertical-align: -2.838ex; width:45.172ex; height:6.843ex;"/></span>, which defines the KeRF.
</p>
<h4><span class="mw-headline" id="Centered_KeRF">Centered KeRF</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Random_forest&action=edit&section=18" title="Edit section: Centered KeRF">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>The construction of Centered KeRF of level <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle k}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>k</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle k}</annotation>
</semantics>
</math></span><img alt="k" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c3c9a2c7b599b37105512c5d570edc034056dd40" style="vertical-align: -0.338ex; width:1.211ex; height:2.176ex;"/></span> is the same as for centered forest, except that predictions are made by <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\tilde {m}}_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>m</mi>
<mo stretchy="false">~<!-- ~ --></mo>
</mover>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>M</mi>
<mo>,</mo>
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<msub>
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<msub>
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>M</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\tilde {m}}_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})}</annotation>
</semantics>
</math></span><img alt="{\tilde  {m}}_{{M,n}}({\mathbf  {x}},\Theta _{1},\ldots ,\Theta _{M})" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3939edfe84d8ece5969248f08b77f0b23843ad3e" style="vertical-align: -1.005ex; width:21.505ex; height:3.009ex;"/></span>, the corresponding kernel function, or connection function is
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\begin{aligned}K_{k}^{cc}(\mathbf {x} ,\mathbf {z} )=\sum _{k_{1},\ldots ,k_{d},\sum _{j=1}^{d}k_{j}=k}&{\frac {k!}{k_{1}!\cdots k_{d}!}}\left({\frac {1}{d}}\right)^{k}\prod _{j=1}^{d}\mathbf {1} _{\lceil 2^{k_{j}}x_{j}\rceil =\lceil 2^{k_{j}}z_{j}\rceil },\\&{\text{ for all }}\mathbf {x} ,\mathbf {z} \in [0,1]^{d}.\end{aligned}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt">
<mtr>
<mtd>
<msubsup>
<mi>K</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>k</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
<mi>c</mi>
</mrow>
</msubsup>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">z</mi>
</mrow>
<mo stretchy="false">)</mo>
<mo>=</mo>
<munder>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<msub>
<mi>k</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<msub>
<mi>k</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>d</mi>
</mrow>
</msub>
<mo>,</mo>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>d</mi>
</mrow>
</munderover>
<msub>
<mi>k</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo>=</mo>
<mi>k</mi>
</mrow>
</munder>
</mtd>
<mtd>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mrow>
<mi>k</mi>
<mo>!</mo>
</mrow>
<mrow>
<msub>
<mi>k</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo>!</mo>
<mo>⋯<!-- ⋯ --></mo>
<msub>
<mi>k</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>d</mi>
</mrow>
</msub>
<mo>!</mo>
</mrow>
</mfrac>
</mrow>
<msup>
<mrow>
<mo>(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>1</mn>
<mi>d</mi>
</mfrac>
</mrow>
<mo>)</mo>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>k</mi>
</mrow>
</msup>
<munderover>
<mo>∏<!-- ∏ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>d</mi>
</mrow>
</munderover>
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mn mathvariant="bold">1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mo fence="false" stretchy="false">⌈<!-- ⌈ --></mo>
<msup>
<mn>2</mn>
<mrow class="MJX-TeXAtom-ORD">
<msub>
<mi>k</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
</mrow>
</msup>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo fence="false" stretchy="false">⌉<!-- ⌉ --></mo>
<mo>=</mo>
<mo fence="false" stretchy="false">⌈<!-- ⌈ --></mo>
<msup>
<mn>2</mn>
<mrow class="MJX-TeXAtom-ORD">
<msub>
<mi>k</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
</mrow>
</msup>
<msub>
<mi>z</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo fence="false" stretchy="false">⌉<!-- ⌉ --></mo>
</mrow>
</msub>
<mo>,</mo>
</mtd>
</mtr>
<mtr>
<mtd></mtd>
<mtd>
<mrow class="MJX-TeXAtom-ORD">
<mtext> for all </mtext>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">z</mi>
</mrow>
<mo>∈<!-- ∈ --></mo>
<mo stretchy="false">[</mo>
<mn>0</mn>
<mo>,</mo>
<mn>1</mn>
<msup>
<mo stretchy="false">]</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>d</mi>
</mrow>
</msup>
<mo>.</mo>
</mtd>
</mtr>
</mtable>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\begin{aligned}K_{k}^{cc}(\mathbf {x} ,\mathbf {z} )=\sum _{k_{1},\ldots ,k_{d},\sum _{j=1}^{d}k_{j}=k}&{\frac {k!}{k_{1}!\cdots k_{d}!}}\left({\frac {1}{d}}\right)^{k}\prod _{j=1}^{d}\mathbf {1} _{\lceil 2^{k_{j}}x_{j}\rceil =\lceil 2^{k_{j}}z_{j}\rceil },\\&{\text{ for all }}\mathbf {x} ,\mathbf {z} \in [0,1]^{d}.\end{aligned}}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\begin{aligned}K_{k}^{cc}(\mathbf {x} ,\mathbf {z} )=\sum _{k_{1},\ldots ,k_{d},\sum _{j=1}^{d}k_{j}=k}&{\frac {k!}{k_{1}!\cdots k_{d}!}}\left({\frac {1}{d}}\right)^{k}\prod _{j=1}^{d}\mathbf {1} _{\lceil 2^{k_{j}}x_{j}\rceil =\lceil 2^{k_{j}}z_{j}\rceil },\\&{\text{ for all }}\mathbf {x} ,\mathbf {z} \in [0,1]^{d}.\end{aligned}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c5c5223ec3c01012f989e01ac149302fbc49161d" style="vertical-align: -5.166ex; margin-bottom: -0.172ex; width:62.773ex; height:11.843ex;"/></span></dd></dl>
<h4><span class="mw-headline" id="Uniform_KeRF">Uniform KeRF</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Random_forest&action=edit&section=19" title="Edit section: Uniform KeRF">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Uniform KeRF is built in the same way as uniform forest, except that predictions are made by <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\tilde {m}}_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>m</mi>
<mo stretchy="false">~<!-- ~ --></mo>
</mover>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>M</mi>
<mo>,</mo>
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<msub>
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<msub>
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>M</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\tilde {m}}_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})}</annotation>
</semantics>
</math></span><img alt="{\tilde  {m}}_{{M,n}}({\mathbf  {x}},\Theta _{1},\ldots ,\Theta _{M})" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3939edfe84d8ece5969248f08b77f0b23843ad3e" style="vertical-align: -1.005ex; width:21.505ex; height:3.009ex;"/></span>, the corresponding kernel function, or connection function is
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle K_{k}^{uf}(\mathbf {0} ,\mathbf {x} )=\sum _{k_{1},\ldots ,k_{d},\sum _{j=1}^{d}k_{j}=k}{\frac {k!}{k_{1}!\ldots k_{d}!}}\left({\frac {1}{d}}\right)^{k}\prod _{m=1}^{d}\left(1-|x_{m}|\sum _{j=0}^{k_{m}-1}{\frac {(-\ln |x_{m}|)^{j}}{j!}}\right){\text{ for all }}\mathbf {x} \in [0,1]^{d}.}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msubsup>
<mi>K</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>k</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>u</mi>
<mi>f</mi>
</mrow>
</msubsup>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mn mathvariant="bold">0</mn>
</mrow>
<mo>,</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo stretchy="false">)</mo>
<mo>=</mo>
<munder>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<msub>
<mi>k</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<msub>
<mi>k</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>d</mi>
</mrow>
</msub>
<mo>,</mo>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>d</mi>
</mrow>
</munderover>
<msub>
<mi>k</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo>=</mo>
<mi>k</mi>
</mrow>
</munder>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mrow>
<mi>k</mi>
<mo>!</mo>
</mrow>
<mrow>
<msub>
<mi>k</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo>!</mo>
<mo>…<!-- … --></mo>
<msub>
<mi>k</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>d</mi>
</mrow>
</msub>
<mo>!</mo>
</mrow>
</mfrac>
</mrow>
<msup>
<mrow>
<mo>(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>1</mn>
<mi>d</mi>
</mfrac>
</mrow>
<mo>)</mo>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>k</mi>
</mrow>
</msup>
<munderover>
<mo>∏<!-- ∏ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>m</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>d</mi>
</mrow>
</munderover>
<mrow>
<mo>(</mo>
<mrow>
<mn>1</mn>
<mo>−<!-- − --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>m</mi>
</mrow>
</msub>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
<mo>=</mo>
<mn>0</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<msub>
<mi>k</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>m</mi>
</mrow>
</msub>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</munderover>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mrow>
<mo stretchy="false">(</mo>
<mo>−<!-- − --></mo>
<mi>ln</mi>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>m</mi>
</mrow>
</msub>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<msup>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msup>
</mrow>
<mrow>
<mi>j</mi>
<mo>!</mo>
</mrow>
</mfrac>
</mrow>
</mrow>
<mo>)</mo>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mtext> for all </mtext>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>∈<!-- ∈ --></mo>
<mo stretchy="false">[</mo>
<mn>0</mn>
<mo>,</mo>
<mn>1</mn>
<msup>
<mo stretchy="false">]</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>d</mi>
</mrow>
</msup>
<mo>.</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle K_{k}^{uf}(\mathbf {0} ,\mathbf {x} )=\sum _{k_{1},\ldots ,k_{d},\sum _{j=1}^{d}k_{j}=k}{\frac {k!}{k_{1}!\ldots k_{d}!}}\left({\frac {1}{d}}\right)^{k}\prod _{m=1}^{d}\left(1-|x_{m}|\sum _{j=0}^{k_{m}-1}{\frac {(-\ln |x_{m}|)^{j}}{j!}}\right){\text{ for all }}\mathbf {x} \in [0,1]^{d}.}</annotation>
</semantics>
</math></span><img alt="{\displaystyle K_{k}^{uf}(\mathbf {0} ,\mathbf {x} )=\sum _{k_{1},\ldots ,k_{d},\sum _{j=1}^{d}k_{j}=k}{\frac {k!}{k_{1}!\ldots k_{d}!}}\left({\frac {1}{d}}\right)^{k}\prod _{m=1}^{d}\left(1-|x_{m}|\sum _{j=0}^{k_{m}-1}{\frac {(-\ln |x_{m}|)^{j}}{j!}}\right){\text{ for all }}\mathbf {x} \in [0,1]^{d}.}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/579a334c0c00016a31e72f5376399c37a30a12b7" style="vertical-align: -4.338ex; width:96.713ex; height:8.676ex;"/></span></dd></dl>
<h3><span class="mw-headline" id="Properties_2">Properties</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Random_forest&action=edit&section=20" title="Edit section: Properties">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<h4><span class="mw-headline" id="Relation_between_KeRF_and_random_forest">Relation between KeRF and random forest</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Random_forest&action=edit&section=21" title="Edit section: Relation between KeRF and random forest">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Predictions given by KeRF and random forests are close if the number of points in each cell is controlled:
</p>
<blockquote>
<p>Assume that there exist sequences <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle (a_{n}),(b_{n})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mo stretchy="false">(</mo>
<msub>
<mi>a</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
<mo>,</mo>
<mo stretchy="false">(</mo>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle (a_{n}),(b_{n})}</annotation>
</semantics>
</math></span><img alt="{\displaystyle (a_{n}),(b_{n})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6d8157b9b64afbd714b16ed6b8de06c32afb696a" style="vertical-align: -0.838ex; width:9.317ex; height:2.843ex;"/></span> such that, almost surely,
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle a_{n}\leq N_{n}(\mathbf {x} ,\Theta )\leq b_{n}{\text{ and }}a_{n}\leq {\frac {1}{M}}\sum _{m=1}^{M}N_{n}{\mathbf {x} ,\Theta _{m}}\leq b_{n}.}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>a</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo>≤<!-- ≤ --></mo>
<msub>
<mi>N</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mo stretchy="false">)</mo>
<mo>≤<!-- ≤ --></mo>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mrow class="MJX-TeXAtom-ORD">
<mtext> and </mtext>
</mrow>
<msub>
<mi>a</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo>≤<!-- ≤ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>1</mn>
<mi>M</mi>
</mfrac>
</mrow>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>m</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>M</mi>
</mrow>
</munderover>
<msub>
<mi>N</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<msub>
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>m</mi>
</mrow>
</msub>
</mrow>
<mo>≤<!-- ≤ --></mo>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo>.</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle a_{n}\leq N_{n}(\mathbf {x} ,\Theta )\leq b_{n}{\text{ and }}a_{n}\leq {\frac {1}{M}}\sum _{m=1}^{M}N_{n}{\mathbf {x} ,\Theta _{m}}\leq b_{n}.}</annotation>
</semantics>
</math></span><img alt="{\displaystyle a_{n}\leq N_{n}(\mathbf {x} ,\Theta )\leq b_{n}{\text{ and }}a_{n}\leq {\frac {1}{M}}\sum _{m=1}^{M}N_{n}{\mathbf {x} ,\Theta _{m}}\leq b_{n}.}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/139dc876ec018ffab1c1d499f269b95a6e8c245a" style="vertical-align: -3.005ex; width:53.034ex; height:7.343ex;"/></span></dd></dl>
<p>Then almost surely,
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle |m_{M,n}(\mathbf {x} )-{\tilde {m}}_{M,n}(\mathbf {x} )|\leq {\frac {b_{n}-a_{n}}{a_{n}}}{\tilde {m}}_{M,n}(\mathbf {x} ).}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<msub>
<mi>m</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>M</mi>
<mo>,</mo>
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo stretchy="false">)</mo>
<mo>−<!-- − --></mo>
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>m</mi>
<mo stretchy="false">~<!-- ~ --></mo>
</mover>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>M</mi>
<mo>,</mo>
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<mo>≤<!-- ≤ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mrow>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo>−<!-- − --></mo>
<msub>
<mi>a</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
</mrow>
<msub>
<mi>a</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
</mfrac>
</mrow>
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>m</mi>
<mo stretchy="false">~<!-- ~ --></mo>
</mover>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>M</mi>
<mo>,</mo>
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo stretchy="false">)</mo>
<mo>.</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle |m_{M,n}(\mathbf {x} )-{\tilde {m}}_{M,n}(\mathbf {x} )|\leq {\frac {b_{n}-a_{n}}{a_{n}}}{\tilde {m}}_{M,n}(\mathbf {x} ).}</annotation>
</semantics>
</math></span><img alt="{\displaystyle |m_{M,n}(\mathbf {x} )-{\tilde {m}}_{M,n}(\mathbf {x} )|\leq {\frac {b_{n}-a_{n}}{a_{n}}}{\tilde {m}}_{M,n}(\mathbf {x} ).}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ad592965613268c7026f8f57701caba977fd34ed" style="vertical-align: -2.171ex; width:42.21ex; height:5.676ex;"/></span></dd></dl>
</blockquote>
<h4><span class="mw-headline" id="Relation_between_infinite_KeRF_and_infinite_random_forest">Relation between infinite KeRF and infinite random forest</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Random_forest&action=edit&section=22" title="Edit section: Relation between infinite KeRF and infinite random forest">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>When the number of trees <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle M}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>M</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle M}</annotation>
</semantics>
</math></span><img alt="M" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f82cade9898ced02fdd08712e5f0c0151758a0dd" style="vertical-align: -0.338ex; width:2.442ex; height:2.176ex;"/></span> goes to infinity, then we have infinite random forest and infinite KeRF. Their estimates are close if the number of observations in each cell is bounded:
</p>
<blockquote>
<p>Assume that there exist sequences <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle (\varepsilon _{n}),(a_{n}),(b_{n})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mo stretchy="false">(</mo>
<msub>
<mi>ε<!-- ε --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
<mo>,</mo>
<mo stretchy="false">(</mo>
<msub>
<mi>a</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
<mo>,</mo>
<mo stretchy="false">(</mo>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle (\varepsilon _{n}),(a_{n}),(b_{n})}</annotation>
</semantics>
</math></span><img alt="{\displaystyle (\varepsilon _{n}),(a_{n}),(b_{n})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/67e5f769151263bad5a8d716a2c6af2686687855" style="vertical-align: -0.838ex; width:14.462ex; height:2.843ex;"/></span> such that, almost surely
</p>
<ul><li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \operatorname {E} [N_{n}(\mathbf {x} ,\Theta )]\geq 1,}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<msub>
<mi>N</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mo stretchy="false">)</mo>
<mo stretchy="false">]</mo>
<mo>≥<!-- ≥ --></mo>
<mn>1</mn>
<mo>,</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \operatorname {E} [N_{n}(\mathbf {x} ,\Theta )]\geq 1,}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \operatorname {E} [N_{n}(\mathbf {x} ,\Theta )]\geq 1,}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/07f5beb5ef7860bc9f527fb5b01fbed3dc0fc716" style="vertical-align: -0.838ex; width:16.931ex; height:2.843ex;"/></span></li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \operatorname {P} [a_{n}\leq N_{n}(\mathbf {x} ,\Theta )\leq b_{n}\mid {\mathcal {D}}_{n}]\geq 1-\varepsilon _{n}/2,}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi mathvariant="normal">P</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<msub>
<mi>a</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo>≤<!-- ≤ --></mo>
<msub>
<mi>N</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mo stretchy="false">)</mo>
<mo>≤<!-- ≤ --></mo>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo>∣<!-- ∣ --></mo>
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">D</mi>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">]</mo>
<mo>≥<!-- ≥ --></mo>
<mn>1</mn>
<mo>−<!-- − --></mo>
<msub>
<mi>ε<!-- ε --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mrow class="MJX-TeXAtom-ORD">
<mo>/</mo>
</mrow>
<mn>2</mn>
<mo>,</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \operatorname {P} [a_{n}\leq N_{n}(\mathbf {x} ,\Theta )\leq b_{n}\mid {\mathcal {D}}_{n}]\geq 1-\varepsilon _{n}/2,}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \operatorname {P} [a_{n}\leq N_{n}(\mathbf {x} ,\Theta )\leq b_{n}\mid {\mathcal {D}}_{n}]\geq 1-\varepsilon _{n}/2,}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4bad78155a4139322247009072e48d6b7bf8c5b9" style="vertical-align: -0.838ex; width:40.207ex; height:2.843ex;"/></span></li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \operatorname {P} [a_{n}\leq \operatorname {E} _{\Theta }[N_{n}(\mathbf {x} ,\Theta )]\leq b_{n}\mid {\mathcal {D}}_{n}]\geq 1-\varepsilon _{n}/2,}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi mathvariant="normal">P</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<msub>
<mi>a</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo>≤<!-- ≤ --></mo>
<msub>
<mi mathvariant="normal">E</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="normal">Θ<!-- Θ --></mi>
</mrow>
</msub>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<msub>
<mi>N</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo>,</mo>
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mo stretchy="false">)</mo>
<mo stretchy="false">]</mo>
<mo>≤<!-- ≤ --></mo>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo>∣<!-- ∣ --></mo>
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">D</mi>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">]</mo>
<mo>≥<!-- ≥ --></mo>
<mn>1</mn>
<mo>−<!-- − --></mo>
<msub>
<mi>ε<!-- ε --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mrow class="MJX-TeXAtom-ORD">
<mo>/</mo>
</mrow>
<mn>2</mn>
<mo>,</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \operatorname {P} [a_{n}\leq \operatorname {E} _{\Theta }[N_{n}(\mathbf {x} ,\Theta )]\leq b_{n}\mid {\mathcal {D}}_{n}]\geq 1-\varepsilon _{n}/2,}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \operatorname {P} [a_{n}\leq \operatorname {E} _{\Theta }[N_{n}(\mathbf {x} ,\Theta )]\leq b_{n}\mid {\mathcal {D}}_{n}]\geq 1-\varepsilon _{n}/2,}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/71711062ff59f3d2075162074e8f94f917a0cdad" style="vertical-align: -0.838ex; width:44.595ex; height:2.843ex;"/></span></li></ul>
<p>Then almost surely,
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle |m_{\infty ,n}(\mathbf {x} )-{\tilde {m}}_{\infty ,n}(\mathbf {x} )|\leq {\frac {b_{n}-a_{n}}{a_{n}}}{\tilde {m}}_{\infty ,n}(\mathbf {x} )+n\varepsilon _{n}\left(\max _{1\leq i\leq n}Y_{i}\right).}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<msub>
<mi>m</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="normal">∞<!-- ∞ --></mi>
<mo>,</mo>
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo stretchy="false">)</mo>
<mo>−<!-- − --></mo>
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>m</mi>
<mo stretchy="false">~<!-- ~ --></mo>
</mover>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="normal">∞<!-- ∞ --></mi>
<mo>,</mo>
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<mo>≤<!-- ≤ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mrow>
<msub>
<mi>b</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo>−<!-- − --></mo>
<msub>
<mi>a</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
</mrow>
<msub>
<mi>a</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
</mfrac>
</mrow>
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>m</mi>
<mo stretchy="false">~<!-- ~ --></mo>
</mover>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="normal">∞<!-- ∞ --></mi>
<mo>,</mo>
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">x</mi>
</mrow>
<mo stretchy="false">)</mo>
<mo>+</mo>
<mi>n</mi>
<msub>
<mi>ε<!-- ε --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mrow>
<mo>(</mo>
<mrow>
<munder>
<mo form="prefix" movablelimits="true">max</mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
<mo>≤<!-- ≤ --></mo>
<mi>i</mi>
<mo>≤<!-- ≤ --></mo>
<mi>n</mi>
</mrow>
</munder>
<msub>
<mi>Y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
</mrow>
<mo>)</mo>
</mrow>
<mo>.</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle |m_{\infty ,n}(\mathbf {x} )-{\tilde {m}}_{\infty ,n}(\mathbf {x} )|\leq {\frac {b_{n}-a_{n}}{a_{n}}}{\tilde {m}}_{\infty ,n}(\mathbf {x} )+n\varepsilon _{n}\left(\max _{1\leq i\leq n}Y_{i}\right).}</annotation>
</semantics>
</math></span><img alt="{\displaystyle |m_{\infty ,n}(\mathbf {x} )-{\tilde {m}}_{\infty ,n}(\mathbf {x} )|\leq {\frac {b_{n}-a_{n}}{a_{n}}}{\tilde {m}}_{\infty ,n}(\mathbf {x} )+n\varepsilon _{n}\left(\max _{1\leq i\leq n}Y_{i}\right).}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/26197c8d5ca305f9a7a922c4227c5daa11350e76" style="vertical-align: -2.505ex; width:60.162ex; height:6.176ex;"/></span></dd></dl>
</blockquote>
<h3><span class="mw-headline" id="Consistency_results">Consistency results</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Random_forest&action=edit&section=23" title="Edit section: Consistency results">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Assume that <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle Y=m(\mathbf {X} )+\varepsilon }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>Y</mi>
<mo>=</mo>
<mi>m</mi>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">X</mi>
</mrow>
<mo stretchy="false">)</mo>
<mo>+</mo>
<mi>ε<!-- ε --></mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle Y=m(\mathbf {X} )+\varepsilon }</annotation>
</semantics>
</math></span><img alt="Y=m({\mathbf  {X}})+\varepsilon " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d8617bcdd783f680a6c45c9d8c627a5730a878e5" style="vertical-align: -0.838ex; width:14.665ex; height:2.843ex;"/></span>, where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \varepsilon }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>ε<!-- ε --></mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \varepsilon }</annotation>
</semantics>
</math></span><img alt="\varepsilon " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a30c89172e5b88edbd45d3e2772c7f5e562e5173" style="vertical-align: -0.338ex; width:1.083ex; height:1.676ex;"/></span> is a centered Gaussian noise, independent of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbf {X} }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">X</mi>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbf {X} }</annotation>
</semantics>
</math></span><img alt="\mathbf {X} " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9f75966a2f9d5672136fa9401ee1e75008f95ffd" style="vertical-align: -0.338ex; width:2.019ex; height:2.176ex;"/></span>, with finite variance <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \sigma ^{2}<\infty }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msup>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo><</mo>
<mi mathvariant="normal">∞<!-- ∞ --></mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \sigma ^{2}<\infty }</annotation>
</semantics>
</math></span><img alt="\sigma ^{2}<\infty " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f9c01c50d5825dbb5b11add552a422af72a9664f" style="vertical-align: -0.338ex; width:7.807ex; height:2.676ex;"/></span>. Moreover, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbf {X} }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">X</mi>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbf {X} }</annotation>
</semantics>
</math></span><img alt="\mathbf {X} " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9f75966a2f9d5672136fa9401ee1e75008f95ffd" style="vertical-align: -0.338ex; width:2.019ex; height:2.176ex;"/></span> is uniformly distributed on <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle [0,1]^{d}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mo stretchy="false">[</mo>
<mn>0</mn>
<mo>,</mo>
<mn>1</mn>
<msup>
<mo stretchy="false">]</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>d</mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle [0,1]^{d}}</annotation>
</semantics>
</math></span><img alt="[0,1]^{d}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e13ae4917276744b214714a20b3cb8ee305e309d" style="vertical-align: -0.838ex; width:5.745ex; height:3.176ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle m}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>m</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle m}</annotation>
</semantics>
</math></span><img alt="m" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0a07d98bb302f3856cbabc47b2b9016692e3f7bc" style="vertical-align: -0.338ex; width:2.04ex; height:1.676ex;"/></span> is <a href="/wiki/Lipschitz" title="Lipschitz">Lipschitz</a>. Scornet<sup class="reference" id="cite_ref-scornet2015random_27-3"><a href="#cite_note-scornet2015random-27">[27]</a></sup> proved upper bounds on the rates of consistency for centered KeRF and uniform KeRF.
</p>
<h4><span class="mw-headline" id="Consistency_of_centered_KeRF">Consistency of centered KeRF</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Random_forest&action=edit&section=24" title="Edit section: Consistency of centered KeRF">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Providing <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle k\rightarrow \infty }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>k</mi>
<mo stretchy="false">→<!-- → --></mo>
<mi mathvariant="normal">∞<!-- ∞ --></mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle k\rightarrow \infty }</annotation>
</semantics>
</math></span><img alt="k\rightarrow\infty" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/612a3ec99f1c9f12de1cfab011e306ae799858ce" style="vertical-align: -0.338ex; width:7.149ex; height:2.176ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle n/2^{k}\rightarrow \infty }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>n</mi>
<mrow class="MJX-TeXAtom-ORD">
<mo>/</mo>
</mrow>
<msup>
<mn>2</mn>
<mrow class="MJX-TeXAtom-ORD">
<mi>k</mi>
</mrow>
</msup>
<mo stretchy="false">→<!-- → --></mo>
<mi mathvariant="normal">∞<!-- ∞ --></mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle n/2^{k}\rightarrow \infty }</annotation>
</semantics>
</math></span><img alt="n/2^{k}\rightarrow \infty " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9fbec163fbc2ac671e4c9f78bdc70a985599dc19" style="vertical-align: -0.838ex; width:10.746ex; height:3.176ex;"/></span>, there exists a constant <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle C_{1}>0}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>C</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo>></mo>
<mn>0</mn>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle C_{1}>0}</annotation>
</semantics>
</math></span><img alt="C_{1}>0" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c7f2ff302742806f4db7411f29e54944b4d9bda7" style="vertical-align: -0.671ex; width:6.977ex; height:2.509ex;"/></span> such that, for all <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle n}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>n</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle n}</annotation>
</semantics>
</math></span><img alt="n" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b" style="vertical-align: -0.338ex; width:1.395ex; height:1.676ex;"/></span>,
<span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbb {E} [{\tilde {m}}_{n}^{cc}(\mathbf {X} )-m(\mathbf {X} )]^{2}\leq C_{1}n^{-1/(3+d\log 2)}(\log n)^{2}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">E</mi>
</mrow>
<mo stretchy="false">[</mo>
<msubsup>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>m</mi>
<mo stretchy="false">~<!-- ~ --></mo>
</mover>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
<mi>c</mi>
</mrow>
</msubsup>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">X</mi>
</mrow>
<mo stretchy="false">)</mo>
<mo>−<!-- − --></mo>
<mi>m</mi>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">X</mi>
</mrow>
<mo stretchy="false">)</mo>
<msup>
<mo stretchy="false">]</mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo>≤<!-- ≤ --></mo>
<msub>
<mi>C</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<msup>
<mi>n</mi>
<mrow class="MJX-TeXAtom-ORD">
<mo>−<!-- − --></mo>
<mn>1</mn>
<mrow class="MJX-TeXAtom-ORD">
<mo>/</mo>
</mrow>
<mo stretchy="false">(</mo>
<mn>3</mn>
<mo>+</mo>
<mi>d</mi>
<mi>log</mi>
<mo>⁡<!-- ⁡ --></mo>
<mn>2</mn>
<mo stretchy="false">)</mo>
</mrow>
</msup>
<mo stretchy="false">(</mo>
<mi>log</mi>
<mo>⁡<!-- ⁡ --></mo>
<mi>n</mi>
<msup>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbb {E} [{\tilde {m}}_{n}^{cc}(\mathbf {X} )-m(\mathbf {X} )]^{2}\leq C_{1}n^{-1/(3+d\log 2)}(\log n)^{2}}</annotation>
</semantics>
</math></span><img alt="{\mathbb  {E}}[{\tilde  {m}}_{n}^{{cc}}({\mathbf  {X}})-m({\mathbf  {X}})]^{2}\leq C_{1}n^{{-1/(3+d\log 2)}}(\log n)^{2}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/bbd76dd80031cfdb1b8042faeab38ecaafefa2e3" style="vertical-align: -0.838ex; width:46.051ex; height:3.343ex;"/></span>.
</p>
<h4><span class="mw-headline" id="Consistency_of_uniform_KeRF">Consistency of uniform KeRF</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Random_forest&action=edit&section=25" title="Edit section: Consistency of uniform KeRF">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Providing <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle k\rightarrow \infty }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>k</mi>
<mo stretchy="false">→<!-- → --></mo>
<mi mathvariant="normal">∞<!-- ∞ --></mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle k\rightarrow \infty }</annotation>
</semantics>
</math></span><img alt="k\rightarrow\infty" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/612a3ec99f1c9f12de1cfab011e306ae799858ce" style="vertical-align: -0.338ex; width:7.149ex; height:2.176ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle n/2^{k}\rightarrow \infty }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>n</mi>
<mrow class="MJX-TeXAtom-ORD">
<mo>/</mo>
</mrow>
<msup>
<mn>2</mn>
<mrow class="MJX-TeXAtom-ORD">
<mi>k</mi>
</mrow>
</msup>
<mo stretchy="false">→<!-- → --></mo>
<mi mathvariant="normal">∞<!-- ∞ --></mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle n/2^{k}\rightarrow \infty }</annotation>
</semantics>
</math></span><img alt="n/2^{k}\rightarrow \infty " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9fbec163fbc2ac671e4c9f78bdc70a985599dc19" style="vertical-align: -0.838ex; width:10.746ex; height:3.176ex;"/></span>, there exists a constant <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle C>0}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>C</mi>
<mo>></mo>
<mn>0</mn>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle C>0}</annotation>
</semantics>
</math></span><img alt="C>0" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c84d4126c6df243734f9355927c026df6b0d3859" style="vertical-align: -0.338ex; width:6.027ex; height:2.176ex;"/></span> such that,
<span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbb {E} [{\tilde {m}}_{n}^{uf}(\mathbf {X} )-m(\mathbf {X} )]^{2}\leq Cn^{-2/(6+3d\log 2)}(\log n)^{2}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">E</mi>
</mrow>
<mo stretchy="false">[</mo>
<msubsup>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>m</mi>
<mo stretchy="false">~<!-- ~ --></mo>
</mover>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>u</mi>
<mi>f</mi>
</mrow>
</msubsup>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">X</mi>
</mrow>
<mo stretchy="false">)</mo>
<mo>−<!-- − --></mo>
<mi>m</mi>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">X</mi>
</mrow>
<mo stretchy="false">)</mo>
<msup>
<mo stretchy="false">]</mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo>≤<!-- ≤ --></mo>
<mi>C</mi>
<msup>
<mi>n</mi>
<mrow class="MJX-TeXAtom-ORD">
<mo>−<!-- − --></mo>
<mn>2</mn>
<mrow class="MJX-TeXAtom-ORD">
<mo>/</mo>
</mrow>
<mo stretchy="false">(</mo>
<mn>6</mn>
<mo>+</mo>
<mn>3</mn>
<mi>d</mi>
<mi>log</mi>
<mo>⁡<!-- ⁡ --></mo>
<mn>2</mn>
<mo stretchy="false">)</mo>
</mrow>
</msup>
<mo stretchy="false">(</mo>
<mi>log</mi>
<mo>⁡<!-- ⁡ --></mo>
<mi>n</mi>
<msup>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbb {E} [{\tilde {m}}_{n}^{uf}(\mathbf {X} )-m(\mathbf {X} )]^{2}\leq Cn^{-2/(6+3d\log 2)}(\log n)^{2}}</annotation>
</semantics>
</math></span><img alt="{\mathbb  {E}}[{\tilde  {m}}_{n}^{{uf}}({\mathbf  {X}})-m({\mathbf  {X}})]^{2}\leq Cn^{{-2/(6+3d\log 2)}}(\log n)^{2}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/51b3c9f4980f936fc01a73621cc56e9857059da4" style="vertical-align: -0.838ex; width:46.344ex; height:3.343ex;"/></span>.
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Random_forest&action=edit&section=26" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a></li>
<li><a href="/wiki/Decision_tree_learning" title="Decision tree learning">Decision tree learning</a></li>
<li><a href="/wiki/Ensemble_learning" title="Ensemble learning">Ensemble learning</a></li>
<li><a href="/wiki/Gradient_boosting" title="Gradient boosting">Gradient boosting</a></li>
<li><a class="mw-redirect" href="/wiki/Non-parametric_statistics" title="Non-parametric statistics">Non-parametric statistics</a></li>
<li><a href="/wiki/Randomized_algorithm" title="Randomized algorithm">Randomized algorithm</a></li></ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Random_forest&action=edit&section=27" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist columns references-column-width" style="-moz-column-width: 32em; -webkit-column-width: 32em; column-width: 32em; list-style-type: decimal;">
<ol class="references">
<li id="cite_note-ho1995-1"><span class="mw-cite-backlink">^ <a href="#cite_ref-ho1995_1-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-ho1995_1-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-ho1995_1-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-ho1995_1-3"><sup><i><b>d</b></i></sup></a></span> <span class="reference-text"><cite class="citation conference cs1" id="CITEREFHo1995">Ho, Tin Kam (1995). <a class="external text" href="https://web.archive.org/web/20160417030218/http://ect.bell-labs.com/who/tkh/publications/papers/odt.pdf" rel="nofollow"><i>Random Decision Forests</i></a> <span class="cs1-format">(PDF)</span>. Proceedings of the 3rd International Conference on Document Analysis and Recognition, Montreal, QC, 14–16 August 1995. pp. 278–282. Archived from <a class="external text" href="http://ect.bell-labs.com/who/tkh/publications/papers/odt.pdf" rel="nofollow">the original</a> <span class="cs1-format">(PDF)</span> on 17 April 2016<span class="reference-accessdate">. Retrieved <span class="nowrap">5 June</span> 2016</span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&rft.genre=conference&rft.btitle=Random+Decision+Forests&rft.pages=278-282&rft.date=1995&rft.aulast=Ho&rft.aufirst=Tin+Kam&rft_id=http%3A%2F%2Fect.bell-labs.com%2Fwho%2Ftkh%2Fpublications%2Fpapers%2Fodt.pdf&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><style data-mw-deduplicate="TemplateStyles:r951705291">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background-image:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png");background-image:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg");background-repeat:no-repeat;background-size:9px;background-position:right .1em center}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background-image:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png");background-image:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg");background-repeat:no-repeat;background-size:9px;background-position:right .1em center}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background-image:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png");background-image:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg");background-repeat:no-repeat;background-size:9px;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background-image:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png");background-image:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg");background-repeat:no-repeat;background-size:12px;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}</style></span>
</li>
<li id="cite_note-ho1998-2"><span class="mw-cite-backlink">^ <a href="#cite_ref-ho1998_2-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-ho1998_2-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-ho1998_2-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-ho1998_2-3"><sup><i><b>d</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFHo1998">Ho TK (1998). <a class="external text" href="http://ect.bell-labs.com/who/tkh/publications/papers/df.pdf" rel="nofollow">"The Random Subspace Method for Constructing Decision Forests"</a> <span class="cs1-format">(PDF)</span>. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>. <b>20</b> (8): 832–844. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1109%2F34.709601" rel="nofollow">10.1109/34.709601</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=IEEE+Transactions+on+Pattern+Analysis+and+Machine+Intelligence&rft.atitle=The+Random+Subspace+Method+for+Constructing+Decision+Forests&rft.volume=20&rft.issue=8&rft.pages=832-844&rft.date=1998&rft_id=info%3Adoi%2F10.1109%2F34.709601&rft.aulast=Ho&rft.aufirst=Tin+Kam&rft_id=http%3A%2F%2Fect.bell-labs.com%2Fwho%2Ftkh%2Fpublications%2Fpapers%2Fdf.pdf&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-elemstatlearn-3"><span class="mw-cite-backlink">^ <a href="#cite_ref-elemstatlearn_3-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-elemstatlearn_3-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-elemstatlearn_3-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-elemstatlearn_3-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-elemstatlearn_3-4"><sup><i><b>e</b></i></sup></a> <a href="#cite_ref-elemstatlearn_3-5"><sup><i><b>f</b></i></sup></a></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFHastieTibshiraniFriedman2008"><a href="/wiki/Trevor_Hastie" title="Trevor Hastie">Hastie, Trevor</a>; <a href="/wiki/Robert_Tibshirani" title="Robert Tibshirani">Tibshirani, Robert</a>; <a href="/wiki/Jerome_H._Friedman" title="Jerome H. Friedman">Friedman, Jerome</a> (2008). <a class="external text" href="http://www-stat.stanford.edu/~tibs/ElemStatLearn/" rel="nofollow"><i>The Elements of Statistical Learning</i></a> (2nd ed.). Springer. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a> <a href="/wiki/Special:BookSources/0-387-95284-5" title="Special:BookSources/0-387-95284-5"><bdi>0-387-95284-5</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&rft.genre=book&rft.btitle=The+Elements+of+Statistical+Learning&rft.edition=2nd&rft.pub=Springer&rft.date=2008&rft.isbn=0-387-95284-5&rft.aulast=Hastie&rft.aufirst=Trevor&rft.au=Tibshirani%2C+Robert&rft.au=Friedman%2C+Jerome&rft_id=http%3A%2F%2Fwww-stat.stanford.edu%2F~tibs%2FElemStatLearn%2F&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-kleinberg1990-4"><span class="mw-cite-backlink">^ <a href="#cite_ref-kleinberg1990_4-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-kleinberg1990_4-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFKleinberg1990">Kleinberg E (1990). <a class="external text" href="https://pdfs.semanticscholar.org/faa4/c502a824a9d64bf3dc26eb90a2c32367921f.pdf" rel="nofollow">"Stochastic Discrimination"</a> <span class="cs1-format">(PDF)</span>. <i><a class="new" href="/w/index.php?title=Annals_of_Mathematics_and_Artificial_Intelligence&action=edit&redlink=1" title="Annals of Mathematics and Artificial Intelligence (page does not exist)">Annals of Mathematics and Artificial Intelligence</a></i>. <b>1</b> (1–4): 207–239. <a class="mw-redirect" href="/wiki/CiteSeerX_(identifier)" title="CiteSeerX (identifier)">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.25.6750" rel="nofollow">10.1.1.25.6750</a></span>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1007%2FBF01531079" rel="nofollow">10.1007/BF01531079</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Annals+of+Mathematics+and+Artificial+Intelligence&rft.atitle=Stochastic+Discrimination&rft.volume=1&rft.issue=1%E2%80%934&rft.pages=207-239&rft.date=1990&rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.25.6750&rft_id=info%3Adoi%2F10.1007%2FBF01531079&rft.aulast=Kleinberg&rft.aufirst=Eugene&rft_id=https%3A%2F%2Fpdfs.semanticscholar.org%2Ffaa4%2Fc502a824a9d64bf3dc26eb90a2c32367921f.pdf&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-kleinberg1996-5"><span class="mw-cite-backlink">^ <a href="#cite_ref-kleinberg1996_5-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-kleinberg1996_5-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFKleinberg1996">Kleinberg E (1996). <a class="external text" href="https://doi.org/10.1214/aos/1032181157" rel="nofollow">"An Overtraining-Resistant Stochastic Modeling Method for Pattern Recognition"</a>. <i><a href="/wiki/Annals_of_Statistics" title="Annals of Statistics">Annals of Statistics</a></i>. <b>24</b> (6): 2319–2349. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="https://doi.org/10.1214%2Faos%2F1032181157" rel="nofollow">10.1214/aos/1032181157</a></span>. <a class="mw-redirect" href="/wiki/MR_(identifier)" title="MR (identifier)">MR</a> <a class="external text" href="//www.ams.org/mathscinet-getitem?mr=1425956" rel="nofollow">1425956</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Annals+of+Statistics&rft.atitle=An+Overtraining-Resistant+Stochastic+Modeling+Method+for+Pattern+Recognition&rft.volume=24&rft.issue=6&rft.pages=2319-2349&rft.date=1996&rft_id=info%3Adoi%2F10.1214%2Faos%2F1032181157&rft_id=%2F%2Fwww.ams.org%2Fmathscinet-getitem%3Fmr%3D1425956&rft.aulast=Kleinberg&rft.aufirst=Eugene&rft_id=%2F%2Fdoi.org%2F10.1214%2Faos%2F1032181157&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-kleinberg2000-6"><span class="mw-cite-backlink">^ <a href="#cite_ref-kleinberg2000_6-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-kleinberg2000_6-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFKleinberg2000">Kleinberg E (2000). <a class="external text" href="https://pdfs.semanticscholar.org/8956/845b0701ec57094c7a8b4ab1f41386899aea.pdf" rel="nofollow">"On the Algorithmic Implementation of Stochastic Discrimination"</a> <span class="cs1-format">(PDF)</span>. <i>IEEE Transactions on PAMI</i>. <b>22</b> (5): 473–490. <a class="mw-redirect" href="/wiki/CiteSeerX_(identifier)" title="CiteSeerX (identifier)">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.33.4131" rel="nofollow">10.1.1.33.4131</a></span>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1109%2F34.857004" rel="nofollow">10.1109/34.857004</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=IEEE+Transactions+on+PAMI&rft.atitle=On+the+Algorithmic+Implementation+of+Stochastic+Discrimination&rft.volume=22&rft.issue=5&rft.pages=473-490&rft.date=2000&rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.33.4131&rft_id=info%3Adoi%2F10.1109%2F34.857004&rft.aulast=Kleinberg&rft.aufirst=Eugene&rft_id=https%3A%2F%2Fpdfs.semanticscholar.org%2F8956%2F845b0701ec57094c7a8b4ab1f41386899aea.pdf&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-breiman2001-7"><span class="mw-cite-backlink">^ <a href="#cite_ref-breiman2001_7-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-breiman2001_7-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-breiman2001_7-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-breiman2001_7-3"><sup><i><b>d</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFBreiman2001"><a href="/wiki/Leo_Breiman" title="Leo Breiman">Breiman L</a> (2001). <a class="external text" href="https://doi.org/10.1023/A:1010933404324" rel="nofollow">"Random Forests"</a>. <i><a href="/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">Machine Learning</a></i>. <b>45</b> (1): 5–32. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="https://doi.org/10.1023%2FA%3A1010933404324" rel="nofollow">10.1023/A:1010933404324</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Machine+Learning&rft.atitle=Random+Forests&rft.volume=45&rft.issue=1&rft.pages=5-32&rft.date=2001&rft_id=info%3Adoi%2F10.1023%2FA%3A1010933404324&rft.aulast=Breiman&rft.aufirst=Leo&rft_id=%2F%2Fdoi.org%2F10.1023%2FA%3A1010933404324&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-rpackage-8"><span class="mw-cite-backlink">^ <a href="#cite_ref-rpackage_8-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-rpackage_8-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation web cs1" id="CITEREFLiaw2012">Liaw A (16 October 2012). <a class="external text" href="https://cran.r-project.org/web/packages/randomForest/randomForest.pdf" rel="nofollow">"Documentation for R package randomForest"</a> <span class="cs1-format">(PDF)</span><span class="reference-accessdate">. Retrieved <span class="nowrap">15 March</span> 2013</span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&rft.genre=unknown&rft.btitle=Documentation+for+R+package+randomForest&rft.date=2012-10-16&rft.aulast=Liaw&rft.aufirst=Andy&rft_id=https%3A%2F%2Fcran.r-project.org%2Fweb%2Fpackages%2FrandomForest%2FrandomForest.pdf&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text">U.S. trademark registration number 3185828, registered 2006/12/19.</span>
</li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text"><cite class="citation web cs1"><a class="external text" href="https://trademarks.justia.com/786/42/random-78642027.html" rel="nofollow">"RANDOM FORESTS Trademark of Health Care Productivity, Inc. - Registration Number 3185828 - Serial Number 78642027 :: Justia Trademarks"</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&rft.genre=unknown&rft.btitle=RANDOM+FORESTS+Trademark+of+Health+Care+Productivity%2C+Inc.+-+Registration+Number+3185828+-+Serial+Number+78642027+%3A%3A+Justia+Trademarks&rft_id=https%3A%2F%2Ftrademarks.justia.com%2F786%2F42%2Frandom-78642027.html&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-amitgeman1997-11"><span class="mw-cite-backlink">^ <a href="#cite_ref-amitgeman1997_11-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-amitgeman1997_11-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFAmitGeman1997">Amit Y, <a href="/wiki/Donald_Geman" title="Donald Geman">Geman D</a> (1997). <a class="external text" href="http://www.cis.jhu.edu/publications/papers_in_database/GEMAN/shape.pdf" rel="nofollow">"Shape quantization and recognition with randomized trees"</a> <span class="cs1-format">(PDF)</span>. <i><a href="/wiki/Neural_Computation_(journal)" title="Neural Computation (journal)">Neural Computation</a></i>. <b>9</b> (7): 1545–1588. <a class="mw-redirect" href="/wiki/CiteSeerX_(identifier)" title="CiteSeerX (identifier)">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.57.6069" rel="nofollow">10.1.1.57.6069</a></span>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1162%2Fneco.1997.9.7.1545" rel="nofollow">10.1162/neco.1997.9.7.1545</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Neural+Computation&rft.atitle=Shape+quantization+and+recognition+with+randomized+trees&rft.volume=9&rft.issue=7&rft.pages=1545-1588&rft.date=1997&rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.57.6069&rft_id=info%3Adoi%2F10.1162%2Fneco.1997.9.7.1545&rft.aulast=Amit&rft.aufirst=Yali&rft.au=Geman%2C+Donald&rft_id=http%3A%2F%2Fwww.cis.jhu.edu%2Fpublications%2Fpapers_in_database%2FGEMAN%2Fshape.pdf&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFDietterich2000">Dietterich, Thomas (2000). <a class="external text" href="https://doi.org/10.1023/A:1007607513941" rel="nofollow">"An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization"</a>. <i><a href="/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">Machine Learning</a></i>. <b>40</b> (2): 139–157. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="https://doi.org/10.1023%2FA%3A1007607513941" rel="nofollow">10.1023/A:1007607513941</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Machine+Learning&rft.atitle=An+Experimental+Comparison+of+Three+Methods+for+Constructing+Ensembles+of+Decision+Trees%3A+Bagging%2C+Boosting%2C+and+Randomization&rft.volume=40&rft.issue=2&rft.pages=139-157&rft.date=2000&rft_id=info%3Adoi%2F10.1023%2FA%3A1007607513941&rft.aulast=Dietterich&rft.aufirst=Thomas&rft_id=%2F%2Fdoi.org%2F10.1023%2FA%3A1007607513941&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-islr-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-islr_13-0">^</a></b></span> <span class="reference-text"><cite class="citation book cs1" id="CITEREFGareth_JamesDaniela_WittenTrevor_HastieRobert_Tibshirani2013">Gareth James; Daniela Witten; Trevor Hastie; Robert Tibshirani (2013). <a class="external text" href="http://www-bcf.usc.edu/~gareth/ISL/" rel="nofollow"><i>An Introduction to Statistical Learning</i></a>. Springer. pp. 316–321.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&rft.genre=book&rft.btitle=An+Introduction+to+Statistical+Learning&rft.pages=316-321&rft.pub=Springer&rft.date=2013&rft.au=Gareth+James&rft.au=Daniela+Witten&rft.au=Trevor+Hastie&rft.au=Robert+Tibshirani&rft_id=http%3A%2F%2Fwww-bcf.usc.edu%2F~gareth%2FISL%2F&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-ho2002-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-ho2002_14-0">^</a></b></span> <span class="reference-text">
<cite class="citation journal cs1" id="CITEREFHo2002">Ho, Tin Kam (2002). <a class="external text" href="http://ect.bell-labs.com/who/tkh/publications/papers/compare.pdf" rel="nofollow">"A Data Complexity Analysis of Comparative Advantages of Decision Forest Constructors"</a> <span class="cs1-format">(PDF)</span>. <i>Pattern Analysis and Applications</i>. <b>5</b> (2): 102–112. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1007%2Fs100440200009" rel="nofollow">10.1007/s100440200009</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Pattern+Analysis+and+Applications&rft.atitle=A+Data+Complexity+Analysis+of+Comparative+Advantages+of+Decision+Forest+Constructors&rft.volume=5&rft.issue=2&rft.pages=102-112&rft.date=2002&rft_id=info%3Adoi%2F10.1007%2Fs100440200009&rft.aulast=Ho&rft.aufirst=Tin+Kam&rft_id=http%3A%2F%2Fect.bell-labs.com%2Fwho%2Ftkh%2Fpublications%2Fpapers%2Fcompare.pdf&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFGeurtsErnstWehenkel2006">Geurts P, Ernst D, Wehenkel L (2006). <a class="external text" href="http://orbi.ulg.ac.be/bitstream/2268/9357/1/geurts-mlj-advance.pdf" rel="nofollow">"Extremely randomized trees"</a> <span class="cs1-format">(PDF)</span>. <i>Machine Learning</i>. <b>63</b>: 3–42. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1007%2Fs10994-006-6226-1" rel="nofollow">10.1007/s10994-006-6226-1</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Machine+Learning&rft.atitle=Extremely+randomized+trees&rft.volume=63&rft.pages=3-42&rft.date=2006&rft_id=info%3Adoi%2F10.1007%2Fs10994-006-6226-1&rft.aulast=Geurts&rft.aufirst=P&rft.au=Ernst%2C+D&rft.au=Wehenkel%2C+L&rft_id=http%3A%2F%2Forbi.ulg.ac.be%2Fbitstream%2F2268%2F9357%2F1%2Fgeurts-mlj-advance.pdf&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFZhuZengKosorok2015">Zhu R, Zeng D, Kosorok MR (2015). <a class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC4760114" rel="nofollow">"Reinforcement Learning Trees"</a>. <i>Journal of the American Statistical Association</i>. <b>110</b> (512): 1770–1784. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1080%2F01621459.2015.1036994" rel="nofollow">10.1080/01621459.2015.1036994</a>. <a class="mw-redirect" href="/wiki/PMC_(identifier)" title="PMC (identifier)">PMC</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC4760114" rel="nofollow">4760114</a></span>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/26903687" rel="nofollow">26903687</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Journal+of+the+American+Statistical+Association&rft.atitle=Reinforcement+Learning+Trees&rft.volume=110&rft.issue=512&rft.pages=1770-1784&rft.date=2015&rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC4760114&rft_id=info%3Apmid%2F26903687&rft_id=info%3Adoi%2F10.1080%2F01621459.2015.1036994&rft.aulast=Zhu&rft.aufirst=R&rft.au=Zeng%2C+D&rft.au=Kosorok%2C+MR&rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC4760114&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-17">^</a></b></span> <span class="reference-text"><cite class="citation conference cs1" id="CITEREFDeng,H.Runger,_G.Tuv,_E.2011">Deng,H.; Runger, G.; Tuv, E. (2011). <a class="external text" href="https://www.researchgate.net/publication/221079908" rel="nofollow"><i>Bias of importance measures for multi-valued attributes and solutions</i></a>. Proceedings of the 21st International Conference on Artificial Neural Networks (ICANN). pp. 293–300.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&rft.genre=conference&rft.btitle=Bias+of+importance+measures+for+multi-valued+attributes+and+solutions&rft.pages=293-300&rft.date=2011&rft.au=Deng%2CH.&rft.au=Runger%2C+G.&rft.au=Tuv%2C+E.&rft_id=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F221079908&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-18">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFAltmannToloşiSanderLengauer2010">Altmann A, Toloşi L, Sander O, Lengauer T (May 2010). <a class="external text" href="http://bioinformatics.oxfordjournals.org/content/early/2010/04/12/bioinformatics.btq134.abstract" rel="nofollow">"Permutation importance: a corrected feature importance measure"</a>. <i>Bioinformatics</i>. <b>26</b> (10): 1340–7. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="https://doi.org/10.1093%2Fbioinformatics%2Fbtq134" rel="nofollow">10.1093/bioinformatics/btq134</a></span>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/20385727" rel="nofollow">20385727</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Bioinformatics&rft.atitle=Permutation+importance%3A+a+corrected+feature+importance+measure&rft.volume=26&rft.issue=10&rft.pages=1340-7&rft.date=2010-05&rft_id=info%3Adoi%2F10.1093%2Fbioinformatics%2Fbtq134&rft_id=info%3Apmid%2F20385727&rft.aulast=Altmann&rft.aufirst=A&rft.au=Tolo%C5%9Fi%2C+L&rft.au=Sander%2C+O&rft.au=Lengauer%2C+T&rft_id=http%3A%2F%2Fbioinformatics.oxfordjournals.org%2Fcontent%2Fearly%2F2010%2F04%2F12%2Fbioinformatics.btq134.abstract&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFStroblBoulesteixAugustin2007">Strobl C, Boulesteix A, Augustin T (2007). <a class="external text" href="https://epub.ub.uni-muenchen.de/1833/1/paper_464.pdf" rel="nofollow">"Unbiased split selection for classification trees based on the Gini index"</a> <span class="cs1-format">(PDF)</span>. <i>Computational Statistics & Data Analysis</i>. <b>52</b>: 483–501. <a class="mw-redirect" href="/wiki/CiteSeerX_(identifier)" title="CiteSeerX (identifier)">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.525.3178" rel="nofollow">10.1.1.525.3178</a></span>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1016%2Fj.csda.2006.12.030" rel="nofollow">10.1016/j.csda.2006.12.030</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Computational+Statistics+%26+Data+Analysis&rft.atitle=Unbiased+split+selection+for+classification+trees+based+on+the+Gini+index&rft.volume=52&rft.pages=483-501&rft.date=2007&rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.525.3178&rft_id=info%3Adoi%2F10.1016%2Fj.csda.2006.12.030&rft.aulast=Strobl&rft.aufirst=Carolin&rft.au=Boulesteix%2C+Anne-Laure&rft.au=Augustin%2C+Thomas&rft_id=https%3A%2F%2Fepub.ub.uni-muenchen.de%2F1833%2F1%2Fpaper_464.pdf&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFPainskyRosset2017">Painsky A, Rosset S (2017). "Cross-Validated Variable Selection in Tree-Based Methods Improves Predictive Performance". <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>. <b>39</b> (11): 2142–2153. <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1512.03444" rel="nofollow">1512.03444</a></span>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1109%2Ftpami.2016.2636831" rel="nofollow">10.1109/tpami.2016.2636831</a>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/28114007" rel="nofollow">28114007</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=IEEE+Transactions+on+Pattern+Analysis+and+Machine+Intelligence&rft.atitle=Cross-Validated+Variable+Selection+in+Tree-Based+Methods+Improves+Predictive+Performance&rft.volume=39&rft.issue=11&rft.pages=2142-2153&rft.date=2017&rft_id=info%3Aarxiv%2F1512.03444&rft_id=info%3Apmid%2F28114007&rft_id=info%3Adoi%2F10.1109%2Ftpami.2016.2636831&rft.aulast=Painsky&rft.aufirst=Amichai&rft.au=Rosset%2C+Saharon&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFTolosiLengauer2011">Tolosi L, Lengauer T (July 2011). <a class="external text" href="http://bioinformatics.oxfordjournals.org/content/27/14/1986.abstract" rel="nofollow">"Classification with correlated features: unreliability of feature ranking and solutions"</a>. <i>Bioinformatics</i>. <b>27</b> (14): 1986–94. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="https://doi.org/10.1093%2Fbioinformatics%2Fbtr300" rel="nofollow">10.1093/bioinformatics/btr300</a></span>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/21576180" rel="nofollow">21576180</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Bioinformatics&rft.atitle=Classification+with+correlated+features%3A+unreliability+of+feature+ranking+and+solutions&rft.volume=27&rft.issue=14&rft.pages=1986-94&rft.date=2011-07&rft_id=info%3Adoi%2F10.1093%2Fbioinformatics%2Fbtr300&rft_id=info%3Apmid%2F21576180&rft.aulast=Tolosi&rft.aufirst=L&rft.au=Lengauer%2C+T&rft_id=http%3A%2F%2Fbioinformatics.oxfordjournals.org%2Fcontent%2F27%2F14%2F1986.abstract&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-linjeon02-22"><span class="mw-cite-backlink">^ <a href="#cite_ref-linjeon02_22-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-linjeon02_22-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation techreport cs1" id="CITEREFLinJeon2002">Lin, Yi; Jeon, Yongho (2002). <i>Random forests and adaptive nearest neighbors</i> (Technical report). Technical Report No. 1055. University of Wisconsin. <a class="mw-redirect" href="/wiki/CiteSeerX_(identifier)" title="CiteSeerX (identifier)">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.153.9168" rel="nofollow">10.1.1.153.9168</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&rft.genre=report&rft.btitle=Random+forests+and+adaptive+nearest+neighbors&rft.series=Technical+Report+No.+1055&rft.pub=University+of+Wisconsin&rft.date=2002&rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.153.9168&rft.aulast=Lin&rft.aufirst=Yi&rft.au=Jeon%2C+Yongho&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1">Shi, T., Horvath, S. (2006). "Unsupervised Learning with Random Forest Predictors". <i>Journal of Computational and Graphical Statistics</i>. <b>15</b> (1): 118–138. <a class="mw-redirect" href="/wiki/CiteSeerX_(identifier)" title="CiteSeerX (identifier)">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.698.2365" rel="nofollow">10.1.1.698.2365</a></span>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1198%2F106186006X94072" rel="nofollow">10.1198/106186006X94072</a>. <a class="mw-redirect" href="/wiki/JSTOR_(identifier)" title="JSTOR (identifier)">JSTOR</a> <a class="external text" href="//www.jstor.org/stable/27594168" rel="nofollow">27594168</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Journal+of+Computational+and+Graphical+Statistics&rft.atitle=Unsupervised+Learning+with+Random+Forest+Predictors&rft.volume=15&rft.issue=1&rft.pages=118-138&rft.date=2006&rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.698.2365&rft_id=%2F%2Fwww.jstor.org%2Fstable%2F27594168&rft_id=info%3Adoi%2F10.1198%2F106186006X94072&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><span class="cs1-maint citation-comment">CS1 maint: uses authors parameter (<a href="/wiki/Category:CS1_maint:_uses_authors_parameter" title="Category:CS1 maint: uses authors parameter">link</a>)</span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-24"><span class="mw-cite-backlink"><b><a href="#cite_ref-24">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFShiSeligsonBelldegrunPalotie2005">Shi T, Seligson D, Belldegrun AS, Palotie A, Horvath S (April 2005). <a class="external text" href="https://doi.org/10.1038/modpathol.3800322" rel="nofollow">"Tumor classification by tissue microarray profiling: random forest clustering applied to renal cell carcinoma"</a>. <i>Modern Pathology</i>. <b>18</b> (4): 547–57. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="https://doi.org/10.1038%2Fmodpathol.3800322" rel="nofollow">10.1038/modpathol.3800322</a></span>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/15529185" rel="nofollow">15529185</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Modern+Pathology&rft.atitle=Tumor+classification+by+tissue+microarray+profiling%3A+random+forest+clustering+applied+to+renal+cell+carcinoma&rft.volume=18&rft.issue=4&rft.pages=547-57&rft.date=2005-04&rft_id=info%3Adoi%2F10.1038%2Fmodpathol.3800322&rft_id=info%3Apmid%2F15529185&rft.aulast=Shi&rft.aufirst=T&rft.au=Seligson%2C+D&rft.au=Belldegrun%2C+AS&rft.au=Palotie%2C+A&rft.au=Horvath%2C+S&rft_id=%2F%2Fdoi.org%2F10.1038%2Fmodpathol.3800322&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-25"><span class="mw-cite-backlink"><b><a href="#cite_ref-25">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1">Prinzie, A., Van den Poel, D. (2008). "Random Forests for multiclass classification: Random MultiNomial Logit". <i>Expert Systems with Applications</i>. <b>34</b> (3): 1721–1732. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1016%2Fj.eswa.2007.01.029" rel="nofollow">10.1016/j.eswa.2007.01.029</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Expert+Systems+with+Applications&rft.atitle=Random+Forests+for+multiclass+classification%3A+Random+MultiNomial+Logit&rft.volume=34&rft.issue=3&rft.pages=1721-1732&rft.date=2008&rft_id=info%3Adoi%2F10.1016%2Fj.eswa.2007.01.029&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><span class="cs1-maint citation-comment">CS1 maint: uses authors parameter (<a href="/wiki/Category:CS1_maint:_uses_authors_parameter" title="Category:CS1 maint: uses authors parameter">link</a>)</span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-26"><span class="mw-cite-backlink"><b><a href="#cite_ref-26">^</a></b></span> <span class="reference-text"><cite class="citation conference cs1" id="CITEREFPrinzie2007">Prinzie, Anita (2007). "Random Multiclass Classification: Generalizing Random Forests to Random MNL and Random NB".  In Roland Wagner; Norman Revell; Günther Pernul (eds.). <i>Database and Expert Systems Applications: 18th International Conference, DEXA 2007, Regensburg, Germany, September 3-7, 2007, Proceedings</i>. Lecture Notes in Computer Science. <b>4653</b>. pp. 349–358. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1007%2F978-3-540-74469-6_35" rel="nofollow">10.1007/978-3-540-74469-6_35</a>. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a> <a href="/wiki/Special:BookSources/978-3-540-74467-2" title="Special:BookSources/978-3-540-74467-2"><bdi>978-3-540-74467-2</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&rft.genre=conference&rft.atitle=Random+Multiclass+Classification%3A+Generalizing+Random+Forests+to+Random+MNL+and+Random+NB&rft.btitle=Database+and+Expert+Systems+Applications%3A+18th+International+Conference%2C+DEXA+2007%2C+Regensburg%2C+Germany%2C+September+3-7%2C+2007%2C+Proceedings&rft.series=Lecture+Notes+in+Computer+Science&rft.pages=349-358&rft.date=2007&rft_id=info%3Adoi%2F10.1007%2F978-3-540-74469-6_35&rft.isbn=978-3-540-74467-2&rft.aulast=Prinzie&rft.aufirst=Anita&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-scornet2015random-27"><span class="mw-cite-backlink">^ <a href="#cite_ref-scornet2015random_27-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-scornet2015random_27-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-scornet2015random_27-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-scornet2015random_27-3"><sup><i><b>d</b></i></sup></a></span> <span class="reference-text"><cite class="citation arxiv cs1" id="CITEREFScornet2015">Scornet, Erwan (2015). "Random forests and kernel methods". <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1502.03836" rel="nofollow">1502.03836</a></span> [<a class="external text" href="//arxiv.org/archive/math.ST" rel="nofollow">math.ST</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=preprint&rft.jtitle=arXiv&rft.atitle=Random+forests+and+kernel+methods&rft.date=2015&rft_id=info%3Aarxiv%2F1502.03836&rft.aulast=Scornet&rft.aufirst=Erwan&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-breiman2000some-28"><span class="mw-cite-backlink"><b><a href="#cite_ref-breiman2000some_28-0">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFBreiman2000"><a href="/wiki/Leo_Breiman" title="Leo Breiman">Breiman, Leo</a> (2000). <a class="external text" href="http://oz.berkeley.edu/~breiman/some_theory2000.pdf" rel="nofollow">"Some infinity theory for predictor ensembles"</a> <span class="cs1-format">(PDF)</span>. Technical Report 579, Statistics Dept. UCB.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.atitle=Some+infinity+theory+for+predictor+ensembles&rft.date=2000&rft.aulast=Breiman&rft.aufirst=Leo&rft_id=http%3A%2F%2Foz.berkeley.edu%2F~breiman%2Fsome_theory2000.pdf&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span> <span class="cs1-hidden-error error citation-comment">Cite journal requires <code class="cs1-code">|journal=</code> (<a href="/wiki/Help:CS1_errors#missing_periodical" title="Help:CS1 errors">help</a>)</span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/><sup class="noprint Inline-Template"><span style="white-space: nowrap;">[<i><a href="/wiki/Wikipedia:Link_rot" title="Wikipedia:Link rot"><span title=" Dead link since May 2017">permanent dead link</span></a></i>]</span></sup></span>
</li>
<li id="cite_note-lin2006random-29"><span class="mw-cite-backlink"><b><a href="#cite_ref-lin2006random_29-0">^</a></b></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFLinJeon2006">Lin, Yi; Jeon, Yongho (2006). "Random forests and adaptive nearest neighbors". <i>Journal of the American Statistical Association</i>. <b>101</b> (474): 578–590. <a class="mw-redirect" href="/wiki/CiteSeerX_(identifier)" title="CiteSeerX (identifier)">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.153.9168" rel="nofollow">10.1.1.153.9168</a></span>. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1198%2F016214505000001230" rel="nofollow">10.1198/016214505000001230</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Journal+of+the+American+Statistical+Association&rft.atitle=Random+forests+and+adaptive+nearest+neighbors&rft.volume=101&rft.issue=474&rft.pages=578-590&rft.date=2006&rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.153.9168&rft_id=info%3Adoi%2F10.1198%2F016214505000001230&rft.aulast=Lin&rft.aufirst=Yi&rft.au=Jeon%2C+Yongho&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-davies2014random-30"><span class="mw-cite-backlink"><b><a href="#cite_ref-davies2014random_30-0">^</a></b></span> <span class="reference-text"><cite class="citation arxiv cs1" id="CITEREFDaviesGhahramani2014">Davies, Alex; Ghahramani, Zoubin (2014). "The Random Forest Kernel and other kernels for big data from random partitions". <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1402.4293" rel="nofollow">1402.4293</a></span> [<a class="external text" href="//arxiv.org/archive/stat.ML" rel="nofollow">stat.ML</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=preprint&rft.jtitle=arXiv&rft.atitle=The+Random+Forest+Kernel+and+other+kernels+for+big+data+from+random+partitions&rft.date=2014&rft_id=info%3Aarxiv%2F1402.4293&rft.aulast=Davies&rft.aufirst=Alex&rft.au=Ghahramani%2C+Zoubin&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-breiman2004consistency-31"><span class="mw-cite-backlink">^ <a href="#cite_ref-breiman2004consistency_31-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-breiman2004consistency_31-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal cs1" id="CITEREFBreimanGhahramani2004">Breiman L, Ghahramani Z (2004). "Consistency for a simple model of random forests". <i>Statistical Department, University of California at Berkeley. Technical Report</i> (670). <a class="mw-redirect" href="/wiki/CiteSeerX_(identifier)" title="CiteSeerX (identifier)">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.618.90" rel="nofollow">10.1.1.618.90</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Statistical+Department%2C+University+of+California+at+Berkeley.+Technical+Report&rft.atitle=Consistency+for+a+simple+model+of+random+forests&rft.issue=670&rft.date=2004&rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.618.90&rft.aulast=Breiman&rft.aufirst=Leo&rft.au=Ghahramani%2C+Zoubin&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-arlot2014analysis-32"><span class="mw-cite-backlink">^ <a href="#cite_ref-arlot2014analysis_32-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-arlot2014analysis_32-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation arxiv cs1" id="CITEREFArlotGenuer2014">Arlot S, Genuer R (2014). "Analysis of purely random forests bias". <a class="mw-redirect" href="/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1407.3939" rel="nofollow">1407.3939</a></span> [<a class="external text" href="//arxiv.org/archive/math.ST" rel="nofollow">math.ST</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=preprint&rft.jtitle=arXiv&rft.atitle=Analysis+of+purely+random+forests+bias&rft.date=2014&rft_id=info%3Aarxiv%2F1407.3939&rft.aulast=Arlot&rft.aufirst=Sylvain&rft.au=Genuer%2C+Robin&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></span>
</li>
</ol></div>
<h2><span class="mw-headline" id="Further_reading">Further reading</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Random_forest&action=edit&section=28" title="Edit section: Further reading">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<table class="metadata mbox-small" role="presentation" style="background-color:#f9f9f9;border:1px solid #aaa;color:#000">
<tbody><tr>
<td class="mbox-image"><img alt="" class="noviewer" data-file-height="104" data-file-width="107" decoding="async" height="39" src="//upload.wikimedia.org/wikipedia/commons/thumb/3/32/Scholia_logo.svg/40px-Scholia_logo.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/3/32/Scholia_logo.svg/60px-Scholia_logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/3/32/Scholia_logo.svg/80px-Scholia_logo.svg.png 2x" width="40"/></td>
<td class="mbox-text plainlist"><a class="extiw" href="https://www.wikidata.org/wiki/Wikidata:Scholia" title="wikidata:Wikidata:Scholia">Scholia</a> has a <i>topic</i> profile for <i><b><a class="extiw" href="https://iw.toolforge.org/scholia/topic/Q245748" title="toolforge:scholia/topic/Q245748">Random forest</a></b></i>.</td></tr>
</tbody></table>
<style data-mw-deduplicate="TemplateStyles:r886047268">.mw-parser-output .refbegin{font-size:90%;margin-bottom:0.5em}.mw-parser-output .refbegin-hanging-indents>ul{list-style-type:none;margin-left:0}.mw-parser-output .refbegin-hanging-indents>ul>li,.mw-parser-output .refbegin-hanging-indents>dl>dd{margin-left:0;padding-left:3.2em;text-indent:-3.2em;list-style:none}.mw-parser-output .refbegin-100{font-size:100%}</style><div class="refbegin reflist" style="">
<ul><li><cite class="citation conference cs1" id="CITEREFPrinziePoel2007">Prinzie A, Poel D (2007). <a class="external text" href="https://www.researchgate.net/publication/225175169" rel="nofollow">"Random Multiclass Classification: Generalizing Random Forests to Random MNL and Random NB"</a>. <i>Database and Expert Systems Applications</i>. <a href="/wiki/Lecture_Notes_in_Computer_Science" title="Lecture Notes in Computer Science">Lecture Notes in Computer Science</a>. <b>4653</b>. p. 349. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1007%2F978-3-540-74469-6_35" rel="nofollow">10.1007/978-3-540-74469-6_35</a>. <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a> <a href="/wiki/Special:BookSources/978-3-540-74467-2" title="Special:BookSources/978-3-540-74467-2"><bdi>978-3-540-74467-2</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&rft.genre=conference&rft.atitle=Random+Multiclass+Classification%3A+Generalizing+Random+Forests+to+Random+MNL+and+Random+NB&rft.btitle=Database+and+Expert+Systems+Applications&rft.series=Lecture+Notes+in+Computer+Science&rft.pages=349&rft.date=2007&rft_id=info%3Adoi%2F10.1007%2F978-3-540-74469-6_35&rft.isbn=978-3-540-74467-2&rft.aulast=Prinzie&rft.aufirst=Anita&rft.au=Poel%2C+Dirk&rft_id=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F225175169&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></li>
<li><cite class="citation journal cs1" id="CITEREFDeniskoHoffman2018">Denisko D, Hoffman MM (February 2018). <a class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC5828645" rel="nofollow">"Classification and interaction in random forests"</a>. <i>Proceedings of the National Academy of Sciences of the United States of America</i>. <b>115</b> (8): 1690–1692. <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a class="external text" href="https://doi.org/10.1073%2Fpnas.1800256115" rel="nofollow">10.1073/pnas.1800256115</a>. <a class="mw-redirect" href="/wiki/PMC_(identifier)" title="PMC (identifier)">PMC</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC5828645" rel="nofollow">5828645</a></span>. <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/29440440" rel="nofollow">29440440</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.jtitle=Proceedings+of+the+National+Academy+of+Sciences+of+the+United+States+of+America&rft.atitle=Classification+and+interaction+in+random+forests&rft.volume=115&rft.issue=8&rft.pages=1690-1692&rft.date=2018-02&rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC5828645&rft_id=info%3Apmid%2F29440440&rft_id=info%3Adoi%2F10.1073%2Fpnas.1800256115&rft.aulast=Denisko&rft.aufirst=D&rft.au=Hoffman%2C+MM&rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC5828645&rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandom+forest"></span><link href="mw-data:TemplateStyles:r951705291" rel="mw-deduplicated-inline-style"/></li></ul>
</div>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Random_forest&action=edit&section=29" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a class="external text" href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm" rel="nofollow">Random Forests classifier description</a> (Leo Breiman's site)</li>
<li><a class="external text" href="https://cran.r-project.org/doc/Rnews/Rnews_2002-3.pdf" rel="nofollow">Liaw, Andy & Wiener, Matthew "Classification and Regression by randomForest" R News (2002) Vol. 2/3 p. 18</a> (Discussion of the use of the random forest package for <a class="mw-redirect" href="/wiki/R_programming_language" title="R programming language">R</a>)</li></ul>
<!-- 
NewPP limit report
Parsed by mw1270
Cached time: 20200817194851
Cache expiry: 2592000
Dynamic content: false
Complications: [vary‐revision‐sha1]
CPU time usage: 0.936 seconds
Real time usage: 1.619 seconds
Preprocessor visited node count: 3549/1000000
Post‐expand include size: 111175/2097152 bytes
Template argument size: 3187/2097152 bytes
Highest expansion depth: 17/40
Expensive parser function count: 2/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 144596/5000000 bytes
Lua time usage: 0.389/10.000 seconds
Lua memory usage: 5.43 MB/50 MB
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00% 1157.373      1 -total
 54.68%  632.817      1 Template:Reflist
 15.41%  178.396     21 Template:Cite_journal
 12.51%  144.748      4 Template:Cite_conference
 11.69%  135.264      1 Template:Dead_link
  9.64%  111.602      1 Template:Fix
  8.94%  103.447      2 Template:Category_handler
  8.81%  102.020      1 Template:Machine_learning_bar
  7.08%   81.935      1 Template:Sidebar_with_collapsible_lists
  6.12%   70.793      1 Template:Short_description
-->
<!-- Saved in parser cache with key enwiki:pcache:idhash:1363880-0!canonical!math=5 and timestamp 20200817194849 and revision id 972396951
 -->
</div><noscript><img alt="" height="1" src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" style="border: none; position: absolute;" title="" width="1"/></noscript>
<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Random_forest&oldid=972396951">https://en.wikipedia.org/w/index.php?title=Random_forest&oldid=972396951</a>"</div></div>
<div class="catlinks" data-mw="interface" id="catlinks"><div class="mw-normal-catlinks" id="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Classification_algorithms" title="Category:Classification algorithms">Classification algorithms</a></li><li><a href="/wiki/Category:Ensemble_learning" title="Category:Ensemble learning">Ensemble learning</a></li><li><a href="/wiki/Category:Decision_trees" title="Category:Decision trees">Decision trees</a></li><li><a href="/wiki/Category:Decision_theory" title="Category:Decision theory">Decision theory</a></li><li><a href="/wiki/Category:Computational_statistics" title="Category:Computational statistics">Computational statistics</a></li><li><a href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></li></ul></div><div class="mw-hidden-catlinks mw-hidden-cats-hidden" id="mw-hidden-catlinks">Hidden categories: <ul><li><a href="/wiki/Category:CS1_maint:_uses_authors_parameter" title="Category:CS1 maint: uses authors parameter">CS1 maint: uses authors parameter</a></li><li><a href="/wiki/Category:CS1_errors:_missing_periodical" title="Category:CS1 errors: missing periodical">CS1 errors: missing periodical</a></li><li><a href="/wiki/Category:All_articles_with_dead_external_links" title="Category:All articles with dead external links">All articles with dead external links</a></li><li><a href="/wiki/Category:Articles_with_dead_external_links_from_May_2017" title="Category:Articles with dead external links from May 2017">Articles with dead external links from May 2017</a></li><li><a href="/wiki/Category:Articles_with_permanently_dead_external_links" title="Category:Articles with permanently dead external links">Articles with permanently dead external links</a></li><li><a href="/wiki/Category:Articles_with_short_description" title="Category:Articles with short description">Articles with short description</a></li><li><a href="/wiki/Category:Short_description_is_different_from_Wikidata" title="Category:Short description is different from Wikidata">Short description is different from Wikidata</a></li><li><a href="/wiki/Category:Articles_containing_potentially_dated_statements_from_2019" title="Category:Articles containing potentially dated statements from 2019">Articles containing potentially dated statements from 2019</a></li><li><a href="/wiki/Category:All_articles_containing_potentially_dated_statements" title="Category:All articles containing potentially dated statements">All articles containing potentially dated statements</a></li></ul></div></div>
</div>
</div>
<div id="mw-data-after-content">
<div class="read-more-container"></div>
</div>
<div id="mw-navigation">
<h2>Navigation menu</h2>
<div id="mw-head">
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav aria-labelledby="p-personal-label" class="vector-menu" id="p-personal" role="navigation">
<h3 id="p-personal-label">
<span>Personal tools</span>
</h3>
<!-- Please do not use the .body class, it is deprecated. -->
<div class="body vector-menu-content">
<!-- Please do not use the .menu class, it is deprecated. -->
<ul class="vector-menu-content-list"><li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a accesskey="n" href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]">Talk</a></li><li id="pt-anoncontribs"><a accesskey="y" href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&returnto=Random+forest" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a accesskey="o" href="/w/index.php?title=Special:UserLogin&returnto=Random+forest" title="You're encouraged to log in; however, it's not mandatory. [o]">Log in</a></li></ul>
</div>
</nav>
<div id="left-navigation">
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav aria-labelledby="p-namespaces-label" class="vector-menu vector-menu-tabs vectorTabs" id="p-namespaces" role="navigation">
<h3 id="p-namespaces-label">
<span>Namespaces</span>
</h3>
<!-- Please do not use the .body class, it is deprecated. -->
<div class="body vector-menu-content">
<!-- Please do not use the .menu class, it is deprecated. -->
<ul class="vector-menu-content-list"><li class="selected" id="ca-nstab-main"><a accesskey="c" href="/wiki/Random_forest" title="View the content page [c]">Article</a></li><li id="ca-talk"><a accesskey="t" href="/wiki/Talk:Random_forest" rel="discussion" title="Discuss improvements to the content page [t]">Talk</a></li></ul>
</div>
</nav>
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav aria-labelledby="p-variants-label" class="vector-menu-empty emptyPortlet vector-menu vector-menu-dropdown vectorMenu" id="p-variants" role="navigation">
<input aria-labelledby="p-variants-label" class="vector-menu-checkbox vectorMenuCheckbox" type="checkbox"/>
<h3 id="p-variants-label">
<span>Variants</span>
</h3>
<!-- Please do not use the .body class, it is deprecated. -->
<div class="body vector-menu-content">
<!-- Please do not use the .menu class, it is deprecated. -->
<ul class="menu vector-menu-content-list"></ul>
</div>
</nav>
</div>
<div id="right-navigation">
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav aria-labelledby="p-views-label" class="vector-menu vector-menu-tabs vectorTabs" id="p-views" role="navigation">
<h3 id="p-views-label">
<span>Views</span>
</h3>
<!-- Please do not use the .body class, it is deprecated. -->
<div class="body vector-menu-content">
<!-- Please do not use the .menu class, it is deprecated. -->
<ul class="vector-menu-content-list"><li class="selected" id="ca-view"><a href="/wiki/Random_forest">Read</a></li><li id="ca-edit"><a accesskey="e" href="/w/index.php?title=Random_forest&action=edit" title="Edit this page [e]">Edit</a></li><li id="ca-history"><a accesskey="h" href="/w/index.php?title=Random_forest&action=history" title="Past revisions of this page [h]">View history</a></li></ul>
</div>
</nav>
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav aria-labelledby="p-cactions-label" class="vector-menu-empty emptyPortlet vector-menu vector-menu-dropdown vectorMenu" id="p-cactions" role="navigation">
<input aria-labelledby="p-cactions-label" class="vector-menu-checkbox vectorMenuCheckbox" type="checkbox"/>
<h3 id="p-cactions-label">
<span>More</span>
</h3>
<!-- Please do not use the .body class, it is deprecated. -->
<div class="body vector-menu-content">
<!-- Please do not use the .menu class, it is deprecated. -->
<ul class="menu vector-menu-content-list"></ul>
</div>
</nav>
<div id="p-search" role="search">
<h3>
<label for="searchInput">Search</label>
</h3>
<form action="/w/index.php" id="searchform">
<div id="simpleSearch">
<input accesskey="f" id="searchInput" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" type="search"/>
<input name="title" type="hidden" value="Special:Search"/>
<input class="searchButton mw-fallbackSearchButton" id="mw-searchButton" name="fulltext" title="Search Wikipedia for this text" type="submit" value="Search">
<input class="searchButton" id="searchButton" name="go" title="Go to a page with this exact name if it exists" type="submit" value="Go"/>
</input></div>
</form>
</div>
</div>
</div>
<div id="mw-panel">
<div id="p-logo" role="banner">
<a class="mw-wiki-logo" href="/wiki/Main_Page" title="Visit the main page"></a>
</div>
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav aria-labelledby="p-navigation-label" class="vector-menu vector-menu-portal portal portal-first" id="p-navigation" role="navigation">
<h3 id="p-navigation-label">
<span>Navigation</span>
</h3>
<!-- Please do not use the .body class, it is deprecated. -->
<div class="body vector-menu-content">
<!-- Please do not use the .menu class, it is deprecated. -->
<ul class="vector-menu-content-list"><li id="n-mainpage-description"><a accesskey="z" href="/wiki/Main_Page" title="Visit the main page [z]">Main page</a></li><li id="n-contents"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a accesskey="x" href="/wiki/Special:Random" title="Visit a randomly selected article [x]">Random article</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Learn about Wikipedia and how it works">About Wikipedia</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact us</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en" title="Support us by donating to the Wikimedia Foundation">Donate</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li></ul>
</div>
</nav>
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav aria-labelledby="p-interaction-label" class="vector-menu vector-menu-portal portal" id="p-interaction" role="navigation">
<h3 id="p-interaction-label">
<span>Contribute</span>
</h3>
<!-- Please do not use the .body class, it is deprecated. -->
<div class="body vector-menu-content">
<!-- Please do not use the .menu class, it is deprecated. -->
<ul class="vector-menu-content-list"><li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a accesskey="r" href="/wiki/Special:RecentChanges" title="A list of recent changes to Wikipedia [r]">Recent changes</a></li><li id="n-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Add images or other media for use on Wikipedia">Upload file</a></li></ul>
</div>
</nav>
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav aria-labelledby="p-tb-label" class="vector-menu vector-menu-portal portal" id="p-tb" role="navigation">
<h3 id="p-tb-label">
<span>Tools</span>
</h3>
<!-- Please do not use the .body class, it is deprecated. -->
<div class="body vector-menu-content">
<!-- Please do not use the .menu class, it is deprecated. -->
<ul class="vector-menu-content-list"><li id="t-whatlinkshere"><a accesskey="j" href="/wiki/Special:WhatLinksHere/Random_forest" title="List of all English Wikipedia pages containing links to this page [j]">What links here</a></li><li id="t-recentchangeslinked"><a accesskey="k" href="/wiki/Special:RecentChangesLinked/Random_forest" rel="nofollow" title="Recent changes in pages linked from this page [k]">Related changes</a></li><li id="t-upload"><a accesskey="u" href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]">Upload file</a></li><li id="t-specialpages"><a accesskey="q" href="/wiki/Special:SpecialPages" title="A list of all special pages [q]">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Random_forest&oldid=972396951" title="Permanent link to this revision of this page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Random_forest&action=info" title="More information about this page">Page information</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&page=Random_forest&id=972396951&wpFormIdentifier=titleform" title="Information on how to cite this page">Cite this page</a></li><li id="t-wikibase"><a accesskey="g" href="https://www.wikidata.org/wiki/Special:EntityPage/Q245748" title="Structured data on this page hosted by Wikidata [g]">Wikidata item</a></li></ul>
</div>
</nav>
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav aria-labelledby="p-coll-print_export-label" class="vector-menu vector-menu-portal portal" id="p-coll-print_export" role="navigation">
<h3 id="p-coll-print_export-label">
<span>Print/export</span>
</h3>
<!-- Please do not use the .body class, it is deprecated. -->
<div class="body vector-menu-content">
<!-- Please do not use the .menu class, it is deprecated. -->
<ul class="vector-menu-content-list"><li id="coll-download-as-rl"><a href="/w/index.php?title=Special:ElectronPdf&page=Random+forest&action=show-download-screen" title="Download this page as a PDF file">Download as PDF</a></li><li id="t-print"><a accesskey="p" href="/w/index.php?title=Random_forest&printable=yes" title="Printable version of this page [p]">Printable version</a></li></ul>
</div>
</nav>
<!-- Please do not use role attribute as CSS selector, it is deprecated. -->
<nav aria-labelledby="p-lang-label" class="vector-menu vector-menu-portal portal" id="p-lang" role="navigation">
<h3 id="p-lang-label">
<span>Languages</span>
</h3>
<!-- Please do not use the .body class, it is deprecated. -->
<div class="body vector-menu-content">
<!-- Please do not use the .menu class, it is deprecated. -->
<ul class="vector-menu-content-list"><li class="interlanguage-link interwiki-cs"><a class="interlanguage-link-target" href="https://cs.wikipedia.org/wiki/N%C3%A1hodn%C3%BD_les" hreflang="cs" lang="cs" title="Náhodný les – Czech">Čeština</a></li><li class="interlanguage-link interwiki-de"><a class="interlanguage-link-target" href="https://de.wikipedia.org/wiki/Random_Forest" hreflang="de" lang="de" title="Random Forest – German">Deutsch</a></li><li class="interlanguage-link interwiki-et"><a class="interlanguage-link-target" href="https://et.wikipedia.org/wiki/Otsustusmets" hreflang="et" lang="et" title="Otsustusmets – Estonian">Eesti</a></li><li class="interlanguage-link interwiki-es"><a class="interlanguage-link-target" href="https://es.wikipedia.org/wiki/Random_forest" hreflang="es" lang="es" title="Random forest – Spanish">Español</a></li><li class="interlanguage-link interwiki-fa"><a class="interlanguage-link-target" href="https://fa.wikipedia.org/wiki/%D8%AC%D9%86%DA%AF%D9%84_%D8%AA%D8%B5%D8%A7%D8%AF%D9%81%DB%8C" hreflang="fa" lang="fa" title="جنگل تصادفی – Persian">فارسی</a></li><li class="interlanguage-link interwiki-fr"><a class="interlanguage-link-target" href="https://fr.wikipedia.org/wiki/For%C3%AAt_d%27arbres_d%C3%A9cisionnels" hreflang="fr" lang="fr" title="Forêt d'arbres décisionnels – French">Français</a></li><li class="interlanguage-link interwiki-gl"><a class="interlanguage-link-target" href="https://gl.wikipedia.org/wiki/Random_Forest" hreflang="gl" lang="gl" title="Random Forest – Galician">Galego</a></li><li class="interlanguage-link interwiki-ko"><a class="interlanguage-link-target" href="https://ko.wikipedia.org/wiki/%EB%9E%9C%EB%8D%A4_%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8" hreflang="ko" lang="ko" title="랜덤 포레스트 – Korean">한국어</a></li><li class="interlanguage-link interwiki-id"><a class="interlanguage-link-target" href="https://id.wikipedia.org/wiki/Random_forest" hreflang="id" lang="id" title="Random forest – Indonesian">Bahasa Indonesia</a></li><li class="interlanguage-link interwiki-it"><a class="interlanguage-link-target" href="https://it.wikipedia.org/wiki/Foresta_casuale" hreflang="it" lang="it" title="Foresta casuale – Italian">Italiano</a></li><li class="interlanguage-link interwiki-ja"><a class="interlanguage-link-target" href="https://ja.wikipedia.org/wiki/%E3%83%A9%E3%83%B3%E3%83%80%E3%83%A0%E3%83%95%E3%82%A9%E3%83%AC%E3%82%B9%E3%83%88" hreflang="ja" lang="ja" title="ランダムフォレスト – Japanese">日本語</a></li><li class="interlanguage-link interwiki-pl"><a class="interlanguage-link-target" href="https://pl.wikipedia.org/wiki/Las_losowy" hreflang="pl" lang="pl" title="Las losowy – Polish">Polski</a></li><li class="interlanguage-link interwiki-ru"><a class="interlanguage-link-target" href="https://ru.wikipedia.org/wiki/Random_forest" hreflang="ru" lang="ru" title="Random forest – Russian">Русский</a></li><li class="interlanguage-link interwiki-simple"><a class="interlanguage-link-target" href="https://simple.wikipedia.org/wiki/Random_forest" hreflang="en-simple" lang="en-simple" title="Random forest – Simple English">Simple English</a></li><li class="interlanguage-link interwiki-tr"><a class="interlanguage-link-target" href="https://tr.wikipedia.org/wiki/Rastgele_orman" hreflang="tr" lang="tr" title="Rastgele orman – Turkish">Türkçe</a></li><li class="interlanguage-link interwiki-uk"><a class="interlanguage-link-target" href="https://uk.wikipedia.org/wiki/Random_forest" hreflang="uk" lang="uk" title="Random forest – Ukrainian">Українська</a></li><li class="interlanguage-link interwiki-zh"><a class="interlanguage-link-target" href="https://zh.wikipedia.org/wiki/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97" hreflang="zh" lang="zh" title="随机森林 – Chinese">中文</a></li></ul>
<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a class="wbc-editpage" href="https://www.wikidata.org/wiki/Special:EntityPage/Q245748#sitelinks-wikipedia" title="Edit interlanguage links">Edit links</a></span></div>
</div>
</nav>
</div>
</div>
<footer class="mw-footer" id="footer" role="contentinfo">
<ul id="footer-info">
<li id="footer-info-lastmod"> This page was last edited on 11 August 2020, at 20:54<span class="anonymous-show"> (UTC)</span>.</li>
<li id="footer-info-copyright">Text is available under the <a href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License" rel="license">Creative Commons Attribution-ShareAlike License</a><a href="//creativecommons.org/licenses/by-sa/3.0/" rel="license" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
</ul>
<ul id="footer-places">
<li id="footer-places-privacy"><a class="extiw" href="https://foundation.wikimedia.org/wiki/Privacy_policy" title="wmf:Privacy policy">Privacy policy</a></li>
<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
<li id="footer-places-mobileview"><a class="noprint stopMobileRedirectToggle" href="//en.m.wikipedia.org/w/index.php?title=Random_forest&mobileaction=toggle_view_mobile">Mobile view</a></li>
<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>
<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
</ul>
<ul class="noprint" id="footer-icons">
<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img alt="Wikimedia Foundation" height="31" loading="lazy" src="/static/images/footer/wikimedia-button.png" srcset="/static/images/footer/wikimedia-button-1.5x.png 1.5x, /static/images/footer/wikimedia-button-2x.png 2x" width="88"/></a></li>
<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img alt="Powered by MediaWiki" height="31" loading="lazy" src="/static/images/footer/poweredby_mediawiki_88x31.png" srcset="/static/images/footer/poweredby_mediawiki_132x47.png 1.5x, /static/images/footer/poweredby_mediawiki_176x62.png 2x" width="88"/></a></li>
</ul>
<div style="clear: both;"></div>
</footer>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.936","walltime":"1.619","ppvisitednodes":{"value":3549,"limit":1000000},"postexpandincludesize":{"value":111175,"limit":2097152},"templateargumentsize":{"value":3187,"limit":2097152},"expansiondepth":{"value":17,"limit":40},"expensivefunctioncount":{"value":2,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":144596,"limit":5000000},"entityaccesscount":{"value":0,"limit":400},"timingprofile":["100.00% 1157.373      1 -total"," 54.68%  632.817      1 Template:Reflist"," 15.41%  178.396     21 Template:Cite_journal"," 12.51%  144.748      4 Template:Cite_conference"," 11.69%  135.264      1 Template:Dead_link","  9.64%  111.602      1 Template:Fix","  8.94%  103.447      2 Template:Category_handler","  8.81%  102.020      1 Template:Machine_learning_bar","  7.08%   81.935      1 Template:Sidebar_with_collapsible_lists","  6.12%   70.793      1 Template:Short_description"]},"scribunto":{"limitreport-timeusage":{"value":"0.389","limit":"10.000"},"limitreport-memusage":{"value":5696201,"limit":52428800}},"cachereport":{"origin":"mw1270","timestamp":"20200817194851","ttl":2592000,"transientcontent":false}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"Random forest","url":"https:\/\/en.wikipedia.org\/wiki\/Random_forest","sameAs":"http:\/\/www.wikidata.org\/entity\/Q245748","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q245748","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2005-01-05T04:27:10Z","dateModified":"2020-08-11T20:54:58Z","image":"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/7\/76\/Random_forest_diagram_complete.png","headline":"statistical algorithm that is used to cluster points of data in functional groups"}</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":138,"wgHostname":"mw1367"});});</script>
</body></html>